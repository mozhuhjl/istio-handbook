{"./":{"url":"./","title":"序言","keywords":"","body":"Istio 服务网格——云原生应用网络构建指南 Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017 年开源。本书作为《Kubernetes Handbook——Kubernetes 中文指南/云原生应用架构实践手册》的姊妹篇，希望读者在了解了 Kubernetes 的基础上阅读。 关于本书 本书的主题包括： 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 基于 Istio 的自定义扩展 迁移到 Istio 服务网格 构建云原生应用网络 如何阅读本书 本书基于 Istio 1.11+ 版本编写，您可以通过以下地址参与到本书的编写或阅读本书： 自行构建 在线浏览（推荐） 书中部分内容来自 Tetrate 出品的 Istio 基础教程，请访问 Tetrate 学院，解锁全部教程及测试，获得 Tetrate 认证的 Istio 认证。 致谢 感谢 ServiceMesher 及云原生社区先后负责翻译了 Envoy 及 Istio 官方文档，为本书的成书提供了大量参考资料。 许可证 本书所有内容支持使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。 参与本书 请参考 Istio 文档样式指南。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/what-is-service-mesh.html":{"url":"concepts/what-is-service-mesh.html","title":"什么是服务网格？","keywords":"","body":"什么是服务网格？ Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO 服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。 服务网格的特点 服务网格有如下几个特点： 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 是 CNCF 成员项目，并在 2021 年 7 月毕业。Istio 在 2018年7月31日宣布 1.0，并在 2020 年 7 月将商标捐献给 Open Usage Commons。 理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。 Phil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉： 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现 服务网格的架构如下图所示： 图 1.2.1：Service Mesh 架构图 图片来自：Pattern: Service Mesh 服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。 服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。 Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。 为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。 在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 Twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。 在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。 参考 Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - cloudnative.to Istio 官方文档 - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/why-service-mesh.html":{"url":"concepts/why-service-mesh.html","title":"为什么要使用服务网格？","keywords":"","body":"为什么要使用服务网格？ 要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。 微服务架构 开发人员经常将云原生应用程序分解为多个执行特定动作的服务。你可能有一个只处理客户的服务和另一个处理订单或付款的服务。所有这些服务都通过网络相互沟通。如果一个新的付款需要被处理，请求会被发送到付款服务。如果客户数据需要更新，请求会被发送到客户服务，等等。 图 1.3.1：微服务架构 这种类型的架构被称为微服务架构。这种架构有几个好处。你可以有多个较小的团队从事个别服务。这些团队可以灵活地选择他们的技术栈和语言，并且通常有独立部署和发布服务的自主权。这种机制得以运作得益于在其背后通信的网络。随着服务数量的增加，它们之间的通信和网络通信也在增加。服务和团队的数量使得监控和管理通信逻辑变得相当复杂。由于我们也知道网络是不可靠的，它们会失败，所有这些的结合使得微服务的管理和监控相当复杂。 服务网格概述 服务网格被定义为一个专门的基础设施层，用于管理服务与服务之间的通信，使其可管理、可见、可控制。在某些版本的定义中，你可能还会听到服务网格如何使服务间的通信安全和可靠。如果我必须用一个更直接的句子来描述服务网格，我会说，服务网格是关于服务之间的通信。 但是，服务网格是如何帮助通信的呢？让我们思考一下通信逻辑和它通常所在的地方。在大多数情况下，开发人员将这种逻辑作为服务的一部分来构建。通信逻辑是处理入站或出站请求的任何代码，重试逻辑，超时，甚至可能是流量路由。因此，无论何时服务 A 调用服务 B，请求都要经过这个通信代码逻辑，这个逻辑决定如何处理这个请求。 图 1.3.2：通信逻辑 我们提到，如果我们采用微服务的方法，最终可能会有大量的服务。我们如何处理所有这些服务的通信逻辑呢？我们可以创建一个包含这种逻辑的共享库，并在多个地方重用它。假设我们对所有的服务都使用相同的堆栈或编程语言，共享库的方法可能会很有效。如果我们不这样做，我们将不得不重新实现这个库，这会带来巨大的工作量而且效率低下。你也可能使用自己本身不拥有代码库的服务。在这种情况下，我们无法控制通信逻辑或监控。 第二个问题是配置。除了配置你的应用程序外，我们还必须维护通信逻辑配置。如果我们需要同时调整或更新多个服务，我们将不得不为每个服务单独进行调整。 服务网格所做的是，它将这种通信逻辑、重试、超时等从单个服务中分离出来，并将其移到一个单独的基础设施层。在服务网格的情况下，基础设施层是一个网络代理的阵列。这些网络代理的集合（每个服务实例旁边都有一个）处理你的服务之间的所有通信逻辑。我们称这些代理为 sideecar，因为它们与每个服务并存。 图 1.3.3：Sidecar 代理 以前，我们让 Customer 服务直接与 Payment 服务通信，现在我们有一个 Customer 服务旁边的代理与 Payment 服务旁边的代理通信。服务网格控制平面以这样一种方式配置代理，即它们透明地拦截所有入站和出站请求。这些代理的集合（基础设施层）形成了一个网络网格，称为服务网格。 将通信逻辑从业务和应用逻辑中分离出来，可以使开发人员专注于业务逻辑，而服务网格运维人员则专注于服务网格配置。 服务网格的功能 服务网格为我们提供了一种一致的方式来连接、保护和观察微服务。网格内的代理捕获了网格内所有通信的请求和指标。每一次失败、每一次成功的调用、重试或超时都可以被捕获、可视化，并发出警报。此外，可以根据请求属性做出决定。例如，我们可以检查入站（或出站）请求并编写规则，将所有具有特定头值的请求路由到不同的服务版本。 所有这些信息和收集到的指标使得一些场景可以合理地直接实现。开发人员和运营商可以配置和执行以下方案，而不需要对服务进行任何代码修改。 mTLS 和自动证书轮换 使用指标识别性能和可靠性问题 在 Grafana 等工具中实现指标的可视化 使用 Jaeger 或 Zipkin（需要对代码进行小的修改，以便在服务之间传播跟踪头信息） 对服务进行调试和追踪 基于权重和请求的流量路由，金丝雀部署，A/B 测试 流量镜像 通过超时和重试提高服务的弹性 通过在服务之间注入故障和延迟来进行混沌测试 检测和弹出不健康的服务实例的断路器 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/the-application-networking-in-post-kubernetes-era.html":{"url":"concepts/the-application-networking-in-post-kubernetes-era.html","title":"后 Kubernetes 时代的应用网络","keywords":"","body":"后 Kubernetes 时代的应用网络 如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。 为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envoy 和 Istio 之间是什么关系？ 本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。 Kubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。 Envoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、MOSN 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景，比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。 本文包含以下内容。 kube-proxy 的作用描述。 Kubernetes 在微服务管理方面的局限性。 Istio 服务网格的功能介绍。 Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。 Kubernetes vs Service Mesh 下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系，本文仅针对 sidecar per-pod 模式，详情请参考服务网格的实现模式。 图 1.4.1：kubernetes vs service mesh 流量转发 Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。 服务发现 图 1.4.2：服务发现 Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar 代理拦截的是进出 pod 的流量。 服务网格的劣势 由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟——由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。 服务网格的优势 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。 Kube-proxy 的不足之处 首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。 Kube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？ Kubernetes 社区给出了一个使用 Deployment 做金丝雀发布的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。 Kubernetes Ingress vs Istio Gateway 如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pods 位于 CNI 创建的网络中。一个 ingress—— 一个在 Kubernetes 中创建的资源对象 - 被创建用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 nginx ingress 控制器和 traefik。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量–如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 nginx 配置语言的原因。直接路由南北流量的唯一方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。 Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载平衡器，用于承载进出网状结构边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 Istio 网站。 Envoy Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Envoy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。以下是 Envoy 中的基本术语及其数据结构的列表，更多细节请参考 Envoy 文档。 图 1.4.3：Envoy proxy 架构图 基本术语 下面是您应该了解的 Envoy 里的基本术语： Downstream（下游）：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。 Upstream（上游）：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。 Listener（监听器）：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster（集群）：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到集群的哪个成员。 在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。 xDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 MOSN。 图 1.4.4：Istio Istio 是一个功能非常丰富的服务网格，包括以下功能。 流量管理。这是 Istio 最基本的功能。 策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。 可观察性。在 sidecar 代理中实现。 安全认证。由 Citadel 组件进行密钥和证书管理。 Istio 中的流量管理 Istio 中定义了以下 CRD 来帮助用户进行流量管理。 网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。 虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。 DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。 EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。 ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。 Kubernetes vs xDS vs Istio 在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。 Kubernetes xDS Istio 服务网格 Endpoint Endpoint WorkloadEntry Service Route VirtualService kube-proxy Route DestinationRule kube-proxy Listener EnvoyFilter Ingress Listener Gateway Service Cluster ServiceEntry 核心观点 Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。 Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。 服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。 服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。 xDS 是服务网格的协议标准之一。 服务网格是 Kubernetes 中服务的一个更高层次的抽象。 总结 如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 Knative 这样的无服务器平台，不过这是后话。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/":{"url":"concepts/","title":"概念原理概述","keywords":"","body":"概念原理概述 在这一章中，你将了解： 服务网格的基本概念 Istio 的架构 stio 中的主要部署模式——Sidecar 模式 透明流量拦截——Istio 的核心设计思想 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/service-mesh-architectures.html":{"url":"concepts/service-mesh-architectures.html","title":"服务网格架构","keywords":"","body":"服务网格架构 这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务网格常用的四种部署模式，然后是其最基本的组成部分——控制平面和数据平面。 服务网格的部署模式 这四种模式分别是Sidecar 代理、节点共享代理、Service Account/节点共享代理、带有微代理的共享远程代理。 图 2.2.1：服务网格的部署模式 这几种架构模式的主要区别是代理所在的位置不同，模式一在每个应用程序旁运行一个代理，模式二在每个节点上运行一个代理，模式三在每个节点中，再根据 ServiceAccount 的数量，每个 ServiceAccount 运行一个代理，模式四在每个应用旁运行一个微代理，这个代理仅处理 mTLS ，而七层代理位于远程，这几种方式在安全性、隔离及运维开销上的对比如这个表格所示。 模式 内存开销 安全性 故障域 运维 Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。 节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。 Service Account/节点共享代理 服务账户/身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同“节点共享代理”。 同“节点共享代理”。 带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用7层策略时，工作负载实例的流量会被重定向到L7代理上，若不需要，则可以直接绕过。该L7代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同“Sidecar 代理”。 Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。这两种模式，我们将在下一节中介绍。 下面我们将介绍服务网格的两个主要组成部分——控制平面和数据平面。 控制平面 控制平面的特点： 不直接解析数据包 与控制平面中的代理通信，下发策略和配置 负责网络行为的可视化 通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署 数据平面 数据平面的特点： 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等 对应用来说透明，即可以做到无感知部署 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/service-mesh-patterns.html":{"url":"concepts/service-mesh-patterns.html","title":"服务网格的实现模式","keywords":"","body":"服务网格的实现模式 我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 图 2.2.1.1：服务网格 架构示意图 当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。 Ingress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。 图 2.2.1.2：Ingress 或边缘代理架构示意图 这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。 路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。 图 2.2.1.3：路由器网格架构示意图 这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。 Proxy per Node 这种架构是在每个节点上都部署一个代理，如果使用 Kubernetes 来部署的话就是使用 DaemonSet 对象，Linkerd 第一代就是使用这种方式部署的，一代的 Linkerd 使用 Scala 开发，基于 JVM 比较消耗资源，二代的 Linkerd 使用 Go 开发。 图 2.2.1.4：Proxy per node 架构示意图 这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个sidecar的方式更节省资源，而且更适合基于物理机/虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。 Sidecar代理/Fabric模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。 图 2.2.1.5：Sidecar代理/Fabric模型示意图 这已经是最接近服务网格架构的一种形态了，唯一缺的就是控制平面了。所有的sidecar都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多sidecar就需要一个统一的控制平面了。 Sidecar代理/控制平面 下面的示意图是目前大多数服务网格的架构图，也可以说是整个服务网格架构演进的最终形态。 图 2.2.1.6：Sidecar 代理/控制平面架构示意图 这种架构将代理作为整个服务网格中的一部分，使用 Kubernetes 部署的话，可以通过以 sidecar 的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。 多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/istio-intro.html":{"url":"concepts/istio-intro.html","title":"Istio 简介","keywords":"","body":"Istio 简介 Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。 1. 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 2. 可观察性 Istio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。 3. 安全性 Istio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。 Istio 组件 Istio 服务网格有两个部分：数据平面和控制平面。 在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。 Istio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。 图 2.2.2.1：Istio 架构 Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。 Envoy 代理也是唯一与流量进行交互的组件。除了前面提到的功能 —— 负载均衡、断路器、故障注入等。Envoy 还支持基于 WebAssembly（WASM）的可插拔扩展模型。这种可扩展性使我们能够执行自定义策略，并为网格中的流量生成遥测数据。 Istiod（控制平面） Istiod 是控制平面组件，提供服务发现、配置和证书管理功能。Istiod 采用 YAML 编写的高级规则，并将其转换为 Envoy 的可操作配置。然后，它把这个配置传播给网格中的所有 sidecar。 Istiod 内部的 Pilot 组件抽象出特定平台的服务发现机制（Kubernetes、Consul 或 VM），并将其转换为 sidecar 可以使用的标准格式。 使用内置的身份和凭证管理，我们可以实现强大的服务间和终端用户认证。通过授权功能，我们可以控制谁可以访问你的服务。 控制平面的部分以前被称为 Citadel，作为一个证书授权机构，生成证书，允许数据平面中的代理之间进行安全的 mTLS 通信。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/istio-architecture.html":{"url":"concepts/istio-architecture.html","title":"Istio 架构解析","keywords":"","body":"Istio 架构解析 Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。 图 2.2.3.1：Istio 的部署架构 Sidecar 模式 Sidecar 模式是 Istio 开源之初就在使用的模式，这种模式将应用程序的功能划分为单独的进程运行在同一个最小调度单元中，比如 Kubernetes 的 Pod 中。这种架构分为两个部分：控制平面和数据平面，控制平面是一个单体应用 Istiod，数据平面是由注入在每个 Pod 中的 Envoy 代理组成。你可以在 Sidecar 中添加更多功能，而不需要修改应用程序代码。这也是服务网格最大的一个卖点之一，将原先的应用程序 SDK 中的功能转移到了 Sidecar 中，这样开发者就可以专注于业务逻辑，而 sidecar 就交由运维来处理。 图 2.2.3.2：Sidecar 模式 我们在看下应用程序 Pod 中的结构。Pod 中包含应用容器和 Sidecar 容器，sidecar 容器与控制平面通信，获取该 Pod 上的所有代理配置，其中还有个 Init 容器，它是在注入 Sidecar 之前启动的，用来修改 Pod 的 IPtables 规则，做流量拦截的。 Proxyless 模式 Proxyless 模式是 Istio 1.11 版本中支持的实验特性，Istio 官网中有篇博客介绍了这个特性。可以直接将 gRPC 服务添加到 Istio 中，不需要再向 Pod 中注入 Envoy 代理。这样做可以极大的提升应用性能，降低网络延迟。有人说这种做法又回答了原始的基于 SDK 的微服务模式，其实非也，它依然使用了 Envoy 的 xDS API，但是因为不再需要向应用程序中注入 Sidecar 代理，因此可以减少应用程序性能的损耗。 图 2.2.3.3：Proxyless 模式 总结 Istio 虽然从诞生之初就是用的是 Sidecar 模式，但是随着网格规模的增大，大量的 Sidecar 对系统造成的瓶颈，及如 eBPF 这里网络技术的演进，未来我们有可能看到部分服务网格的功能，如透明流量劫持、证书配置等下沉到内核层，Istio 的架构也不一定会一成不变。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/sidecar-pattern.html":{"url":"concepts/sidecar-pattern.html","title":"Sidecar 模式","keywords":"","body":"Sidecar 模式 Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。 就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。 让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。 每个微服务都需要具有可观察性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。 但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。 使用 Sidecar 模式的优势 通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。 Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在服务网格中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述了节点代理和 Sidecar 模式的服务网格架构。 使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。 参考 理解 Istio 服务网格中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 微服务中的 Sidecar 设计模式解析 - cloudnative.to Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/sidecar-injection-deep-dive.html":{"url":"concepts/sidecar-injection-deep-dive.html","title":"Istio 中的 Sidecar 注入与流量劫持详解","keywords":"","body":"Istio 中的 Sidecar 注入与流量劫持详解 在讲解 Istio 如何将 Envoy 代理注入到应用程序 Pod 中之前，我们需要先了解以下几个概念： Sidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式。 Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 iptables：流量劫持是通过 iptables 转发实现的。 查看目前 reviews-v1-745ffc55b7-2l2lw Pod 中运行的容器： $ kubectl -n default get pod reviews-v1-745ffc55b7-2l2lw -o=jsonpath='{..spec.containers[*].name}' reviews istio-proxy reviews 即应用容器，istio-proxy 即 Envoy 代理的 sidecar 容器。另外该 Pod 中实际上还运行过一个 Init 容器，因为它执行结束就自动终止了，所以我们看不到该容器的存在。关注 jsonpath 的用法请参考 JSONPath Support。 Init 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。 Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。 在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。 关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。 Sidecar 注入示例分析 本文我们将以 Istio 官方示例 bookinfo 中 reivews 服务为例，来接讲解 Sidecar 容器注入的额流程，每个注入了 Sidecar 的 Pod 中除了原先应用的应用本身的容器外，都会多出来这样两个容器： istio-init：用于给 Sidecar 容器即 Envoy 代理做初始化，设置 iptables 端口转发 istio-proxy：Envoy 代理容器，运行 Envoy 代理 接下来将分别解析下这两个容器。 Init 容器解析 Istio 在 Pod 中注入的 Init 容器名为 istio-init，如果你查看 reviews Deployment 配置，你将看到其中 initContaienrs 的启动参数： initContainers: - name: istio-init image: docker.io/istio/proxyv2:1.13.1 args: - istio-iptables - '-p' - '15001' - '-z' - '15006' - '-u' - '1337' - '-m' - REDIRECT - '-i' - '*' - '-x' - '' - '-b' - '*' - '-d' - 15090,15021,15020 我们看到 istio-init 容器的入口是 istio-iptables 命令，该命令是用于初始化路由表的。 Init 容器启动入口 Init 容器的启动入口是 /usr/local/bin/istio-iptable 命令，该命令的用法如下： $ istio-iptables -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h] -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001） -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337） -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值） -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 -z: 所有入站 TCP 流量重定向端口（默认为 $INBOUND_CAPTURE_PORT 15006） 关于该命令的详细代码请查看 GitHub：tools/istio-iptables/pkg/cmd/root.go。 再参考 istio-init 容器的启动参数，完整的启动命令如下： $ /usr/local/bin/istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i '*' -x \"\" -b * -d \"15090,15201,15020\" 该容器存在的意义就是让 Envoy 代理可以拦截所有的进出 Pod 的流量，即将入站流量重定向到 Sidecar，再拦截应用容器的出站流量经过 Sidecar 处理后再出站。 命令解析 这条启动命令的作用是： 将应用容器的所有流量都转发到 Envoy 的 15006 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 Envoy 代理。 将除了 15090、15201、15020 端口以外的所有端口的流量重定向到 Envoy 代理。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 Sidecar 容器中。 istio-proxy 容器解析 为了查看 iptables 配置，我们需要登陆到 Sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 reviews Pod 所在的主机上使用 docker 命令登陆容器中查看。 查看 reviews Pod 所在的主机。 $ kubectl -n default get pod -l app=reviews -o wide NAME READY STATUS RESTARTS AGE IP NODE reviews-v1-745ffc55b7-2l2lw 2/2 Running 0 1d 172.33.78.10 node3 从输出结果中可以看到该 Pod 运行在 node3 上，使用 vagrant 命令登陆到 node3 主机中并切换为 root 用户。 $ vagrant ssh node3 $ sudo -i 查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables.sh 传递的参数中指定将入站流量重定向到 Envoy 的模式为 “REDIRECT”，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables，规则配置请参考 iptables 规则配置。 理解 iptables iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。 在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。 下图展示了 iptables 调用链。 图 2.3.1.1：iptables 调用链 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表： raw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换（例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。 不同的表中的具有的链类型如下表所示： 规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ ✓ 下图是 iptables 的调用链顺序。 图 2.3.1.2：iptables 调用链 iptables 命令 iptables 命令的主要用途是修改这些表中的规则。iptables 命令格式如下： $ iptables [-t 表名] 命令选项［链名]［条件匹配］[-j 目标动作或跳转］ Init 容器中的 /istio-iptables.sh 启动入口脚本就是执行 iptables 初始化的。 理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。 $ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。 下图是 iptables 的建议结构图，流量在经过 INPUT 链之后就进入了上层协议栈，比如 图 2.3.1.3：iptables结构图 每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。 pkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，后者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。 关于 iptables 规则请参考常见iptables使用规则场景整理。 target 支持的类型 target 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念。 从输出结果中可以看到 Init 容器没有在 iptables 的默认链路中创建任何规则，而是创建了新的链路。 查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。 图 2.3.1.4：Envoy sidecar 流量劫持流程示意图 Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。 进入到 reviews pod Reviews 服务有三个版本，我们进入到其中任意一个版本，例如 reviews-1，首先你需要搞清楚这个 pod 运行在哪个节点上，知道那个容器的具体 ID，然后使用 SSH 登录那个节点，使用 ps 命令查看到那个容器的具体 IP，使用 nsenter 命令进入该容器。 nsenter -t{PID} -n 为什么不直接使用 kubectl 进入容器？ Istio 向 pod 中自动注入的 sidecar 容器（名为 istio-proxy）其中默认的用户是 istio-proxy，该用户没有权限查看路由表规则，即当你在该容器中运行 iptabes 命令时会得到 iptables -t nat -L -v 这样的结果，而且你又没有 root 权限。对于 reviews 容器也是一样，默认用户的 UID 是 1000，而且这个用户又没有名字，一样也无法切换为 root 用户，系统中默认没有安装 iptabels 命令。所以我们只能登录到 Pod 的宿主节点上，使用 nsenter 命令进入容器内部。 查看路由表 下面是查看 nat 表中的规则，其中链的名字中包含 ISTIO 前缀的是由 Init 容器注入的，规则匹配是根据下面显示的顺序来执行的，其中会有多次跳转。 # 查看 NAT 表中规则配置的详细信息 $ iptables -t nat -L -v # PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上 Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 2 120 ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链 Chain INPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上 Chain OUTPUT (policy ACCEPT 41146 packets, 3845K bytes) pkts bytes target prot opt in out source destination 93 5580 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理 Chain POSTROUTING (policy ACCEPT 41199 packets, 3848K bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有目的地为 9080 端口的入站流量重定向到 ISTIO_IN_REDIRECT 链上 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 2 120 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere tcp dpt:9080 # ISTIO_IN_REDIRECT 链：将所有的入站流量跳转到本地的 15006 端口，至此成功的拦截了流量到 Envoy Chain ISTIO_IN_REDIRECT (1 references) pkts bytes target prot opt in out source destination 2 120 REDIRECT tcp -- any any anywhere anywhere redir ports 15006 # ISTIO_OUTPUT 链：选择需要重定向到 Envoy（即本地） 的出站流量，所有非 localhost 的流量全部转发到 ISTIO_REDIRECT。为了避免流量在该 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。如果目的地非 localhost 就跳转到 ISTIO_REDIRECT；如果流量是来自 istio-proxy 用户空间的，那么就跳出该链，返回它的调用链继续执行下一条规则（OUPT 的下一条规则，无需对流量进行处理）；所有的非 istio-proxy 用户空间的目的地是 localhost 的流量就跳转到 ISTIO_REDIRECT Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 ISTIO_REDIRECT all -- any lo anywhere !localhost 40 2400 RETURN all -- any any anywhere anywhere owner UID match istio-proxy 0 0 RETURN all -- any any anywhere anywhere owner GID match istio-proxy 0 0 RETURN all -- any any anywhere localhost 53 3180 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT 链：将所有流量重定向到 Envoy（即本地） 的 15001 端口 Chain ISTIO_REDIRECT (2 references) pkts bytes target prot opt in out source destination 53 3180 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 iptables 显示的链的顺序，即流量规则匹配的顺序。其中要特别注意 ISTIO_OUTPUT 链中的规则配置。为了避免流量一直在 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。 ISTIO_OUTPUT 链规则匹配的详细过程如下： 如果目的地非 localhost 就跳转到 ISTIO_REDIRECT 链 所有来自 istio-proxy 用户空间的非 localhost 流量跳转到它的调用点 OUTPUT 继续执行 OUTPUT 链的下一条规则，因为 OUTPUT 链中没有下一条规则了，所以会继续执行 POSTROUTING 链然后跳出 iptables，直接访问目的地 如果流量不是来自 istio-proxy 用户空间，又是对 localhost 的访问，那么就跳出 iptables，直接访问目的地 其它所有情况都跳转到 ISTIO_REDIRECT 链 其实在最后这条规则前还可以增加 IP 地址过滤，让某些 IP 地址段不通过 Envoy 代理。 以上 iptables 规则都是 Init 容器启动的时使用 istio-iptables 命令生成的，详细过程可以查看该命令行程序。 参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 - jimmysong.io JSONPath Support - kubernetes.io iptables 命令使用说明 - wangchujiang.com How To List and Delete Iptables Firewall Rules - digitalocean.com 一句一句解说 iptables的详细中文手册 - cnblog.com 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/istio-sidecar-injector.html":{"url":"concepts/istio-sidecar-injector.html","title":"Sidecar 的自动注入过程详解","keywords":"","body":"Sidecar 的自动注入过程详解 在 Sidecar 注入与流量劫持详解中我只是简单介绍了 Sidecar 注入的步骤，但是没有涉及到具体的 Sidecar 注入流程与细节，这一篇将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。 Sidecar 注入过程 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 webhook，参考 Istio 官网中对 Istio sidecar 注入的描述。 图 2.3.2.1：Sidecar 注入流程图 手动注入 sidecar 与自动注入 sidecar 的区别 不论是手动注入还是自动注入，sidecar 的注入过程有需要遵循如下步骤： Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧； Istio 和 sidecar 配置保存在 istio 和 istio-sidecar-injector 这两个 ConfigMap 中，其中包含了 Go template，所谓自动 sidecar 注入就是将生成 Pod 配置从应用 YAML 文件期间转移到 mutable webhook 中。 参考 注入 Istio sidecar - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/traffic-management.html":{"url":"concepts/traffic-management.html","title":"流量管理","keywords":"","body":"流量管理 这一章节将大家了解 Istio 流量管理中的各种概念的含义及表示方法。 流量管理是 Isito 中的最基础功能，使用 Istio 的流量管理模型，本质上是将流量与基础设施扩容解耦，让运维人员可以通过 Pilot 指定流量遵循什么规则，而不是指定哪些 pod/VM 应该接收流量——Pilot 和智能 Envoy 代理会帮我们搞定。 所谓流量管理是指： 控制服务之间的路由：通过在 VirtualService 中的规则条件匹配来设置路由，可以在服务间拆分流量。 控制路由上流量的行为：设定好路由之后，就可以在路由上指定超时和重试机制，例如超时时间、重试次数等；做错误注入、设置断路器等。可以由 VirtualService 和 DestinationRule 共同完成。 显式地向网格中注册服务：显示地引入 Service Mesh 内部或外部的服务，纳入服务网格管理。由 ServiceEntry 实现。 控制网格边缘的南北向流量：为了管理进入 Istio service mesh 的南北向入口流量，需要创建 Gateway 对象并与 VirtualService 绑定。 关于流量管理的详细介绍请参考 Istio 官方文档。 参考 流量管理 - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/traffic-management-basic.html":{"url":"concepts/traffic-management-basic.html","title":"流量管理基础概念","keywords":"","body":"流量管理基础概念 下面将带您了解 Istio 流量管理相关的基础概念与配置示例。 VirtualService：在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule：是 VirtualService 路由生效后，配置应用与请求的策略集。 ServiceEntry：通常用于在 Istio 服务网格之外启用的服务请求。 Gateway：为 HTTP/TCP 流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。 EnvoyFilter：描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的。 注：本文中的示例引用自 Istio 官方 Bookinfo 示例，见：Istio 代码库，且对于配置的讲解都以在 Kubernetes 中部署的服务为准。 VirtualService VirtualService 故名思义，就是虚拟服务，在 Istio 1.0 以前叫做 RouteRule。VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。 注意：VirtualService 中的规则是按照在 YAML 文件中的顺序执行的，这就是为什么在存在多条规则时，需要慎重考虑优先级的原因。 配置说明 下面是 VirtualService 的配置说明。 字段 类型 描述 hosts string[] 必要字段：流量的目标主机。可以是带有通配符前缀的 DNS 名称，也可以是 IP 地址。根据所在平台情况，还可能使用短名称来代替 FQDN。这种场景下，短名称到 FQDN 的具体转换过程是要靠下层平台完成的。一个主机名只能在一个 VirtualService 中定义。同一个 VirtualService 中可以用于控制多个 HTTP 和 TCP 端口的流量属性。Kubernetes 用户注意：当使用服务的短名称时（例如使用 reviews，而不是 reviews.default.svc.cluster.local），Istio 会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。假设 “default” 命名空间的一条规则中包含了一个 reviews 的 host 引用，就会被视为 reviews.default.svc.cluster.local，而不会考虑 reviews 服务所在的命名空间。为了避免可能的错误配置，建议使用 FQDN 来进行服务引用。 hosts 字段对 HTTP 和 TCP 服务都是有效的。网格中的服务也就是在服务注册表中注册的服务，必须使用他们的注册名进行引用；只有 Gateway 定义的服务才可以使用 IP 地址。 gateways string[] Gateway 名称列表，Sidecar 会据此使用路由。VirtualService 对象可以用于网格中的 Sidecar，也可以用于一个或多个 Gateway。这里公开的选择条件可以在协议相关的路由过滤条件中进行覆盖。保留字 mesh 用来指代网格中的所有 Sidecar。当这一字段被省略时，就会使用缺省值（mesh），也就是针对网格中的所有 Sidecar 生效。如果提供了 gateways 字段，这一规则就只会应用到声明的 Gateway 之中。要让规则同时对 Gateway 和网格内服务生效，需要显式的将 mesh 加入 gateways 列表。 http HTTPRoute[] HTTP 流量规则的有序列表。这个列表对名称前缀为 http-、http2-、grpc- 的服务端口，或者协议为 HTTP、HTTP2、GRPC 以及终结的 TLS，另外还有使用 HTTP、HTTP2 以及 GRPC 协议的 ServiceEntry 都是有效的。进入流量会使用匹配到的第一条规则。 tls TLSRoute[] 一个有序列表，对应的是透传 TLS 和 HTTPS 流量。路由过程通常利用 ClientHello 消息中的 SNI 来完成。TLS 路由通常应用在 https-、tls- 前缀的平台服务端口，或者经 Gateway 透传的 HTTPS、TLS 协议端口，以及使用 HTTPS 或者 TLS 协议的 ServiceEntry 端口上。注意：没有关联 VirtualService 的 https- 或者 tls- 端口流量会被视为透传 TCP 流量。 tcp TCPRoute[] 一个针对透传 TCP 流量的有序路由列表。TCP 路由对所有 HTTP 和 TLS 之外的端口生效。进入流量会使用匹配到的第一条规则。 示例 下面的例子中配置了一个名为 reviews 的 VirtualService，该配置的作用是将所有发送给 reviews 服务的流量发送到 v1 版本的子集。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 该配置中流量的目标主机是 reviews，如果该服务和规则部署在 Kubernetes 的 default namespace 下的话，对应于 Kubernetes 中的服务的 DNS 名称就是 reviews.default.svc.cluster.local。 我们在 hosts 配置了服务的名字只是表示该配置是针对 reviews.default.svc.cluster.local 的服务的路由规则，但是具体将对该服务的访问的流量路由到哪些服务的哪些实例上，就是要通过 destination 的配置了。 我们看到上面的 VirtualService 的 HTTP 路由中还定义了一个 destination。destination 用于定义在网络中可寻址的服务，请求或连接在经过路由规则的处理之后，就会被发送给 destination。destination.host 应该明确指向服务注册表中的一个服务。Istio 的服务注册表除包含平台服务注册表中的所有服务（例如 Kubernetes 服务、Consul 服务）之外，还包含了 ServiceEntry 资源所定义的服务。VirtualService 中只定义流量发送给哪个服务的路由规则，但是并不知道要发送的服务的地址是什么，这就需要 DestinationRule 来定义了。 subset 配置流量目的地的子集，下文会讲到。VirtualService 中其实可以除了 hosts 字段外其他什么都不配置，路由规则可以在 DestinationRule 中单独配置来覆盖此处的默认规则。 Subset subset 不属于 Istio 创建的 CRD，但是它是一条重要的配置信息，有必要单独说明下。subset 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景。另外在 subset 中可以覆盖服务级别的即 VirtualService 中的定义的流量策略。 以下是subset 的配置信息。对于 Kubernetes 中的服务，一个 subset 相当于使用 label 的匹配条件选出来的 service。 字段 类型 描述 name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。 labels map 必要字段。使用标签对服务注册表中的服务端点进行筛选。 trafficPolicy TrafficPolicy 应用到这一 subset 的流量策略。缺省情况下 subset 会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。 DestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。 配置说明 下面是 DestinationRule 的配置说明。 字段 类型 描述 name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。 labels map 必要字段。使用标签对服务注册表中的服务端点进行筛选。 trafficPolicy TrafficPolicy 应用到这一子集的流量策略。缺省情况下子集会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。 示例 下面是一条对 productpage 服务的流量目的地策略的配置。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 该路由策略将所有对 reviews 服务的流量路由到 v1 的 subset。 ServiceEntry Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册。 EnvoyFilter EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的，属于高级配置，用于扩展 Envoy 中的过滤器的。 Gateway Gateway 为 HTTP/TCP 流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量，相当于前端代理。与 Kubernetes 的 Ingress 不同，Istio Gateway 只配置四层到六层的功能（例如开放端口或者 TLS 配置），而 Kubernetes 的 Ingress 是七层的。将 VirtualService 绑定到 Gateway 上，用户就可以使用标准的 Istio 规则来控制进入的 HTTP 和 TCP 流量。 Gateway 设置了一个集群外部流量访问集群中的某些服务的入口，而这些流量究竟如何路由到那些服务上则需要通过配置 VirtualServcie 来绑定。下面仍然以 productpage 这个服务来说明。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # 使用默认的控制器 servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"*\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 上面的例子中 bookinfo 这个 VirtualService 中绑定到了 bookinfo-gateway。bookinfo-gateway 使用了标签选择器选择对应的 Kubernetes pod，即下图中的 pod。 图 2.4.1.1：istio ingress gateway pod 我们再看下 istio-ingressgateway 的 YAML 安装配置。 # Deployment 配置 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgateway spec: replicas: 1 template: metadata: labels: app: istio-ingressgateway istio: ingressgateway annotations: sidecar.istio.io/inject: \"false\" scheduler.alpha.kubernetes.io/critical-pod: \"\" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: \"gcr.io/istio-release/proxyv2:1.0.0\" # 容器启动命令入口是 /usr/local/bin/pilot-agent，后面跟参数 proxy 就会启动一个 Envoy 进程 imagePullPolicy: IfNotPresent ports: - containerPort: 80 - containerPort: 443 - containerPort: 31400 - containerPort: 15011 - containerPort: 8060 - containerPort: 15030 - containerPort: 15031 args: - proxy - router - -v - \"2\" - --discoveryRefreshDelay - '1s' #discoveryRefreshDelay - --drainDuration - '45s' #drainDuration - --parentShutdownDuration - '1m0s' #parentShutdownDuration - --connectTimeout - '10s' #connectTimeout - --serviceCluster - istio-ingressgateway - --zipkinAddress - zipkin:9411 - --statsdUdpAddress - istio-statsd-prom-bridge:9125 - --proxyAdminPort - \"15000\" - --controlPlaneAuthPolicy - NONE - --discoveryAddress - istio-pilot.istio-system:8080 resources: requests: cpu: 10m env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name volumeMounts: ... # 服务配置 --- apiVersion: v1 kind: Service metadata: name: istio-ingressgateway namespace: istio-system annotations: labels: chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgateway spec: type: NodePort selector: app: istio-ingressgateway istio: ingressgateway ports: - name: http2 # 将 ingressgateway 的 80 端口映射到节点的 31380 端口以代理 HTTP 请求 nodePort: 31380 port: 80 targetPort: 80 - name: https nodePort: 31390 port: 443 - name: tcp nodePort: 31400 port: 31400 - name: tcp-pilot-grpc-tls port: 15011 targetPort: 15011 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: http2-prometheus port: 15030 targetPort: 15030 - name: http2-grafana port: 15031 targetPort: 15031 我们看到 ingressgateway 使用的是 proxyv2 镜像，该镜像容器的启动命令入口是 /usr/local/bin/pilot-agent，后面跟参数 proxy 就会启动一个 Envoy 进程，因此 Envoy 既作为 sidecar 也作为边缘代理，egressgateway 的情况也是类似，只不过它控制的是集群内部对外集群外部的请求。这正好验证了本文开头中所画的 Istio Pilot 架构图。请求 /productpage 、/login、/logout、/api/v1/products 这些 URL 的流量转发给 productpage 服务的 9080 端口，而这些流量进入集群内又是经过 ingressgateway pod 代理的，通过访问 ingressgateway pod 所在的宿主机的 31380 端口进入集群内部的。 示例 我们以官方的 bookinfo 示例来解析流量管理配置。下图是 VirtualService 和 DestinationRule 的示意图，其中只显示了 productpage 和 reviews 服务。 图 2.4.1.2：VirtualSerivce 和 DestimationRule 示意图 在前提条件中我部署了该示例，并列出了该示例中的所有 pod，现在我们使用 istioctl 命令来启动查看 productpage-v1-745ffc55b7-2l2lw pod 中的流量配置。 查看 pod 中 Envoy sidecar 的启动配置信息 Bootstrap 消息是 Envoy 配置的根本来源，Bootstrap 消息的一个关键的概念是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从静态资源中获得也可以从动态资源中配置的 LDS 或 CDS 之类的 xDS 服务获取。关于 xDS 服务的详解请参考 Envoy 中的 xDS REST 和 gRPC 协议详解。 $ istioctl proxy-config bootstrap productpage-v1-745ffc55b7-2l2lw -o json { \"bootstrap\": { \"node\": { \"id\": \"sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local\", \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-745ffc55b7-2l2lw\", \"istio\": \"sidecar\" }, \"buildVersion\": \"0/1.8.0-dev//RELEASE\" }, \"staticResources\": { # Envoy 的静态配置，除非销毁后重设，否则不会改变，配置中会明确指定每个上游主机的已解析网络名称（ IP 地址、端口、unix 域套接字等）。 \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connectTimeout\": \"10.000s\", \"hosts\": [ { # istio-pilot 的地址，指定控制平面地址，这个必须是通过静态的方式配置的 \"socketAddress\": { \"address\": \"istio-pilot.istio-system\", \"portValue\": 15010 } } ], \"circuitBreakers\": { # 断路器配置 \"thresholds\": [ { \"maxConnections\": 100000, \"maxPendingRequests\": 100000, \"maxRequests\": 100000 }, { \"priority\": \"HIGH\", \"maxConnections\": 100000, \"maxPendingRequests\": 100000, \"maxRequests\": 100000 } ] }, \"http2ProtocolOptions\": { }, \"upstreamConnectionOptions\": { # 上游连接选项 \"tcpKeepalive\": { \"keepaliveTime\": 300 } } }, { # zipkin 分布式追踪地址配置 \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"zipkin.istio-system\", \"portValue\": 9411 } } ] } ] }, # 以下是动态配置 \"dynamicResources\": { \"ldsConfig\": { # Listener Discovery Service 配置，直接使用 ADS 配置，此处不用配置 \"ads\": { } }, \"cdsConfig\": { # Cluster Discovery Service 配置，直接使用 ADS 配置，此处不用配置 \"ads\": { } }, \"adsConfig\": { # Aggregated Discovery Service 配置，ADS 中集成了 LDS、RDS、CDS \"apiType\": \"GRPC\", \"grpcServices\": [ { \"envoyGrpc\": { \"clusterName\": \"xds-grpc\" } } ], \"refreshDelay\": \"1.000s\" } }, \"statsSinks\": [ # metric 汇聚的地址 { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": { \"address\": \"10.254.109.175\", \"port_value\": 9125 } } } } ], \"statsConfig\": { \"useAllDefaultTags\": false }, \"tracing\": { # zipkin 地址 \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } }, \"admin\": { \"accessLogPath\": \"/dev/stdout\", \"address\": { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 15000 } } } }, \"lastUpdated\": \"2018-09-04T03:38:45.645Z\" } 以上为初始信息。 创建一个名为 reviews 的 VirtualService。 $ cat 上面的 VirtualService 定义只是定义了访问 reviews 服务的流量要全部流向 reviews服务的 v1子集，至于哪些实例是 v1 子集，VirtualService 中并没有定义，这就需要再创建个 DestinationRule。 $ cat 同时还可以为每个 subset 设置负载均衡规则。这里面也可以同时创建多个子集，例如同时创建3个 subset 分别对应3个版本的实例。 $ cat 同时配置了三个 subset 当你需要切分流量时可以直接修改 VirtualService 中 destination 里的 subset 即可，还可以根据百分比拆分流量，配置超时和重试，进行错误注入等。 当然上面这个例子中只是简单的将流量全部导到某个 VirtualService 的 subset 中，还可以根据其他限定条件如 HTTP headers、pod 的 label、URL 等。 此时再查询 productpage-v1-745ffc55b7-2l2lw pod 的配置信息。 $ istioctl proxy-config clusters productpage-v1-8d69b45c-bcjqv|grep reviews reviews.default.svc.cluster.local 9080 - outbound EDS reviews.default.svc.cluster.local 9080 v1 outbound EDS reviews.default.svc.cluster.local 9080 v2 outbound EDS reviews.default.svc.cluster.local 9080 v3 outbound EDS 可以看到 reviews 服务的 EDS 设置中包含了3个 subset，另外读者还可以自己运行 istioctl proxy-config listeners 和 istioctl proxy-config route 来查询 pod 的监听器和路由配置。 参考 Envoy 官方文档中文版 - cloudnative.to Envoy v2 API 概览 - cloudnative.to Envoy 中的 xDS REST 和 gRPC 协议详解 - cloudnative.to Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"concepts/sidecar-traffic-routing-deep-dive.html":{"url":"concepts/sidecar-traffic-routing-deep-dive.html","title":"Istio 中的 Sidecar 的流量路由详解","keywords":"","body":"Istio 中的 Sidecar 的流量路由详解 本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。关于流量拦截的详细分析请参考理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持。 下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。 图 2.4.2.1：Bookinfo 示例 Sidecar 注入及流量劫持步骤概述 下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。 1. Kubernetes 通过 Admission Controller 自动注入，或者用户使用 istioctl 命令手动注入 sidecar 容器。 2. 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。 3. 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式） 将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。 4. 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考通过管理接口获取完整配置。 Sidecar proxy 与应用容器的启动顺序问题 启动 sidecar proxy 和应用容器，究竟哪个容器先启动呢？正常情况是 Envoy Sidecar 和应用程序容器全部启动完成后再开始接收流量请求。但是我们无法预料哪个容器会先启动，那么容器启动顺序是否会对 Envoy 劫持流量有影响呢？答案是肯定的，不过分为以下两种情况。 情况1：应用容器先启动，而 sidecar proxy 仍未就绪 这种情况下，流量被 iptables 转移到 15001 端口，而 Pod 中没有监听该端口，TCP 链接就无法建立，请求失败。 情况2：Sidecar 先启动，请求到达而应用程序仍未就绪 这种情况下请求也肯定会失败，至于是在哪一步开始失败的，留给读者来思考。 问题：如果为 sidecar proxy 和应用程序容器添加就绪和存活探针是否可以解决该问题呢？ 5. 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。 6. Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新。 Envoy 如何处理路由转发 下图展示的是 productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews 服务内部时，reviews 服务内部的 Envoy Sidecar 是如何做流量拦截和路由转发的。可以在 Google Drive 上下载原图。 图 2.4.2.2：Envoy sidecar 流量劫持与路由转发示意图 第一步开始时，productpage Pod 中的 Envoy sidecar 已经通过 EDS 选择出了要请求的 reviews 服务的一个 Pod，知晓了其 IP 地址，发送 TCP 连接请求。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以 reviews-v1-cb8655c75-b97zc 这一个 Pod 中的 Sidecar 流量转发步骤来说明。 理解 Inbound Handler Inbound handler 的作用是将 iptables 拦截到的 downstream 的流量转交给 localhost，与 Pod 内的应用程序容器建立连接。 查看下 reviews-v1-cb8655c75-b97zc pod 中的 Listener。 运行 istioctl pc listener reviews-v1-cb8655c75-b97zc 查看该 Pod 中的具有哪些 Listener。 ADDRESS PORT TYPE 172.33.3.3 9080 HTTP 当来自 productpage 的流量抵达 reviews Pod 的时候已经，downstream 必须明确知道 Pod 的 IP 地址为 172.33.3.3 所以才会访问该 Pod，所以该请求是 172.33.3.3:9080。 virtual Listener 从该 Pod 的 Listener 列表中可以看到，0.0.0.0:15001/TCP 的 Listener（其实际名字是 virtual）监听所有的 Inbound 流量，下面是该 Listener 的详细配置。 { \"name\": \"virtual\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15001 } }, \"filterChains\": [ { \"filters\": [ { \"name\": \"envoy.tcp_proxy\", \"config\": { \"cluster\": \"BlackHoleCluster\", \"stat_prefix\": \"BlackHoleCluster\" } } ] } ], \"useOriginalDst\": true } UseOriginalDst：从配置中可以看出 useOriginalDst 配置指定为 true，这是一个布尔值，缺省为 false，使用 iptables 重定向连接时，proxy 接收的端口可能与原始目的地址的端口不一样，如此处 proxy 接收的端口为 15001，而原始目的地端口为 9080。当此标志设置为 true 时，Listener 将连接重定向到与原始目的地址关联的 Listener，此处为 172.33.3.3:9080。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理，即该 virtual Listener，经过 envoy.tcp_proxy 过滤器处理转发给 BlackHoleCluster，这个 Cluster 的作用正如它的名字，当 Envoy 找不到匹配的虚拟监听器时，就会将请求发送给它，并返回 404。这个将于下文提到的 Listener 中设置 bindToPort 相呼应。 注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15006 端口将 iptables 拦截的流量经由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener。 Listener 172.33.3.3_9080 上文说到进入 Inbound handler 的流量被 virtual Listener 转移到 172.33.3.3_9080 Listener，我们在查看下该 Listener 配置。 运行 istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json 查看。 [{ \"name\": \"172.33.3.3_9080\", \"address\": { \"socketAddress\": { \"address\": \"172.33.3.3\", \"portValue\": 9080 } }, \"filterChains\": [ { \"filterChainMatch\": { \"transportProtocol\": \"raw_buffer\" }, \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { ... \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] }, \"use_remote_address\": false, ... } } ]， \"deprecatedV1\": { \"bindToPort\": false } ... }, { \"filterChainMatch\": { \"transportProtocol\": \"tls\" }, \"tlsContext\": {... }, \"filters\": [... ] } ], ... }] bindToPort：注意其中有一个 bindToPort 的配置，其值为 false，该配置的缺省值为 true，表示将 Listener 绑定到端口上，此处设置为 false 则该 Listener 只能处理其他 Listener 转移过来的流量，即上文所说的 virtual Listener，我们看其中的 filterChains.filters 中的 envoy.http_connection_manager 配置部分： \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] } 该配置表示流量将转交给 Cluster inbound|9080||reviews.default.svc.cluster.local 处理。 Cluster inbound|9080||reviews.default.svc.cluster.local 运行 istioctl pc cluster reviews-v1-cb8655c75-b97zc --fqdn reviews.default.svc.cluster.local --direction inbound -o json 查看该 Cluster 的配置如下。 [ { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 9080 } } ], \"circuitBreakers\": { \"thresholds\": [ {} ] } } ] 可以看到该 Cluster 的 Endpoint 直接对应的就是 localhost，再经过 iptables 转发流量就被应用程序容器消费了。 理解 Outbound Handler 因为 reviews 会向 ratings 服务发送 HTTP 请求，请求的地址是：http://ratings.default.svc.cluster.local:9080/，Outbound handler 的作用是将 iptables 拦截到的本地应用程序发出的流量，经由 Envoy 判断如何路由到 upstream。 应用程序容器发出的请求为 Outbound 流量，被 iptables 劫持后转移给 Envoy Outbound handler 处理，然后经过 virtual Listener、0.0.0.0_9080 Listener，然后通过 Route 9080 找到 upstream 的 cluster，进而通过 EDS 找到 Endpoint 执行路由动作。 Route 9080 reviews 会请求 ratings 服务，运行 istioctl proxy-config routes reviews-v1-cb8655c75-b97zc --name 9080 -o json 查看 route 配置，因为 Envoy 会根据 HTTP header 中的 domains 来匹配 VirtualHost，所以下面只列举了 ratings.default.svc.cluster.local:9080 这一个 VirtualHost。 [{ \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.254.234.130\", \"10.254.234.130:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0.000s\", \"maxGrpcTimeout\": \"0.000s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" }, \"perFilterConfig\": {... } } ] }, ..] 从该 Virtual Host 配置中可以看到将流量路由到 Cluster outbound|9080||ratings.default.svc.cluster.local。 Endpoint outbound|9080||ratings.default.svc.cluster.local Istio 1.1 以前版本不支持使用 istioctl 命令直接查询 Cluster 的 Endpoint，可以使用查询 Pilot 的 debug 端点的方式折中。 kubectl exec reviews-v1-cb8655c75-b97zc -c istio-proxy curl http://istio-pilot.istio-system.svc.cluster.local:9093/debug/edsz > endpoints.json endpoints.json 文件中包含了所有 Cluster 的 Endpoint 信息，我们只选取其中的 outbound|9080||ratings.default.svc.cluster.local Cluster 的结果如下。 { \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } Endpoint 可以是一个或多个，Envoy 将根据一定规则选择适当的 Endpoint 来路由。 注：Istio 1.1 将支持 istioctl pc endpoint 命令来查询 Endpoint。 参考 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io Istio流量管理实现机制深度解析 - zhaohuabing.com Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/":{"url":"data-plane/","title":"数据平面概述","keywords":"","body":"数据平面概述 Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据平面，例如蚂蚁开源的 MOSN。因为本书的重点是介绍 Istio，因此笔者不会在这一章节重点着墨，感兴趣的读者可以阅读 Envoy 基础教程进行进一步的学习。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-terminology.html":{"url":"data-plane/envoy-terminology.html","title":"Envoy 中的基本术语","keywords":"","body":"Envoy 中的基本术语 在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样在阅读后续的章节时，可以保证我们的思想一致。在这一节我将为你介绍 Envoy 中常用的术语。 Envoy 中常用术语有： 主机（Host）：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。 下游（Downstream）：下游主机连接到 Envoy，主动向 Envoy 发送请求并期待获得响应。 上游（Upstream）：上游主机收到来自 Envoy 的连接请求同时相应该请求。 集群（Cluster）: 集群是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。 网格（Mesh）：网格是一组互相协调以提供一致网络拓扑的主机。Envoy 网格是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。 运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或 更改 Envoy 主配置的情况下，通过更改设置来影响操作。 监听器（Listener）: 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix 域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。 监听器过滤器（Listener filter）：Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。 HTTP 路由表（HTTP Route Table）：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。 健康检查（Health checking）：健康检查会与 SDS 服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/istio-sidecar-proxy-config.html":{"url":"data-plane/istio-sidecar-proxy-config.html","title":"Istio sidecar proxy 配置","keywords":"","body":"Istio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio，再部署 bookinfo 示例，那么在 default 命名空间下有一个名字类似于 ratings-v1-7c9949d479-dwkr4 的 Pod，使用下面的命令查看该 Pod 的 Envoy sidecar 的全量配置： kubectl -n default exec ratings-v1-7c9949d479-dwkr4 -c istio-proxy curl http://localhost:15000/config_dump > dump-rating.json 将 Envoy 的运行时配置 dump 出来之后你将看到一个长 6000 余行的配置文件。 下图展示的是 Envoy 的配置。 图 3.3.1：Envoy 配置 Istio 会在为 Service Mesh 中的每个 Pod 注入 Sidecar 的时候同时为 Envoy 注入 Bootstrap 配置，其余的配置是通过 Pilot 下发的，注意整个数据平面即 Service Mesh 中的 Envoy 的动态配置应该是相同的。您也可以使用上面的命令检查其他 sidecar 的 Envoy 配置是否跟最上面的那个相同。 使用下面的命令检查 Service Mesh 中的所有有 Sidecar 注入的 Pod 中的 proxy 配置是否同步。 $ istioctl proxy-status PROXY CDS LDS EDS RDS PILOT VERSION details-v1-876bf485f-sx7df.default SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-5bf6d97f79-6lz4x 1.0.0 ... istioctl 这个命令行工具就像 kubectl 一样有很多神器的魔法，通过它可以高效的管理 Istio 和 debug。 参考 kubernetes-vagrant-centos-cluster - github.com Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-proxy-config-deep-dive.html":{"url":"data-plane/envoy-proxy-config-deep-dive.html","title":"Envoy proxy 配置详解","keywords":"","body":"Envoy proxy 配置详解 Istio envoy sidecar proxy 配置中包含以下四个部分。 bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。 每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。 Envoy 的配置 dump 出来后的结构如下图所示。 图 3.4.1：Envoy 配置 由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。 图 3.4.2：bootstrap 配置 上图是 bootstrap 的配置信息。 Bootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener或 Cluster 这些资源既可以从静态资源配置中获得也可以从动态资源中配置的 LDS 或 CDS 之类的 xDS 服务获取。 Listener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。 Listener 的特点 每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。 Listener 的数据结构 Listener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。 { \"name\": \"...\", \"address\": \"{...}\", \"filter_chains\": [], \"use_original_dst\": \"{...}\", \"per_connection_buffer_limit_bytes\": \"{...}\", \"metadata\": \"{...}\", \"drain_type\": \"...\", \"listener_filters\": [], \"transparent\": \"{...}\", \"freebind\": \"{...}\", \"socket_options\": [], \"tcp_fast_open_queue_length\": \"{...}\", \"bugfix_reverse_write_filter_order\": \"{...}\" } 下面是关于上述数据结构中的常用配置解析。 name：该 listener 的 UUID，唯一限定名，默认60个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。 address：监听的逻辑/物理地址和端口号，例如 \"address\": { \"socket_address\": { \"address\": \"10.254.74.159\", \"port_value\": 15011 } } filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Envoy 会根据该配置顺序执行 filter。 use_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是，Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener。 Cluster Cluster 是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现 cluster 的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到 cluster 的哪个成员。 Cluster 的特点 一组逻辑上相同的主机构成一个 cluster。 可以在 cluster 中定义各种负载均衡策略。 新加入的 cluster 需要一个热身的过程才可以给路由引用，该过程是原子的，即在 cluster 热身之前对于 Envoy 及 Service Mesh 的其余部分来说是不可见的。 可以通过多种方式来配置 cluster，例如静态类型、严格限定 DNS、逻辑 DNS、EDS 等。 Cluster 的数据结构 Cluster 的数据结构如下，除了 name 字段，其他都是可选的。 { \"name\": \"...\", \"alt_stat_name\": \"...\", \"type\": \"...\", \"eds_cluster_config\": \"{...}\", \"connect_timeout\": \"{...}\", \"per_connection_buffer_limit_bytes\": \"{...}\", \"lb_policy\": \"...\", \"hosts\": [], \"load_assignment\": \"{...}\", \"health_checks\": [], \"max_requests_per_connection\": \"{...}\", \"circuit_breakers\": \"{...}\", \"tls_context\": \"{...}\", \"common_http_protocol_options\": \"{...}\", \"http_protocol_options\": \"{...}\", \"http2_protocol_options\": \"{...}\", \"extension_protocol_options\": \"{...}\", \"dns_refresh_rate\": \"{...}\", \"dns_lookup_family\": \"...\", \"dns_resolvers\": [], \"outlier_detection\": \"{...}\", \"cleanup_interval\": \"{...}\", \"upstream_bind_config\": \"{...}\", \"lb_subset_config\": \"{...}\", \"ring_hash_lb_config\": \"{...}\", \"original_dst_lb_config\": \"{...}\", \"least_request_lb_config\": \"{...}\", \"common_lb_config\": \"{...}\", \"transport_socket\": \"{...}\", \"metadata\": \"{...}\", \"protocol_selection\": \"...\", \"upstream_connection_options\": \"{...}\", \"close_connections_on_host_health_failure\": \"...\", \"drain_connections_on_host_removal\": \"...\" } 下面是关于上述数据结构中的常用配置解析。 name：如果你留意到作为 Sidecar 启动的 Envoy 的参数的会注意到 --max-obj-name-len 189，该选项用来用来指定 cluster 的名字，例如 inbound|9080||ratings.default.svc.cluster.local。该名字字符串由 | 分隔成四个部分，分别是 inbound 或 outbound 代表入向流量或出向流量、端口号、subcluster 名称、FQDN，其中 subcluster 名称将对应于 Istio DestinationRule 中配置的 subnet，如果是按照多版本按比例路由的话，该值可以是版本号。 type：即服务发现类型，支持的参数有 STATIC（缺省值）、STRICT_DNS、LOGICAL_DNS、EDS、ORIGINAL_DST。 hosts：这是个列表，配置负载均衡的 IP 地址和端口，只有使用了 STATIC、STRICT_DNS、LOGICAL_DNS 服务发现类型时才需要配置。 eds_cluster_config：如果使用 EDS 做服务发现，则需要配置该项目，其中包括的配置有 service_name 和 ads。 Route 我们在这里所说的路由指的是 HTTP 路由，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。 HTTP 路由的特点 前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由。 基于优先级的路由。 基于哈希策略的路由。 Route 的数据结构 { \"name\": \"...\", \"virtual_hosts\": [], \"internal_only_headers\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [], \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"validate_clusters\": \"{...}\" } 下面是关于上述数据结构中的常用配置解析。 name：该名字跟 envoy.http_connection_manager filter 中的 http_filters.rds.route_config_name 一致，在 Istio Service Mesh 中为 Envoy 下发的配置中的 Route 是以监听的端口号作为名字，而同一个名字下面的 virtual_hosts 可以有多个值（数组形式）。 virtual_hosts：因为 VirtualHosts 是 Envoy 中引入的一个重要概念，我们在下文将详细说明 virtual_hosts 的数据结构。 validate_clusters：这是一个布尔值，用来设置开启使用 cluster manager 来检测路由表引用的 cluster 是否有效。如果是路由表是通过 route_config 静态配置的则该值默认设置为 true，如果是使用 rds 动态配置的话，则该值默认设置为 false。 route.VirtualHost VirtualHost 即上文中 Route 配置中的 virtual_hosts，VirtualHost 是路由配置中的顶级元素。每个虚拟主机都有一个逻辑名称以及一组根据传入请求的 host header 路由到它的域。这允许单个 Listener 为多个顶级域路径树提供服务。基于域选择了虚拟主机后 Envoy 就会处理路由以查看要路由到哪个上游集群或是否执行重定向。 VirtualHost 的数据结构 下面是 VirtualHost 的数据结构，除了 name 和 domains 是必须配置项外，其他皆为可选项。 { \"name\": \"...\", \"domains\": [], \"routes\": [], \"require_tls\": \"...\", \"virtual_clusters\": [], \"rate_limits\": [], \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [], \"cors\": \"{...}\", \"per_filter_config\": \"{...}\", \"include_request_attempt_count\": \"...\" } 下面是关于上述数据结构中的常用配置解析。 name：该 VirtualHost 的名字，一般是 FQDN 加端口，如 details.default.svc.cluster.local:9080。 domains：这是个用来匹配 VirtualHost 的域名（host/authority header）列表，也可以使用通配符，但是通配符不能匹配空字符，除了仅使用 * 作为 domains，注意列表中的值不能重复和存在交集，只要有一条 domain 被匹配上了，就会执行路由。Istio 会为该值配置所有地址解析形式，包括 IP 地址、FQDN 和短域名等。 routes：针对入口流量的有序路由列表，第一个匹配上的路由将被执行。我们在下文将详细说明 route 的数据结构。 下面是一个实际的 VirtualHost 的例子，该配置来自 Bookinfo 应用的 details 应用的 Sidecar 服务。 { \"name\": \"details.default.svc.cluster.local:9080\", \"domains\": [ \"details.default.svc.cluster.local\", \"details.default.svc.cluster.local:9080\", \"details\", \"details:9080\", \"details.default.svc.cluster\", \"details.default.svc.cluster:9080\", \"details.default.svc\", \"details.default.svc:9080\", \"details.default\", \"details.default:9080\", \"10.254.4.113\", \"10.254.4.113:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||details.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"details.default.svc.cluster.local:9080/*\" }, \"per_filter_config\": { \"mixer\": { \"forward_attributes\": { \"attributes\": { \"destination.service.uid\": { \"string_value\": \"istio://default/services/details\" }, \"destination.service.host\": { \"string_value\": \"details.default.svc.cluster.local\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.name\": { \"string_value\": \"details\" }, \"destination.service\": { \"string_value\": \"details.default.svc.cluster.local\" } } }, \"mixer_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"details.default.svc.cluster.local\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/details\" }, \"destination.service.name\": { \"string_value\": \"details\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service\": { \"string_value\": \"details.default.svc.cluster.local\" } } }, \"disable_check_calls\": true } } } ] } route.Route 路由既是如何匹配请求的规范，也是对下一步做什么的指示（例如，redirect、forward、rewrite等）。 route.Route 的数据结构 下面是是 route.Route 的数据结构，除了 match 之外其余都是可选的。 { \"match\": \"{...}\", \"route\": \"{...}\", \"redirect\": \"{...}\", \"direct_response\": \"{...}\", \"metadata\": \"{...}\", \"decorator\": \"{...}\", \"per_filter_config\": \"{...}\", \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [] } 下面是关于上述数据结构中的常用配置解析。 match：路由匹配参数。例如 URL prefix（前缀）、path（URL 的完整路径）、regex（规则表达式）等。 route：这里面配置路由的行为，可以是 route、redirect 和 direct_response，不过这里面没有专门的一个配置项用来配置以上三种行为，而是根据实际填充的配置项来确定的。例如在此处添加 cluster 配置则暗示路由动作为”route“，表示将流量路由到该 cluster。 decorator：被匹配的路由的修饰符，表示被匹配的虚拟主机和 URL。该配置里有且只有一个必须配置的项 operation，例如 details.default.svc.cluster.local:9080/*。 per_filter_config：这是一个 map 类型，per_filter_config 字段可用于为 filter 提供特定路由的配置。Map 的 key 应与 filleter 名称匹配，例如用于 HTTP buffer filter 的 envoy.buffer。 参考 Envoy 中文文档 - cloudnative.to Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-api.html":{"url":"data-plane/envoy-api.html","title":"Envoy API","keywords":"","body":"Envoy API Envoy 提供了如下的 API： CDS（Cluster Discovery Service）：集群发现服务 EDS（Endpoint Discovery Service）：端点发现服务 HDS（Health Discovery Service）：健康发现服务 LDS（Listener Discovery Service）：监听器发现服务 MS（Metric Service）：将 metric 推送到远端服务器 RLS（Rate Limit Service）：速率限制服务 RDS（Route Discovery Service）：路由发现服务 SDS（Secret Discovery Service）：秘钥发现服务 所有名称以 DS 结尾的服务统称为 xDS。 本书中仅讨论 v2 版本的 API，因为 Envoy 仍在不断开发和完善中，随着版本迭代也有可能新增一些 API，本章的重点在于 xDS 协议。 Envoy xDS 协议 Envoy xDS 为 Istio 控制平面与数据平面通信的基本协议，只要代理支持该协议表达形式就可以创建自己 Sidecar 来替换 Envoy。这一章中将带大家了解 Envoy xDS。 Envoy 是 Istio Service Mesh 中默认的 Sidecar，Istio 在 Envoy 的基础上按照 Envoy 的 xDS 协议扩展了其控制平面，在讲到 Envoy xDS 协议之前还需要我们先熟悉下 Envoy 的基本术语。下面列举了 Envoy 里的基本术语及其数据结构解析，关于 Envoy 的详细介绍请参考 Envoy 文档，至于 Envoy 在 Service Mesh（不仅限于 Istio） 中是如何作为转发代理工作的请参考网易云刘超的这篇深入解读 Service Mesh 背后的技术细节 以及理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持，本文引用其中的一些观点，详细内容不再赘述。 关于 xDS 的版本 有一点需要大家注意，就是 Envoy 的 API 有 v1 和 v2 两个版本，从 Envoy 1.5.0 起 v2 API 就已经生产就绪了，为了能够让用户顺利的向 v2 版本的额 API 过度，Envoy 启动的时候设置了一个 --v2-config-only 的标志，Envoy 不同版本对 v1/v2 API 的支持详情请参考 Envoy v1 配置废弃时间表。 Envoy 的作者 Matt Klein 在 Service Mesh 中的通用数据平面 API 设计这篇文章中说明了 Envoy API v1 的历史及其缺点，还有 v2 的引入。v2 API 是 v1 的演进，而不是革命，它是 v1 功能的超集。 在 Istio 1.0 及以上版本中使用的是 Envoy 1.8.0-dev 版本，其支持 v2 的 API，同时在 Envoy 作为 Sidecar proxy 启动的使用使用了例如下面的命令： $ /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster ratings --service-node sidecar~172.33.14.2~ratings-v1-8558d4458d-ld8x9.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only 上面是都 Bookinfo 示例中的 rating pod 中的 sidecar 启动的分析，可以看到其中指定了 --v2-config-only，表明 Istio 1.0+ 只支持 xDS v2 的 API。 REST-JSON & gPRC API 单个的基本 xDS 订阅服务，如 CDS、EDS、LDS、RDS、SDS 同时支持 REST-JSON 和 gRPC API 配置。高级 API，如 HDS、ADS 和 EDS 多维 LB 仅支持 gRPC。这是为了避免将复杂的双向流语义映射到 REST。 参考 Service Mesh 中的通用数据平面 API 设计 - cloudnative.to Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-xds-protocol.html":{"url":"data-plane/envoy-xds-protocol.html","title":"xDS 协议解析","keywords":"","body":"xDS 协议解析 Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL。后两种方式会发送 DiscoveryRequest 请求消息，发现的对应资源则包含在响应消息 DiscoveryResponse 中。下面，我们将具体讨论每种订阅类型。 文件订阅 发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。Envoy 使用 inotify（Mac OS X 上为 kqueue）来监控文件的变化，在文件被更新时，Envoy 读取保存的 DiscoveryResponse 数据进行解析，数据格式可以为二进制 protobuf、JSON、YAML 和协议文本等。 译者注：core.ConfigSource 配置格式如下： { \"path\": \"...\", \"api_config_source\": \"{...}\", \"ads\": \"{...}\" } 文件订阅方式可提供统计数据和日志信息，但是缺少 ACK/NACK 更新的机制。如果更新的配置被拒绝，xDS API 则继续使用最后一个的有效配置。 gRPC 流式订阅 单资源类型发现 每个 xDS API 可以单独配置 ApiConfigSource，指向对应的上游管理服务器的集群地址。每个 xDS 资源类型会启动一个独立的双向 gRPC 流，可能对应不同的管理服务器。API 交付方式采用最终一致性。可以参考后续聚合服务发现（ADS） 章节来了解必要的显式控制序列。 译者注：core.ApiConfigSource 配置格式如下： { \"api_type\": \"...\", \"cluster_names\": [], \"grpc_services\": [], \"refresh_delay\": \"{...}\", \"request_timeout\": \"{...}\" } 类型 URL 每个 xDS API 都与给定的资源的类型存在 1:1 对应。关系如下： LDS： envoy.api.v2.Listener RDS： envoy.api.v2.RouteConfiguration CDS： envoy.api.v2.Cluster EDS： envoy.api.v2.ClusterLoadAssignment SDS：envoy.api.v2.Auth.Secret 类型 URL 的概念如下所示，其采用 type.googleapis.com/ 的形式，例如 CDS 对应于 type.googleapis.com/envoy.api.v2.Cluster。在 Envoy 的请求和管理服务器的响应中，都包括了资源类型 URL。 ACK/NACK 和版本 每个 Envoy 流以 DiscoveryRequest 开始，包括了列表订阅的资源、订阅资源对应的类型 URL、节点标识符和空的 version_info。EDS 请求示例如下： version_info: node: { id: envoy } resource_names: - foo - bar type_url: type.googleapis.com/envoy.api.v2.ClusterLoadAssignment response_nonce: 管理服务器可立刻或等待资源就绪时发送 DiscoveryResponse作为响应，示例如下： version_info: X resources: - foo ClusterLoadAssignment proto encoding - bar ClusterLoadAssignment proto encoding type_url: type.googleapis.com/envoy.api.v2.ClusterLoadAssignment nonce: A Envoy 在处理 DiscoveryResponse 响应后，将通过流发送一个新的请求，请求包含应用成功的最后一个版本号和管理服务器提供的 nonce。如果本次更新已成功应用，则 version_info 的值设置为 X，如下序列图所示： 图 3.6.1：ACK 后的版本更新 在此序列图及后续中，将统一使用以下缩写格式： DiscoveryRequest： (V=version_info，R=resource_names，N=response_nonce，T=type_url) DiscoveryResponse： (V=version_info，R=resources，N=nonce，T=type_url) 译者注：在信息安全中，Nonce是一个在加密通信只能使用一次的数字。在认证协议中，它往往是一个随机或伪随机数，以避免重放攻击。Nonce也用于流密码以确保安全。如果需要使用相同的密钥加密一个以上的消息，就需要Nonce来确保不同的消息与该密钥加密的密钥流不同。（引用自维基百科）在本文中nonce是每次更新的数据包的唯一标识。 版本为 Envoy 和管理服务器提供了共享当前应用配置的概念和通过 ACK/NACK 来进行配置更新的机制。如果 Envoy 拒绝配置更新 X，则回复 error_detail 及前一个的版本号，在当前情况下为空的初始版本号，error_detail 包含了有关错误的更加详细的信息： 图 3.6.2：NACK 无版本更新 后续，API 更新可能会在新版本 Y 上成功： 图 3.6.3：ACK 紧接着 NACK 每个流都有自己的版本概念，但不存在跨资源类型的共享版本。在不使用 ADS 的情况下，每个资源类型可能具有不同的版本，因为 Envoy API 允许指向不同的 EDS/RDS 资源配置并对应不同的 ConfigSources。 何时发送更新 管理服务器应该只向 Envoy 客户端发送上次 DiscoveryResponse 后更新过的资源。Envoy 则会根据接受或拒绝 DiscoveryResponse 的情况，立即回复包含 ACK/NACK 的 DiscoveryRequest 请求。如果管理服务器每次发送相同的资源集结果，而不是根据其更新情况，则会导致 Envoy 和管理服务器通讯效率大打折扣。 在同一个流中，新的 DiscoveryRequests 将取代此前具有相同的资源类型 DiscoveryRequest 请求。这意味着管理服务器只需要响应给定资源类型最新的 DiscoveryRequest 请求即可。 资源提示 DiscoveryRequest 中的 resource_names 信息作为资源提示出现。一些资源类型，例如 Cluster 和 Listener 将使用一个空的 resource_names，因为 Envoy 需要获取管理服务器对应于节点标识的所有 Cluster（CDS）和 Listener（LDS）。对于其他资源类型，如 RouteConfigurations（RDS）和 ClusterLoadAssignments（EDS），则遵循此前的 CDS/LDS 更新，Envoy 能够明确地枚举这些资源。 LDS/CDS 资源提示信息将始终为空，并且期望管理服务器的每个响应都提供 LDS/CDS 资源的完整状态。缺席的 Listener 或 Cluster 将被删除。 对于 EDS/RDS，管理服务器并不需要为每个请求的资源进行响应，而且还可能提供额外未请求的资源。resource_names 只是一个提示。Envoy 将默默地忽略返回的多余资源。如果请求的资源中缺少相应的 RDS 或 EDS 更新，Envoy 将保留对应资源的最后的值。管理服务器可能会依据 DiscoveryRequest 中 node 标识推断其所需的 EDS/RDS 资源，在这种情况下，提示信息可能会被丢弃。从相应的角度来看，空的 EDS/RDS DiscoveryResponse 响应实际上是表明在 Envoy 中为一个空的资源。 当 Listener 或 Cluster 被删除时，其对应的 EDS 和 RDS 资源也需要在 Envoy 实例中删除。为使 EDS 资源被 Envoy 已知或跟踪，就必须存在应用过的 Cluster 定义（如通过 CDS 获取）。RDS 和 Listeners 之间存在类似的关系（如通过 LDS 获取）。 对于 EDS/RDS ，Envoy 可以为每个给定类型的资源生成不同的流（如每个 ConfigSource 都有自己的上游管理服务器的集群）或当指定资源类型的请求发送到同一个管理服务器的时候，允许将多个资源请求组合在一起发送。虽然可以单个实现，但管理服务器应具备处理每个给定资源类型中对单个或多个 resource_names 请求的能力。下面的两个序列图对于获取两个 EDS 资源都是有效的 {foo，bar}： 图 3.6.4：一个流上多个 EDS 请求 图 3.6.5：不同流上的多个 EDS 请求 资源更新 如上所述，Envoy 可能会更新 DiscoveryRequest 中出现的 resource_names 列表，其中 DiscoveryRequest 是用来 ACK/NACK 管理服务器的特定的 DiscoveryResponse 。此外，Envoy 后续可能会发送额外的 DiscoveryRequests ，用于在特定 version_info 上使用新的资源提示来更新管理服务器。例如，如果 Envoy 在 EDS 版本 X 时仅知道集群 foo，但在随后收到的 CDS 更新时额外获取了集群 bar ，它可能会为版本 X 发出额外的 DiscoveryRequest 请求，并将 {foo，bar} 作为请求的 resource_names 。 图 3.6.6：CDS 响应导致 EDS 资源更新 这里可能会出现竞争状况；如果 Envoy 在版本 X 上发布了资源提示更新请求，但在管理服务器处理该请求之前发送了新的版本号为 Y 的响应，针对 version_info 为 X 的版本，资源提示更新可能会被解释为拒绝 Y 。为避免这种情况，通过使用管理服务器提供的 nonce，Envoy 可用来保证每个 DiscoveryRequest 对应到相应的 DiscoveryResponse ： 图 3.6.7：EDS 更新速率激发 nonces 管理服务器不应该为含有过期 nonce 的 DiscoveryRequest 发送 DiscoveryResponse 响应。在向 Envoy 发送的 DiscoveryResponse 中包含了的新 nonce ，则此前的 nonce 将过期。在资源新版本就绪之前，管理服务器不需要向 Envoy 发送更新。同版本的早期请求将会过期。在新版本就绪时，管理服务器可能会处理同一个版本号的多个 DiscoveryRequests请求。 图 3.6.8：请求变的陈旧 上述资源更新序列表明 Envoy 并不能期待其发出的每个 DiscoveryRequest 都得到 DiscoveryResponse 响应。 最终一致性考虑 由于 Envoy 的 xDS API 采用最终一致性，因此在更新期间可能导致流量被丢弃。例如，如果通过 CDS/EDS 仅获取到了集群 X，而且 RouteConfiguration 引用了集群 X；在 CDS/EDS 更新集群 Y 配置之前，如果将 RouteConfiguration 将引用的集群调整为 Y ，那么流量将被吸入黑洞而丢弃，直至集群 Y 被 Envoy 实例获取。 对某些应用程序，可接受临时的流量丢弃，客户端重试或其他 Envoy sidecar 会掩盖流量丢弃。那些对流量丢弃不能容忍的场景，可以通过以下方式避免流量丢失，CDS/EDS 更新同时携带 X 和 Y ，然后发送 RDS 更新从 X 切换到 Y ，此后发送丢弃 X 的 CDS/EDS 更新。 一般来说，为避免流量丢弃，更新的顺序应该遵循 make before break 模型，其中 必须始终先推送 CDS 更新（如果有）。 EDS 更新（如果有）必须在相应集群的 CDS 更新后到达。 LDS 更新必须在相应的 CDS/EDS 更新后到达。 与新添加的监听器相关的 RDS 更新必须在最后到达。 最后，删除过期的 CDS 集群和相关的 EDS 端点（不再被引用的端点）。 如果没有新的集群/路由/监听器或者允许更新时临时流量丢失的情况下，可以独立推送 xDS 更新。请注意，在 LDS 更新的情况下，监听器须在接收流量之前被预热，例如如其配置了依赖的路由，则先需先从 RDS 进行获取。添加/删除/更新集群信息时，集群也需要进行预热。另一方面，如果管理平面确保路由更新时所引用的集群已经准备就绪，路由可以不用预热。 聚合服务发现（ADS） 当管理服务器进行资源分发时，通过上述保证交互顺序的方式来避免流量丢弃是一项很有挑战的工作。ADS 允许单一管理服务器通过单个 gRPC 流，提供所有的 API 更新。配合仔细规划的更新顺序，ADS 可规避更新过程中流量丢失。使用 ADS，在单个流上可通过类型 URL 来进行复用多个独立的 DiscoveryRequest/DiscoveryResponse 序列。对于任何给定类型的 URL，以上 DiscoveryRequest 和 DiscoveryResponse 消息序列都适用。 更新序列可能如下所示： 图 3.6.9：EDS/CDS 在一个 ADS 流上多路复用 每个 Envoy 实例可使用单独的 ADS 流。 最小化 ADS 配置的 bootstrap.yaml 片段示例如下： node: id: dynamic_resources: cds_config: {ads: {}} lds_config: {ads: {}} ads_config: api_type: GRPC grpc_services: envoy_grpc: cluster_name: ads_cluster static_resources: clusters: - name: ads_cluster connect_timeout: { seconds: 5 } type: STATIC hosts: - socket_address: address: port_value: lb_policy: ROUND_ROBIN http2_protocol_options: {} admin: ... 增量 xDS 增量 xDS 是可用于允许的 ADS、CDS 和 RDS 单独 xDS 端点： xDS 客户端对跟踪资源列表进行增量更新。这支持 Envoy 按需/惰性地请求额外资源。例如，当与未知集群相对应的请求到达时，可能会发生这种情况。 xDS 服务器可以增量更新客户端上的资源。这支持 xDS 资源可伸缩性的目标。管理服务器只需交付更改的单个集群，而不是在修改单个集群时交付所有上万个集群。 xDS 增量会话始终位于 gRPC 双向流的上下文中。这允许 xDS 服务器能够跟踪到连接的 xDS 客户端的状态。xDS REST 版本不支持增量。 在增量 xDS 中，nonce 字段是必需的，用于匹配 IncrementalDiscoveryResponse 关联的 ACK 或 NACK IncrementalDiscoveryRequest。可选地，存在响应消息级别的 system_version_info，但仅用于调试目的。 IncrementalDiscoveryRequest 可在以下 3 种情况下发送： xDS 双向 gRPC 流的初始消息。 作为对先前的 IncrementalDiscoveryResponse 的 ACK 或 NACK 响应。在这种情况下，response_nonce 被设置为响应中的 nonce 值。ACK 或 NACK 由可由 error_detail 字段是否出现来区分。 客户端自发的 IncrementalDiscoveryRequest。此场景下可以采用动态添加或删除被跟踪的 resource_names 集。这种场景下，必须忽略 response_nonce。 在第一个示例中，客户端连接并接收它的第一个更新并 ACK。第二次更新失败，客户端发送 NACK 拒绝更新。xDS客户端后续会自发地请求 “wc” 相关资源。 图 3.6.10：增量 session 示例 在重新连接时，支持增量的 xDS 客户端可能会告诉服务器其已知资源从而避免通过网络重新发送它们。 图 3.6.11：增量重连示例 REST-JSON 轮询订阅 单个 xDS API 可对 REST 端点进行同步（长）轮询。除了无持久流与管理服务器交互外，消息顺序与上述相似。在任何时间点，只存在一个未完成的请求，因此响应消息中的 nonce 在 REST-JSON 中是可选的。DiscoveryRequest 和 DiscoveryResponse 的消息编码遵循 JSON 变换 proto3 规范。ADS 不支持 REST-JSON 轮询。 当轮询期间设置为较小的值时，则可以等同于长轮询，这时要求避免发送 DiscoveryResponse，除非对请求的资源发生了更改。 参考 本文译自 xDS REST and gRPC protocol，译者：狄卫华，审校：宋净超 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-lds.html":{"url":"data-plane/envoy-lds.html","title":"LDS（监听器发现服务）","keywords":"","body":"LDS（监听器发现服务） Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。 Listener 更新的语义如下： 每个 Listener 必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的 Listener ，管理服务必须提供 Listener 的唯一名称。 当一个 Listener 被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果 Listener 引用 RDS 配置，那么在 Listener 迁移到 “active” 之前，将会解析并提取该配置。 Listener 一旦创建，实际上就会保持不变。因此，更新 Listener 时，会创建一个全新的 Listener （使用相同的监听套接字）。新增加的 Listener 都会通过上面所描述的相同“预热”过程。 当更新或删除 Listener 时，旧的 Listener 将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。Listener 移除之后，该 Listener 所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。 注意 Envoy 从 1.9 版本开始已不再支持 v1 API。 统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接 参考 Listener discovery service(LDS) - envoyproxy.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-rds.html":{"url":"data-plane/envoy-rds.html","title":"RDS（路由发现服务）","keywords":"","body":"RDS（路由发现服务） 路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。 注意：Envoy 从 1.9 版本开始已不再支持 v1 API。 统计 RDS 的统计树以 http..rds..*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 参考 Route discovery service(RDS) - envoyproxy.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-cds.html":{"url":"data-plane/envoy-cds.html","title":"CDS（集群发现服务）","keywords":"","body":"CDS（集群发现服务） 集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取集群管理器的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。 关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取的 cluster 配置，请参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）一文中的 CDS 服务部分。 注意 在 Envoy 配置中静态定义的 cluster 不能通过 CDS API 进行修改或删除。 Envoy 从 1.9 版本开始已不再支持 v1 API。 统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接 参考 Cluster discovery service - envoyproxy.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-eds.html":{"url":"data-plane/envoy-eds.html","title":"EDS（端点发现服务）","keywords":"","body":"EDS（端点发现服务） EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。 Endpoint Endpoint 即上游主机标识。它的数据结构如下： { \"address\": \"{...}\", \"health_check_config\": \"{...}\" } 其中包括端点的地址和健康检查配置。详情请参考 Endpoints。 终端发现服务（EDS）是一个基于 gRPC 或 REST-JSON API 服务器的 xDS 管理服务，在 Envoy 中用来获取集群成员。集群成员在 Envoy 的术语中被称为“终端”。对于每个集群，Envoy 都会通过发现服务来获取成员的终端。由于以下几个原因，EDS 是首选的服务发现机制： Envoy 对每个上游主机都有明确的了解（与通过 DNS 解析的负载均衡进行路由相比而言），并可以做出更智能的负载均衡决策。 在每个主机的发现 API 响应中携带的额外属性通知 Envoy 负载均衡权重、金丝雀状态、区域等。这些附加属性在负载均衡、统计信息收集等过程中会被 Envoy 网格全局使用。 Envoy 提供了 Java 和 Go 语言版本的 EDS 和其他发现服务的参考 gRPC 实现。 通常，主动健康检查与最终一致的服务发现服务数据结合使用，以进行负载均衡和路由决策。 参考 服务发现 - cloudnative.to Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-sds.html":{"url":"data-plane/envoy-sds.html","title":"SDS（秘钥发现服务）","keywords":"","body":"SDS（秘钥发现服务） SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secrets 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。 SDS 带来的最大的好处就是简化证书管理。要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中。如果证书过期，还需要更新 secret 和需要重新部署代理容器。使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。 如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端口，但是将重置与端口的连接。 上游集群的处理方式类似，如果需要通过 SDS 从远程获取集群客户端证书，则不会将其标记为 active 状态，在获得证书之前也它不会被使用。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则集群将被标记为 active，可以处理请求，但路由到该集群的请求都将被拒绝。 使用 SDS 的静态集群需定义 SDS 集群（除非使用不需要集群的 Google gRPC），则必须在使用静态集群之前定义 SDS 集群。 Envoy 代理和 SDS 服务器之间的连接必须是安全的。可以在同一主机上运行 SDS 服务器，使用 Unix Domain Socket 进行连接。否则，需要代理和 SDS 服务器之间的 mTLS。在这种情况下，必须静态配置 SDS 连接的客户端证书。 SDS Server SDS server 需要实现 SecretDiscoveryService 这个 gRPC 服务。遵循与其他 xDS 相同的协议。 SDS 配置 SDS 支持静态配置也支持动态配置。 静态配置 可以在static_resources 的 secrets 中配置 TLS 证书。 动态配置 从远程 SDS server 获取 secret。 通过 Unix Domain Socket 访问 gRPC SDS server。 通过 UDS 访问 gRPC SDS server。 通过 Envoy gRPC 访问 SDS server。 配置详情请参考 Envoy 官方文档。 参考 Secret discovery service (SDS) - envoyproxy.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"data-plane/envoy-ads.html":{"url":"data-plane/envoy-ads.html","title":"ADS（聚合发现服务）","keywords":"","body":"ADS（聚合发现服务） 虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（如 RDS 和 EDS）可能需要管理多个流并连接到不同的管理服务器。 ADS 通过适当得排序 xDS 可以无中断的更新 Envoy 的配置。例如，假设 foo.com 已映射到集群 X。我们希望将路由表中将该映射更改为在集群 Y。为此，必须首先提供 X、Y 这两个集群的 CDS/EDS 更新。 如果没有 ADS，CDS/EDS/RDS 流可能指向不同的管理服务器，或者位于需要协调的不同 gRPC流连接的同一管理服务器上。EDS 资源请求可以跨两个不同的流分开，一个用于 X，一个用于 Y。ADS 将这些流合并到单个流和单个管理服务器，从而无需分布式同步就可以正确地对更新进行排序。使用 ADS，管理服务器将在单个流上提供 CDS、EDS 和 RDS 更新。 参考 Aggregated Discovery Service - envoyproxy.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"setup/":{"url":"setup/","title":"安装概述","keywords":"","body":"安装概述 有两种方法可以在单个 Kubernetes 集群上安装 Istio： 使用 Istioctl（istioctl） 使用 Istio Operator 在本章中，我们将使用 Istio Operator 在一个 Kubernetes 集群上安装 Istio。 使用 Istioctl 安装 Istioctl 是一个命令行工具，我们可以用它来安装和定制 Istio 的安装。使用该命令行工具，我们生成一个包含所有 Istio 资源的 YAML 文件，然后将其部署到 Kubernetes 集群上。 使用 Istio Operator 安装 与 istioctl 相比，Istio Operator 安装的优势在于，我们不需要手动升级 Istio。相反，我们可以部署 Istio Operator，为你管理安装。我们通过更新一个自定义资源来控制 Operator，而操作员则为你应用配置变化。 生产部署情况如何？ 在决定 Istio 的生产部署模式时，还有一些额外的考虑因素需要牢记。我们可以配置 Istio 在不同的部署模型中运行 —— 可能跨越多个集群和网络，并使用多个控制平面。我们将在高级功能模块中了解其他部署模式、多集群安装以及在虚拟机上运行工作负载。 平台安装指南 Istio 可以安装在不同的 Kubernetes 平台上。关于特定云供应商的最新安装指南，请参考平台安装文档。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"setup/istio-installation.html":{"url":"setup/istio-installation.html","title":"安装 Istio","keywords":"","body":"安装 Istio 要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。 我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes集群。 要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。 我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes 集群。 Minikube Docker Desktop kind MicroK8s 当使用本地 Kubernetes 集群时，确保你的计算机满足 Istio 安装的最低要求（如 16384MB 内存和 4 个 CPU）。另外，确保 Kubernetes 集群的版本是 v1.19.0 或更高。 Minikube 在这次培训中，我们将使用 Minikube 的 Hypervisor。Hypervisor 的选择将取决于你的操作系统。要安装 Minikube 和 Hypervisor，你可以按照安装说明进行。 安装了 Minikube 后，我们可以创建并启动 Kubernetes 集群。下面的命令使用 VirtualBox 管理程序启动一个 Minikube 集群。 minikube start --memory=16384 --cpus=4 --driver=virtualbox 请确保用你所使用的 Hypervisor 的名字替换 -driver=virtualbox。关于可用的选项，见下表。 标志名称 更多信息 hyperkit HyperKit hyperv Hyper-V kvm2 KVM docker Docker podman Podman parallels Parallels virtualbox VirtualBox vmware VMware Fusion 为了检查集群是否正在运行，我们可以使用 Kubernetes CLI，运行 kubectl get nodes 命令。 $ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 151m v1.19.0 注意：如果你使用 Brew 软件包管理器安装了 Minikube，你也安装了 Kubernetes CLI。 Kubernetes CLI 如果你需要安装 Kubernetes CLI，请遵循这些说明。 我们可以运行 kubectl version 来检查 CLI 是否已经安装。你应该看到与此类似的输出。 $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.2\", GitCommit:\"f5743093fd1c663cb0cbc89748f730662345d44d\", GitTreeState:\"clean\", BuildDate:\"2020-09-16T21:51:49Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:23:04Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"} 下载 Istio 在本课程中，我们将使用 Istio 1.10.3。安装 Istio 的第一步是下载 Istio CLI（istioctl）、安装清单、示例和工具。 安装最新版本的最简单方法是使用 downloadIstio 脚本。打开一个终端窗口，打开你要下载 Istio 的文件夹，然后运行下载脚本。 $ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.3 sh - Istio 发行版被下载并解压到名为 istio-1.9.0 的文件夹中。为了访问 istioctl，我们应该把它添加到 path 中。 $ cd istio-1.10.3 $ export PATH=$PWD/bin:$PATH 要检查 istioctl 是否在 path 里，运行 istioctl version。你应该看到这样的输出。 $ istioctl version no running Istio pods in \"istio-system\" 1.10.3 安装 Istio Istio 支持多个配置文件（profile）。配置文件之间的区别在于所安装的组件。 $ istioctl profile list Istio configuration profiles: default demo empty minimal preview remote 推荐用于生产部署的配置文件是 default 配置文件。我们将安装 demo 配置文件，因为它包含所有的核心组件，启用了跟踪和日志记录，并且是为了学习不同的 Istio 功能。 我们也可以从 minimal 的组件开始，以后单独安装其他功能，如 ingress 和 egress 网关。 因为我们将使用 Istio 操作员进行安装，所以我们必须先部署 Operator。 要部署 Istio Operator，请运行： $ istioctl operator init Using operator Deployment image: docker.io/istio/operator:1.9.0 ✔ Istio operator installed ✔ Installation complete init 命令创建了 istio-operator 命名空间，并部署了 CRD、Operator Deployment 以及 operator 工作所需的其他资源。安装完成后，Operator 就可以使用了。 要安装 Istio，我们必须创建 IstioOperator 资源，并指定我们要使用的配置配置文件。 创建一个名为 demo-profile.yaml 的文件，内容如下： apiVersion: v1 kind: Namespace metadata: name: istio-system --- apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: demo-istio-install spec: profile: demo 我们还在文件中添加了命名空间资源，以创建 istio-system 命名空间。 我们需要做的最后一件事是创建资源： $ kubectl apply -f demo-profile.yaml namespace/istio-system created istiooperator.install.istio.io/demo-istio-install created 一旦 Operator 检测到 IstioOperator 资源，它将开始安装 Istio。整个过程可能需要5分钟左右。 为了检查安装的状态，我们可以看看 istio-system 命名空间中的Pod的状态。 $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-egressgateway-6db9994577-sn95p 1/1 Running 0 79s istio-ingressgateway-58649bfdf4-cs4fk 1/1 Running 0 79s istiod-dd4b7db5-nxrjv 1/1 Running 0 111s 当所有的 Pod 都在运行时，Operator 已经完成了 Istio 的安装。 启用 sidecar 注入 正如我们在上一节中所了解的，服务网格需要与每个应用程序一起运行的 sidecar 代理。 要将 sidecar 代理注入到现有的 Kubernetes 部署中，我们可以使用 istioctl 命令中的 kube-inject 动作。 然而，我们也可以在任何 Kubernetes 命名空间上启用自动 sidecar 注入。如果我们用 istio-injection=enabled 标记命名空间，Istio 会自动为我们在该命名空间中创建的任何 Kubernetes Pod 注入 sidecar。 让我们通过添加标签来启用默认命名空间的自动 sidecar 注入。 $ kubectl label namespace default istio-injection=enabled namespace/default labeled 要检查命名空间是否被标记，请运行下面的命令。default 命名空间应该是唯一一个启用了该值的命名空间。 $ kubectl get namespace -L istio-injection NAME STATUS AGE ISTIO-INJECTION default Active 32m enabled istio-operator Active 27m disabled istio-system Active 15m kube-node-lease Active 32m kube-public Active 32m kube-system Active 32m 现在我们可以尝试在 default 命名空间创建一个 Deployment，并观察注入的代理。我们将创建一个名为 my-nginx 的 Deployment，使用 nginx 镜像的单一容器。 $ kubectl create deploy my-nginx --image=nginx deployment.apps/my-nginx created 如果我们看一下 Pod，你会发现 Pod 里有两个容器。 $ kubectl get po NAME READY STATUS RESTARTS AGE my-nginx-6b74b79f57-hmvj8 2/2 Running 0 62s 同样地，描述 Pod 显示 Kubernetes 同时创建了一个 nginx 容器和一个 istio-proxy 容器： $ kubectl describe po my-nginx-6b74b79f57-hmvj8 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 118s default-scheduler Successfully assigned default/my-nginx-6b74b79f57-hmvj8 to minikube Normal Pulling 117s kubelet Pulling image \"docker.io/istio/proxyv2:1.9.0\" Normal Pulled 116s kubelet Successfully pulled image \"docker.io/istio/proxyv2:1.9.0\" in 1.102544635s Normal Created 115s kubelet Created container istio-init Normal Started 115s kubelet Started container istio-init Normal Pulling 115s kubelet Pulling image \"nginx\" Normal Created 78s kubelet Created container nginx Normal Pulled 78s kubelet Successfully pulled image \"nginx\" in 36.157915646s Normal Started 77s kubelet Started container nginx Normal Pulling 77s kubelet Pulling image \"docker.io/istio/proxyv2:1.9.0\" Normal Pulled 76s kubelet Successfully pulled image \"docker.io/istio/proxyv2:1.9.0\" in 1.050876635s Normal Created 76s kubelet Created container istio-proxy Normal Started 76s kubelet Started container istio-proxy 运行下面的命令，删除部署： $ kubectl delete deployment my-nginx deployment.apps \"my-nginx\" deleted 更新和卸载 Istio 如果我们想更新当前的安装或改变配置文件，我们将需要更新先前部署的 IstioOperator 资源。 要删除安装，我们必须删除 IstioOperator，比如说： $ kubectl delete istiooperator -n istio-system demo-istio-install 一旦 Operator 删除了Istio，我们也可以通过运行下面的命令来删除 Operator： $ istioctl operator remove 请确保在删除 Operator 之前先删除 IstioOperator 资源。否则，可能会有剩余的 Istio 资源。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"setup/getmesh.html":{"url":"setup/getmesh.html","title":"GetMesh","keywords":"","body":"GetMesh Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的生命周期和变更管理实践来说可能非常激进。GetMesh 通过针对不同的 Kubernetes 分发版测试所有 Istio 版本以确保功能的完整性来解决这一问题。GetMesh 的 Istio 版本在安全补丁和其他错误更新方面得到积极的支持，并拥有比上游 Istio 提供的更长的支持期。 一些服务网格客户需要支持更高的安全要求。GetMesh 通过提供两种 Istio 发行版来解决合规性问题。 tetrate 发行版，跟踪上游 Istio 并可能应用额外的补丁。 tetratefips 发行版，是符合 FIPS 标准的 tetrate 版本。 如何开始使用？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令来下载最新版本的 GetMesh 和认证的 Istio。 如何开始？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令下载最新版本的 GetMesh 并认证 Istio。 curl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 我们可以运行 version 命令以确保 GetMesh 被成功安装。例如： $ getmesh version getmesh version: 1.1.1 active istioctl: 1.8.3-tetrate-v0 no running Istio pods in \"istio-system\" 版本命令输出 GetMesh 的版本、活跃的 Istio CLI 的版本以及 Kubernetes 集群上安装的 Istio 的版本。 使用 GetMesh 安装 Istio GetMesh 通过 Kubernetes 配置文件与活跃的 Kubernetes 集群进行通信。 要在当前活动的 Kubernetes 集群上安装 Istio 的演示配置文件，我们可以像这样使用 getmesh istioctl 命令： getmesh istioctl install --set profile=demo 该命令将检查集群，以确保它准备好安装 Istio，一旦你确认，安装程序将继续使用选定的配置文件安装 Istio。 如果我们现在检查版本，你会注意到输出显示控制平面和数据平面的版本。 验证配置 config-validate 命令允许你对当前配置和任何尚未应用的 YAML 清单进行验证。 该命令使用外部资源调用一系列验证，如上游 Istio 验证、Kiali 库和 GetMesh 自定义配置检查。 下面是一个命令输出的例子，如果没有标记为 Istio 注入的命名空间。 $ getmesh config-validate Running the config validator. This may take some time... 2021-08-02T19:20:33.873244Z info klog Throttling request took 1.196458809s, request: GET:https://35.185.226.9/api/v1/namespaces/istio-system/configmaps/istio[] NAMESPACE NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE default default Namespace IST0102 Info The namespace is not enabled for Istio injection. Run 'kubectl label namespace default istio-injection=enabled' to enable it, or 'kubectl label namespace default istio-injection=disabled' to explicitly mark it as not needing injection. The error codes of the found issues are prefixed by 'IST' or 'KIA'. For the detailed explanation, please refer to - https://istio.io/latest/docs/reference/config/analysis/ for 'IST' error codes - https://kiali.io/documentation/latest/validations/ for 'KIA' error codes 同样，你也可以传入一个 YAML 文件来验证它，然后再将它部署到集群。例如： $ getmesh config-validate my-resources.yaml 管理多个 Istio CLI 我们可以使用 show 命令来列出当前下载的 Istio 版本： getmesh show 输出如下所示： 1.8.2-tetrate-v0 1.8.3-tetrate-v0 (Active) 如果我们想使用的版本在电脑上没有，我们可以使用 getmesh list 命令来列出所有可信的 Istio 版本： $ getmesh list ISTIO VERSION FLAVOR FLAVOR VERSION K8S VERSIONS *1.9.5 tetrate 0 1.17,1.18,1.19,1.20 1.9.5 istio 0 1.17,1.18,1.19,1.20 1.9.4 tetrate 0 1.17,1.18,1.19,1.20 1.9.4 istio 0 1.17,1.18,1.19,1.20 1.9.0 tetrate 0 1.17,1.18,1.19,1.20 1.9.0 tetratefips 1 1.17,1.18,1.19,1.20 1.9.0 istio 0 1.17,1.18,1.19,1.20 1.8.6 tetrate 0 1.16,1.17,1.18,1.19 1.8.6 istio 0 1.16,1.17,1.18,1.19 1.8.5 tetrate 0 1.16,1.17,1.18,1.19 1.8.5 istio 0 1.16,1.17,1.18,1.19 1.8.3 tetrate 0 1.16,1.17,1.18,1.19 1.8.3 tetratefips 1 1.16,1.17,1.18,1.19 1.8.3 istio 0 1.16,1.17,1.18,1.19 1.7.8 tetrate 0 1.16,1.17,1.18 1.7.8 istio 0 1.16,1.17,1.18 要获取一个特定的版本（比方说1.8.2 tetratefips），我们可以使用 fetch 命令： getmesh fetch --version 1.9.0 --flavor tetratefips --flavor-version 1 当上述命令完成后，GetMesh 将获取的 Istio CLI 版本设置为 Istio CLI 的活动版本。例如，运行 show 命令现在显示 tetratefips 1.9.0 版本是活跃的： $ getmesh show 1.9.0-tetratefips-v1 (Active) 1.9.5-tetrate-v0 同样，如果我们运行 getmesh istioctl version ，我们会发现正在使用的 Istio CLI 的版本： $ getmesh istioctl version client version: 1.9.0-tetratefips-v1 control plane version: 1.9.5-tetrate-v0 data plane version: 1.9.5-tetrate-v0 (2 proxies) 要切换到不同版本的 Istio CLI，我们可以运行 getmesh switch 命令： getmesh switch --version 1.9.5 --flavor tetrate --flavor-version 0 CA 集成 我们没有使用自签的根证书，而是从 GCP CAS（证书授权服务）获得一个中间的 Istio 证书授权（CA）来签署工作负载证书。 假设你已经配置了你的 CAS 实例，你可以用 CA 的参数创建一个 YAML 配置。下面是 YAML 配置的一个例子： providerName: \"gcp\" providerConfig: gcp: # 这将持有你在 GCP 上创建的证书授权的完整 CA 名称 casCAName: \"projects/tetrate-io-istio/locations/us-west1/certificateAuthorities/tetrate-example-io\" certificateParameters: secretOptions: istioCANamespace: \"istio-system\" # `cacerts` secrets 所在的命名空间 overrideExistingCACertsSecret: true # 重写已存在的 `cacerts` secret，使用新的替换 caOptions: validityDays: 365 # CA 到期前的有效天数 keyLength: 2048 # 创建的 key 的比特数 certSigningRequestParams: # x509.CertificateRequest；大部分字段省略 subject: commonname: \"tetrate.example.io\" country: - \"US\" locality: - \"Sunnyvale\" organization: - \"Istio\" organizationunit: - \"engineering\" emailaddresses: - \"youremail@example.io\" 配置到位后，你可以使用 gen-ca 命令来创建 cacert。 getmesh gen-ca --config-file gcp-cas-config.yaml 该命令在 istio-system 中创建 cacerts Kubernetes Secret。为了让 istiod 接受新的 cert，你必须重新启动 istiod。 如果你创建一个 sample 工作负载，并检查所使用的证书，你会发现 CA 是发布工作负载的那个。 Istio CA certs 集成可用于 GCP CA 服务和 AWS Private CA 服务。 发现选择器 发现选择器是 Istio 1.10 中引入的新功能之一。发现选择器允许我们控制 Istio 控制平面观察和发送配置更新的命名空间。 默认情况下，Istio 控制平面会观察和处理集群中所有 Kubernetes 资源的更新。服务网格中的所有 Envoy代理的配置方式是，它们可以到达服务网格中的每个工作负载，并接受与工作负载相关的所有端口的流量。 例如，我们在不同的命名空间部署了两个工作负载——foo 和 bar。尽管我们知道 foo 永远不会与 bar 通信，反之亦然，但一个服务的端点将被包含在另一个服务的已发现端点列表中。 图 4.3.1：Foo and bar 如果我们运行 istioctl proxy-config 命令，列出 foo 命名空间的 foo 工作负载可以看到的所有端点，你会注意到一个名为 bar 的服务条目： $ istioctl proxy-config endpoints deploy/foo.foo ENDPOINT STATUS OUTLIER CHECK CLUSTER … 10.4.1.4:31400 HEALTHY OK outbound|31400||istio-ingressgateway.istio-system.svc.cluster.local 10.4.1.5:80 HEALTHY OK outbound|80||foo.foo.svc.cluster.local 10.4.2.2:53 HEALTHY OK outbound|53||kube-dns.kube-system.svc.cluster.local 10.4.4.2:8383 HEALTHY OK outbound|8383||istio-operator.istio-operator.svc.cluster.local 10.4.4.3:8080 HEALTHY OK outbound|80||istio-egressgateway.istio-system.svc.cluster.local 10.4.4.3:8443 HEALTHY OK outbound|443||istio-egressgateway.istio-system.svc.cluster.local 10.4.4.4:80 HEALTHY OK outbound|80||bar.bar.svc.cluster.local ... 如果 Istio 不断用集群中每个服务的信息来更新代理，即使这些服务是不相关的，我们可以想象这将如何拖累事情。 如果这听起来很熟悉，你可能知道已经有一个解决方案了——Sidecar 资源。 我们将在后面的模块中讨论 Sidecar 资源。 配置发现选择器 发现选择器可以在 MeshConfig 中的 Mesh 层面上进行配置。它们是一个 Kubernetes 选择器的列表，指定了 Istio 在向 sidecar 推送配置时观察和更新的命名空间的集合。 就像 Sidecar 资源一样，discoverySelectors 可以用来限制被 Istio 观察和处理的项目数量。 我们可以更新 IstioOperator 以包括 discoverySelectors 字段，如下所示： apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istio-demo spec: meshConfig: discoverySelectors: - matchLabels: env: test 上面的例子将 env=test 设置为一个匹配标签。这意味着标有 env=test 标签的命名空间中的工作负载将被包含在 Istio 监控和更新的命名空间列表中。 如果我们给 foo 命名空间贴上 env=test 标签，然后列出端点，我们会发现现在配置中列出的端点没有那么多。这是因为我们标注的唯一命名空间是 foo 命名空间，这也是 Istio 控制平面观察和发送更新的唯一命名空间。 $ istioctl proxy-config endpoints deploy/foo.foo ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.4.1.5:80 HEALTHY OK outbound|80||foo.foo.svc.cluster.local 127.0.0.1:15000 HEALTHY OK prometheus_stats 127.0.0.1:15020 HEALTHY OK agent unix://./etc/istio/proxy/SDS HEALTHY OK sds-grpc unix://./etc/istio/proxy/XDS HEALTHY OK xds-grpc 如果我们把命名空间 bar 也贴上标签，然后重新运行 istioctl proxy-config 命令，我们会发现 bar 端点显示为 foo 服务配置的一部分。 $ istioctl proxy-config endpoints deploy/foo.foo ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.4.1.5:80 HEALTHY OK outbound|80||foo.foo.svc.cluster.local 10.4.4.4:80 HEALTHY OK outbound|80||bar.bar.svc.cluster.local 127.0.0.1:15000 HEALTHY OK prometheus_stats 127.0.0.1:15020 HEALTHY OK agent unix://./etc/istio/proxy/SDS HEALTHY OK sds-grpc unix://./etc/istio/proxy/XDS HEALTHY OK xds-grpc Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"setup/istio-observability-tool-kiali.html":{"url":"setup/istio-observability-tool-kiali.html","title":"可观察性工具 kiali","keywords":"","body":"可观察性工具 kiali Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观察性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。 关于 Kiali 单体应用使用微服务架构拆分成了许多微服务的组合。服务的数量显著增加，就对需要了解服务之间的通信模式，例如容错（通过超时、重试、断路等）以及分布式跟踪，以便能够看到服务调用的去向。服务网格可以在平台级别上提供这些服务，并使应用程序编写者从以上繁重的通信模式中解放出来。路由决策在网格级别完成。Kiali 与Istio 合作，可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。 Kiali 提供的功能 Kiali 提供以下功能： 服务拓扑图 分布式跟踪 指标度量收集和图标 配置校验 健康检查和显示 服务发现 下图展示了 kiali 中显示的 Bookinfo 示例的服务拓扑图。 你可以使用 kubernetes-vagrant-centos-cluster 来快速启动一个运行 Kiali 的 Kubernetes 集群。 编译安装与试用 Kilia pod 中运行的进程是 /opt/kiali/kiali -config /kiali-configuration/config.yaml -v 4。 /kiali-configuration/config.yaml 是使用 ConfigMap 挂载进去的，用于配置 Kiali 的 Web 根路径和外部服务地址。 server: port: 20001 web_root: / external_services: jaeger: url: \"http://172.17.8.101:31888\" grafana: url: \"http://grafana.istio-system:3000\" Kiali 中的基本概念 在了解 Kiali 如何提供 Service Mesh 中微服务可观察性之前，我们需要先了解下 Kiali 如何划分监控类别的。 Application：使用运行的工作负载，必须使用 Istio 的将 Label 标记为 app 才算。注意，如果一个应用有多个版本，只要 app 标签的值相同就是属于同一个应用。 Deployment：即 Kubernetes 中的 Deployment。 Label：这个值对于 Istio 很重要，因为 Istio 要用它来标记 metrics。每个 Application 要求包括 app 和 version 两个 label。 Namespace：通常用于区分项目和用户。 Service：即 Kubernetes 中的 Service，不过要求必须有 app label。 Workload：Kubernetes 中的所有常用资源类型如 Deployment、StatefulSet、Job 等都可以检测到，不论这些负载是否加入到 Istio Service Mesh 中。 Application、Workload 与 Service 的关系如下图所示。 Kilia 的详细 API 使用说明请查看 Swagger API 文档，在 Kiali 的根目录下运行下面的命令可以查看 API 文档。 make swagger-serve Swagger UI 如下图。 架构 Kiali 部署完成后只启动了一个 Pod，前后端都集成在这一个 Pod 中。Kiali 也有一些依赖的组件，例如如果要在 Kiali 的页面中获取到监控 metric 需要使用在 istio-system 中部署 Prometheus。分布式卓总直接下图是 Kiali 的架构，来自 Kiali 官网。 Kiali 使用传统的前后端分离架构： 后端使用 Go 编写：https://github.com/kiali/kiali，为前端提供 API，所有消息使用 JSON 编码，使用 ConfigMap 和 Secret 来存储配置。直接与 Kubernetes 和 Istio 通信来获取数据。 前端使用 Typescript 编写：https://github.com/kiali/kiali-ui，无状态，除了一些证书保存在浏览器中。于查询后端 API，可以跳转访问 Jaeger 分布式追踪和 Grafana 监控页面。 Jaeger 和 Grafana 都是可选组件，使用的都是外部服务，不是由 Kiali 部署的，需要在 kiali-configmap.yaml 中配置 URL。注意该 URL 必须是从你本地浏览器中可以直接访问到的地址。 注意：如果服务之间没有任何请求就不会在 Prometheus 中保存数据也就无法显示服务拓扑图，所以大家在部署完 Bookinfo 服务之后向 productpage 服务发送一些请求用于生成服务拓扑图。 服务拓扑图 Kiali 中的服务拓扑图比起 Istio 原来默认部署的 ServiceGraph 的效果更炫也更加直观，具有更多选项。 例如使用 CURL 模拟请求。 $ curl -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwiZXhwIjoxNTM5NjczOTYyfQ.6gNz4W6yA9Bih4RkTbcSvqdaiRqsyj8c8o6ictM9iDs\" http://172.17.8.101:32439/api/namespaces/all/graph?duration=60s&graphType=versionedApp&injectServiceNodes=false&appenders=dead_node,sidecars_check,istio 会得到如下的返回的 JSON 返回值，为了节省篇幅其中省略了部分结果： { \"timestamp\": 1539296648, \"graphType\": \"versionedApp\", \"elements\": { \"nodes\": [ { \"data\": { \"id\": \"6519157be154675342fb76c41edc731c\", \"nodeType\": \"app\", \"namespace\": \"default\", \"app\": \"reviews\", \"isGroup\": \"version\" } }, ... { \"data\": { \"id\": \"6249668dd0a91adb9e62994d36563365\", \"nodeType\": \"app\", \"namespace\": \"istio-system\", \"workload\": \"istio-ingressgateway\", \"app\": \"istio-ingressgateway\", \"version\": \"unknown\", \"rateOut\": \"0.691\", \"isOutside\": true, \"isRoot\": true } } ], \"edges\": [ { \"data\": { \"id\": \"d51ca2a95d721427bbe27ed209766ec5\", \"source\": \"06e488a37fc9aa5b0e0805db4f16ae69\", \"target\": \"31150e7e5adf85b63f22fbd8255803d7\", \"rate\": \"0.236\", \"percentRate\": \"17.089\", \"responseTime\": \"0.152\" } }, ... { \"data\": { \"id\": \"1dda06d9904bcf727d1b6a113be58556\", \"source\": \"80f71758099020586131c3565075935d\", \"target\": \"4b64bda48e5a3c7e50ab1c63836c9469\", \"rate\": \"0.236\", \"responseTime\": \"0.022\" } } ] } } 该值中包含了每个 node 和 edege 的信息，Node 即图中的每个节点，其中包含了节点的配置信息，Edge 即节点间的关系还有流量情况。前端可以根据该信息绘制服务拓扑图，我们下面将查看下 kiali 的后端，看看它是如何生成以上格式的 JSON 信息的。 注：详细的 REST API 使用和字段说明请查看 swagger 生成的 API 文档。 代码解析 下面将带大家了解 Kiali 的后端代码基本结构。 路由配置 服务拓扑图的路由信息保存在 kiali/routing/routes.go 文件中。 { \"GraphNamespace\", \"GET\", \"/api/namespaces/{namespace}/graph\", handlers.GraphNamespace, true, }, { \"GraphAppVersion\", \"GET\", \"/api/namespaces/{namespace}/applications/{app}/versions/{version}/graph\", handlers.GraphNode, true, }, { \"GraphApp\", \"GET\", \"/api/namespaces/{namespace}/applications/{app}/graph\", handlers.GraphNode, true, }, { \"GraphService\", \"GET\", \"/api/namespaces/{namespace}/services/{service}/graph\", handlers.GraphNode, true, }, { \"GraphWorkload\", \"GET\", \"/api/namespaces/{namespace}/workloads/{workload}/graph\", handlers.GraphNode, true, } 直接查看 Swagger 生成的 API 文档也可以。 PQL 查询语句构建 kiali/handlers/graph.go 中处理 HTTP 请求，服务拓扑图中所有的指标信息都是从 Prometheus 中查询得到的。 Kiali 的服务状态拓扑是根据 namespace 来查询的，例如 default namespace 下的服务指标查询 PQL： round(sum(rate(istio_requests_total{reporter=\"source\",source_workload_namespace=\"default\",response_code=~\"[2345][0-9][0-9]\"} [600s])) by (source_workload_namespace,source_workload,source_app,source_version,destination_service_namespace,destination_service_name,destination_workload,destination_app,destination_version,response_code),0.001) 其中的参数都是通过页面选择传入的（构建的 PQL 中的选项在 kiali/graph/options/options.go 中定义）： reporter=\"source\"：metric 报告来源，源服务（source）是 envoy 代理的下游客户端。在服务网格里，一个源服务通常是一个工作负载，但是入口流量的源服务有可能包含其他客户端，例如浏览器，或者一个移动应用。 source_workload_namespace=\"default\"：选择命名空间。 response_code：返回码区间。 [600s]：查询的数据中的时间间隔。 关于 PQL 的详细使用方式请参考 QUERY EXAMPLES - prometheus.io。 这里面包含了所有 workload 的流量信息，做简单的操作就可以计算出 application/service 的流量状况。 HTTP 处理逻辑 HTTP 请求的处理逻辑入口位于 kiali/handlers/graph.go，路径为： func graphNamespaces(o options.Options, client *prometheus.Client) graph.TrafficMap { switch o.Vendor { case \"cytoscape\": default: checkError(errors.New(fmt.Sprintf(\"Vendor [%s] not supported\", o.Vendor))) } log.Debugf(\"Build [%s] graph for [%v] namespaces [%s]\", o.GraphType, len(o.Namespaces), o.Namespaces) trafficMap := graph.NewTrafficMap() for _, namespace := range o.Namespaces { log.Debugf(\"Build traffic map for namespace [%s]\", namespace) namespaceTrafficMap := buildNamespaceTrafficMap(namespace, o, client) for _, a := range o.Appenders { a.AppendGraph(namespaceTrafficMap, namespace) // Appender 用于添加 service graph } mergeTrafficMaps(trafficMap, namespaceTrafficMap) //将不同的 namespace 下的服务状态合并 } // appender 用于添加/删除/修改 node 信息。操作完成后可以做出如下判断： // - 将其标记外来者（即不在请求的 namespace 中的 node） // - 将其标记内部流量制造者（即位于 namespace 中只有向外的 edge） markOutsiders(trafficMap, o) markTrafficGenerators(trafficMap) if graph.GraphTypeService == o.GraphType { trafficMap = reduceToServiceGraph(trafficMap) } return trafficMap } Appender 是一个接口，在 service graph 中注入详细的信息，它的定义如下： // Appender 由任何代码提供实现，以附加具有补充信息的 service graph。如果出错，appender应该执行 panic 并将其作为错误响应处理。 type Appender interface { // AppendGraph 在提供的 traffic map 上执行 appender 工作。Map 最初可能是空的。允许 appender 添加或删除映射条目。 AppendGraph(trafficMap graph.TrafficMap, namespace string) } Appender 位于 kiali/graph/appender 目录下，目前一共有如下实现： DeadNodeAppender：用于将不想要 node 从 service graph 中删除。 IstioAppender：获取指定 namespace 下 Istio 的详细信息，当前版本获取指定 namespace 下的 VirtualService 和 DestinationRule 信息。 ResponseTimeAppender：获取响应时间。 SecurityPolicyAppender：在 service graph 中添加安全性策略信息。 SidecarsCheckAppender：检查 Sidecar 的配置信息，例如 Pod 中是否有 App label。 UnusedNodeAppender：未加入 Service Mesh 的 node。 我们再来看下在 kiali/graph/graph.go 中定义的 TrafficMap 结构。 // TrafficMap 是 App 与 Node 之间的映射，每个节点都可选择保存 Edge 数据。Metadata 是用于保存任何期望的 node 或 edge 信息的通用映射。每个 app 节点应具有唯一的 namespace + workload。请注意，在同一 namespace 中有两个具有相同 name + version 的节点是可行的但可能并不常见。 type TrafficMap map[string]*Node type Node struct { ID string // unique identifier for the node NodeType string // Node type Namespace string // Namespace Workload string // Workload (deployment) name App string // Workload app label value Version string // Workload version label value Service string // Service name Edges []*Edge // child nodes Metadata map[string]interface{} // app-specific data } type Edge struct { Source *Node Dest *Node Metadata map[string]interface{} // app-specific data } 以上只是对 Kiali 部分代码的解读，更详细的实现大家可以克隆 kiali 的代码自己研究。 参考 Kiali.io QUERY EXAMPLES - prometheus.io replace Service Graph with Kiali #9066 - github.com rootsongjc/kubernetes-vagrant-centos-cluster - github.com Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"setup/bookinfo-sample.html":{"url":"setup/bookinfo-sample.html","title":"Bookinfo 示例","keywords":"","body":"Bookinfo 示例 Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点： 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时推出了多个版本 Bookinfo 应用部署架构 以下为 Istio 官方提供的该应用的架构图。 图 4.5.1：Istio 的 Bookinfo 示例应用架构图 Bookinfo 应用分为四个单独的微服务，其中每个微服务的部署的结构中都注入了一个 Sidecar： productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 使用 kubernetes-vagrant-centos-cluster 部署的 Kubernetes 集群和 Istio 服务的话可以直接运行下面的命令部署 Bookinfo 示例： $ kubectl apply -n default -f 关于该示例的介绍和详细步骤请参考 Bookinfo 应用。 Bookinfo 示例及 Istio 服务整体架构 从 Bookinfo 应用部署架构中可以看到该应用的几个微服务之间的关系，但是并没有描绘应用与 Istio 控制平面、Kubernetes 平台的关系，下图中描绘的是应用和平台整体的架构。 图 4.5.2：Bookinfo 示例与 Istio 的整体架构图 从图中可以看出 Istio 整体架构的特点： 模块化：很多模块可以选择性的开启，如负责证书管理的 istio-citadel 默认就没有启用 可定制化：可观察性的组件可以定制化和替换 参考 Bookinfo 应用 - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/":{"url":"config/networking/","title":"流量管理配置概述","keywords":"","body":"流量管理配置概述 流量管理一节描述如何配置 Istio 服务网格中的 HTTP/TCP 流量，其配置对象包括： VirtualService DestinationRule Gateway EnvoyFilter ServiceEntry Sidecar WorkloadEntry WorkloadGroup Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/virtual-service.html":{"url":"config/networking/virtual-service.html","title":"VirtualService","keywords":"","body":"VirtualService VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术语。 Service 是与服务注册表（service registry）中的唯一名称绑定的应用行为单元。服务由多个网络 端点（endpoint） 组成，这些端点由运行在 pod、容器、虚拟机等的工作负载实例实现。 服务版本，又称子集（subset）：在持续部署方案中，对于一个给定的服务，可能有不同的实例子集，运行应用程序二进制的不同变体。这些变体不一定是不同的 API 版本。它们可能是同一服务的迭代变化，部署在不同的环境（prod、staging、dev 等）。发生这种情况的常见场景包括 A/B 测试、金丝雀发布等。一个特定版本的选择可以根据各种标准（header、URL 等）和 / 或分配给每个版本的权重来决定。每个服务都有一个由其所有实例组成的默认版本。 源（source）：下游客户端调用服务。 Host：客户端在尝试连接到服务时使用的地址。 访问模型（access model）：应用程序只针对目标服务（Host），而不了解各个服务版本（子集）。版本的实际选择是由代理/sidecar 决定的，使应用程序代码能够从依赖服务的演变中解脱出来。 VirtualService 定义了一套当主机被寻址时应用的流量路由规则。每个路由规则定义了特定协议流量的匹配标准。如果流量被匹配，那么它将被发送到注册表中定义的指定目标服务（或它的子集/版本）。 流量的来源也可以在路由规则中进行匹配。这允许为特定的客户环境定制路由。 示例 以下是 Kubernetes 上的例子，默认情况下，所有的 HTTP 流量都会被路由到标签为 version: v1 的 reviews 服务的 pod 上。此外，路径以 /wpcatalog/ 或 /consumercatalog/ 开头的 HTTP 请求将被重写为 /newcatalog，并被发送到标签为 version: v2 的 pod 上。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews-route spec: hosts: - reviews.prod.svc.cluster.local http: - name: \"reviews-v2-routes\" match: - uri: prefix: \"/wpcatalog\" - uri: prefix: \"/consumercatalog\" rewrite: uri: \"/newcatalog\" route: - destination: host: reviews.prod.svc.cluster.local subset: v2 - name: \"reviews-v1-route\" route: - destination: host: reviews.prod.svc.cluster.local subset: v1 途径目的地的一个子集/版本是通过对一个命名的服务子集的引用来识别的，这个子集必须在一个相应的 DestinationRule 中声明。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews.prod.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 关于 VirtualService 配置的详细用法请参考 Istio 官方文档。 参考 Virtual Service - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/destination-rule.html":{"url":"config/networking/destination-rule.html","title":"DestinationRule","keywords":"","body":"DestinationRule DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载均衡的配置、来自 sidecar 的连接池大小，以及用于检测和驱逐负载均衡池中不健康主机的离群检测设置。 示例 例如，ratings 服务的一个简单的负载均衡策略看起来如下。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_CONN 可以通过定义一个命名的 subset 并覆盖在服务级别指定的设置来指定特定版本的策略。下面的规则对前往由带有标签（version:v3）的端点（如 pod）组成的名为 testversion 的子集的所有流量使用轮回负载均衡策略。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_CONN subsets: - name: testversion labels: version: v3 trafficPolicy: loadBalancer: simple: ROUND_ROBIN 注意：只有当路由规则明确地将流量发送到这个子集，为子集指定的策略才会生效。 流量策略也可以针对特定的端口进行定制。下面的规则对所有到 80 号端口的流量使用最少连接的负载均衡策略，而对 9080 号端口的流量使用轮流负载均衡设置。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings-port spec: host: ratings.prod.svc.cluster.local trafficPolicy: # 应用到所有端口 portLevelSettings: - port: number: 80 loadBalancer: simple: LEAST_CONN - port: number: 9080 loadBalancer: simple: ROUND_ROBIN 关于 DestinationRule 配置的详细用法请参考 Istio 官方文档。 参考 Destination Rule - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/gateway.html":{"url":"config/networking/gateway.html","title":"Gateway","keywords":"","body":"Gateway Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该暴露的端口、要使用的协议类型、负载均衡器的 SNI 配置等。 示例 例如，下面的 Gateway 配置设置了一个代理，作为负载均衡器，暴露了 80 和 9080 端口（http）、443（https）、9443（https）和 2379 端口（TCP）的入口（ingress）。网关将被应用于运行在标签为 app: my-gateway-controller 的 pod 上的代理。虽然 Istio 将配置代理来监听这些端口，但用户有责任确保这些端口的外部流量被允许进入网格。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com tls: httpsRedirect: true # 对 HTTP 请求发送 301 重定向 - port: number: 443 name: https-443 protocol: HTTPS hosts: - uk.bookinfo.com - eu.bookinfo.com tls: mode: SIMPLE # 在此端口上启用 HTTPS serverCertificate: /etc/certs/servercert.pem privateKey: /etc/certs/privatekey.pem - port: number: 9443 name: https-9443 protocol: HTTPS hosts: - \"bookinfo-namespace/*.bookinfo.com\" tls: mode: SIMPLE # 在此端口上启用 HTTPS credentialName: bookinfo-secret # 从 Kubernetes secret 中获取证书 - port: number: 9080 name: http-wildcard protocol: HTTP hosts: - \"*\" - port: number: 2379 # 通过此端口暴露内部服务 name: mongo protocol: MONGO hosts: - \"*\" 上面的网关规范描述了负载均衡器的 L4 到 L6 属性。然后，一个 VirtualService 可以被绑定到一个 Gateway 上，以控制到达特定主机或网关端口的流量的转发。 例如，下面的 VirtualService 将 https://uk.bookinfo.com/reviews、https://eu.bookinfo.com/reviews、 http://uk.bookinfo.com:9080/reviews、http://eu.bookinfo.com:9080/reviews 的流量分成两个版本（prod 和 qa）的内部 reviews 服务，端口为 9080。此外，包含 cookie user: dev-123 的请求将被发送到 qa 版本的特殊端口 7777。同样的规则也适用于网格内部对 reviews.prod.svc.cluster.local 服务的请求。这个规则适用于 443、9080 端口。请注意，http://uk.bookinfo.com 会被重定向到 https://uk.bookinfo.com（即 80 重定向到 443）。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo-rule namespace: bookinfo-namespace spec: hosts: - reviews.prod.svc.cluster.local - uk.bookinfo.com - eu.bookinfo.com gateways: - some-config-namespace/my-gateway - mesh # 应用到网格中所有的 sidecar http: - match: - headers: cookie: exact: \"user=dev-123\" route: - destination: port: number: 7777 host: reviews.qa.svc.cluster.local - match: - uri: prefix: /reviews/ route: - destination: port: number: 9080 # 如果它是 reviews 的唯一端口，则可以省略。 host: reviews.prod.svc.cluster.local weight: 80 - destination: host: reviews.qa.svc.cluster.local weight: 20 下面的 VirtualService 将到达（外部）27017 端口的流量转发到 5555 端口的内部 Mongo 服务器。这个规则在网格内部不适用，因为网关列表中省略了保留名称 mesh。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo-mongo namespace: bookinfo-namespace spec: hosts: - mongosvr.prod.svc.cluster.local # 内部 Mongo 服务的名字 gateways: - some-config-namespace/my-gateway # 如果网关与虚拟服务处于同一命名空间，可以省略命名空间。 tcp: - match: - port: 27017 route: - destination: host: mongo.prod.svc.cluster.local port: number: 5555 以使用 hosts 字段中的命名空间/主机名语法来限制可以绑定到网关服务器的虚拟服务集。例如，下面的网关允许 ns1 命名空间中的任何虚拟服务与之绑定，而只限制 ns2 命名空间中的 foo.bar.com 主机的虚拟服务与之绑定。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"ns1/*\" - \"ns2/foo.bar.com\" 关于 Gateway 配置的详细用法请参考 Istio 官方文档。 参考 Gateway - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/envoy-filter.html":{"url":"config/networking/envoy-filter.html","title":"EnvoyFilter","keywords":"","body":"EnvoyFilter EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与其他 Istio 网络对象不同，EnvoyFilter 是累加应用。对于特定命名空间中的特定工作负载，可以存在任意数量的 EnvoyFilter。这些 EnvoyFilter 的应用顺序如下：配置根命名空间中的所有 EnvoyFilter，其次是工作负载命名空间中的所有匹配 EnvoyFilter。 注意一 该 API 的某些方面与 Istio 网络子系统的内部实现以及 Envoy 的 xDS API 有很深的关系。虽然 EnvoyFilter API 本身将保持向后兼容，但通过该机制提供的任何 Envoy 配置应在 Istio 代理版本升级时仔细审查，以确保废弃的字段被适当地删除和替换。 注意二 当多个 EnvoyFilter 被绑定到特定命名空间的同一个工作负载时，所有补丁将按照创建顺序处理。如果多个 EnvoyFilter 的配置相互冲突，则其行为将无法确定。 注意三 要将 EnvoyFilter 资源应用于系统中的所有工作负载（sidecar 和 gateway）上，请在 config 根命名空间中定义该资源，不要使用 workloadSelector。 示例 下面的例子在名为 istio-config 的根命名空间中声明了一个全局默认的 EnvoyFilter 资源，在系统中的所有 sidecar 上添加了一个自定义的协议过滤器，用于 outbound 端口 9307。该过滤器应在终止 tcp_proxy 过滤器之前添加，以便生效。此外，它为 gateway 和 sidecar 的所有 HTTP 连接设置了 30 秒的空闲超时。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: custom-protocol namespace: istio-config # 如在 meshConfig 资源中定义的。 spec: configPatches: - applyTo: NETWORK_FILTER match: context: SIDECAR_OUTBOUND # 将会匹配所有 sidecar 中的所有 outbound listener listener: portNumber: 9307 filterChain: filter: name: \"envoy.filters.network.tcp_proxy\" patch: operation: INSERT_BEFORE value: # 这是完整的过滤器配置，包括名称和 typed_config 部分。 name: \"envoy.config.filter.network.custom_protocol\" typed_config: ... - applyTo: NETWORK_FILTER # HTTP 连接管理器是 Envoy 的一个过滤器。 match: # 省略了上下文，因此这同时适用于 sidecar 和 gateway。 listener: filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: MERGE value: name: \"envoy.filters.network.http_connection_manager\" typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\" common_http_protocol_options: idle_timeout: 30s 下面的例子启用了 Envoy 的 Lua 过滤器，用于处理所有到达 bookinfo 命名空间中的对 reviews 服务 pod 的 8080 端口的 HTTP 调用，标签为 app: reviews。Lua 过滤器调用外部服务internal.org.net:8888，这需要在 Envoy 中定义一个特殊的 cluster。该 cluster 也被添加到 sidecar 中，作为该配置的一部分。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: reviews-lua namespace: bookinfo spec: workloadSelector: labels: app: reviews configPatches: # 第一个补丁将 lua过 滤器添加到监听器/http 连接管理器。 - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND listener: portNumber: 8080 filterChain: filter: name: \"envoy.filters.network.http_connection_manager\" subFilter: name: \"envoy.filters.http.router\" patch: operation: INSERT_BEFORE value: # lua 过滤器配置 name: envoy.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_request(request_handle) -- 向上游主机进行 HTTP 调用，header、body 和 time欧特 如下。 local headers, body = request_handle:httpCall( \"lua_cluster\", { [\":method\"] = \"POST\", [\":path\"] = \"/acl\", [\":authority\"] = \"internal.org.net\" }, \"authorize call\", 5000) end # 第二个补丁添加了被 lua 代码引用的 cluster，cds 匹配被省略，因为正在添加一个新的 cluster。 - applyTo: CLUSTER match: context: SIDECAR_OUTBOUND patch: operation: ADD value: # cluster 配置 name: \"lua_cluster\" type: STRICT_DNS connect_timeout: 0.5s lb_policy: ROUND_ROBIN load_assignment: cluster_name: lua_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: protocol: TCP address: \"internal.org.net\" port_value: 8888 下面的例子覆盖了 SNI 主机 app.example.com 在 istio-system 命名空间的 ingress gateway 的监听器中的 HTTP 连接管理器的某些字段（HTTP 空闲超时和X-Forward-For信任跳数）。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: hcm-tweaks namespace: istio-system spec: workloadSelector: labels: istio: ingressgateway configPatches: - applyTo: NETWORK_FILTER # HTTP 连接管理器是 Envoy 中的一个过滤器。 match: context: GATEWAY listener: filterChain: sni: app.example.com filter: name: \"envoy.filters.network.http_connection_manager\" patch: operation: MERGE value: typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\" xff_num_trusted_hops: 5 common_http_protocol_options: idle_timeout: 30s 下面的例子插入了一个产生 istio_operationId 属性的 attributegen 过滤器，该属性被 istio.stats fiter 消费。filterClass:STATS 对这种依赖关系进行编码。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: reviews-request-operation namespace: myns spec: workloadSelector: labels: app: reviews configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND patch: operation: ADD filterClass: STATS # 这个过滤器将在 Istio 统计过滤器之前运行。 value: name: istio.request_operation typed_config: \"@type\": type.googleapis.com/udpa.type.v1.TypedStruct type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm value: config: configuration: | { \"attributes\": [ { \"output_attribute\": \"istio_operationId\", \"match\": [ { \"value\": \"ListReviews\", \"condition\": \"request.url_path == '/reviews' && request.method == 'GET'\" }] }] } vm_config: runtime: envoy.wasm.runtime.null code: local: { inline_string: \"envoy.wasm.attributegen\" } 下面的例子在 myns 命名空间中插入了一个 http ext_authz 过滤器。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: myns-ext-authz namespace: myns spec: configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND patch: operation: ADD filterClass: AUTHZ # 该过滤器将在 Istio authz 过滤器之后运行。 value: name: envoy.filters.http.ext_authz typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz grpc_service: envoy_grpc: cluster_name: acme-ext-authz initial_metadata: - key: foo value: myauth.acme # 本地 ext auth 服务器要求的。 myns 命名空间中的一个工作负载需要访问一个不接受初始元数据的不同 ext_auth服务器。由于 proto merge 不能删除字段，下面的配置使用 REPLACE 操作。如果你不需要继承字段，REPLACE 比 MERGE 更适合。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: mysvc-ext-authz namespace: myns spec: workloadSelector: labels: app: mysvc configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND patch: operation: REPLACE value: name: envoy.filters.http.ext_authz typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz grpc_service: envoy_grpc: cluster_name: acme-ext-authz-alt 下面的例子为所有 inbound 的 sidecar HTTP 请求部署了一个 Wasm 扩展。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: wasm-example namespace: myns spec: configPatches: # 第一个补丁定义了一个 Wasm 扩展，并提供了一个 URL 来获取 Wasm 二进制文件，以及二进制配置。它应该出现在应用它的下一个补丁之前。这个资源对命名空间 \"myns\" 中的所有代理是可见的。有可能在多个命名空间为同一名称 \"my-wasm-extension\" 提供多个定义。我们建议，如果需要覆盖，那么可以用 REPLACE 覆盖每个命名空间的根级定义；如果不需要覆盖，那么这个名字应该用命名空间 \"myns/my-wasm-extension\" 来限定，以避免意外的名字冲突。 - applyTo: EXTENSION_CONFIG patch: operation: ADD # REPLACE is also supported, and would override a cluster level resource with the same name. value: name: my-wasm-extension typed_config: \"@type\": type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm config: root_id: my-wasm-root-id vm_config: vm_id: my-wasm-vm-id runtime: envoy.wasm.runtime.v8 code: remote: http_uri: uri: http://my-wasm-binary-uri configuration: \"@type\": \"type.googleapis.com/google.protobuf.StringValue\" value: | {} # 第二个补丁指示将上述 Wasm 过滤器应用于 listener/http 连接管理器。 - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND patch: operation: ADD filterClass: AUTHZ # 这个过滤器将在 Istio authz 过滤器之后运行。 value: name: my-wasm-extension # 这个必须匹配上面的名字。 config_discovery: config_source: api_config_source: api_type: GRPC transport_api_version: V3 grpc_services: - envoy_grpc: cluster_name: xds-grpc type_urls: [\"envoy.extensions.filters.http.wasm.v3.Wasm\"] 关于 EnvoyFilter 配置的详细用法请参考 Istio 官方文档。 参考 EnvoyFilter - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/sidecar.html":{"url":"config/networking/sidecar.html","title":"Sidecar","keywords":"","body":"Sidecar Sidecar 描述了 sidecar 代理的配置，该代理负责调解与它所连接的工作负载实例的 inbound 和 outbound 通信。默认情况下，Istio 会对网格中的所有 sidecar 代理进行必要的配置，以达到网格中的每个工作负载实例，并接受与工作负载相关的所有端口的流量。Sidecar 配置提供了一种方法来微调代理在转发工作负载的流量时将接受的一组端口和协议。此外，还可以限制代理在转发工作负载实例的 outbound 流量时可以到达的服务集。 网格中的服务和配置被组织到一个或多个命名空间（例如，Kubernetes 命名空间）。命名空间中的 Sidecar 配置将适用于同一命名空间中的一个或多个工作负载实例，并通过 workloadSelector 字段进行选择。在没有 workloadSelector 的情况下，它将适用于同一命名空间的所有工作负载实例。在确定应用于工作负载实例的 Sidecar 配置时，将优先考虑具有选择该工作负载实例的 workloadSelector 的资源，而不是没有任何 workloadSelector 的 Sidecar 配置。 注意事项 在配置 Sidecar 时需要注意以下事项。 注意一 每个命名空间只能有一个没有任何 workloadSelector 的 Sidecar 配置，该配置为该命名空间的所有 pod 指定了默认配置。建议对整个命名空间的 sidecar 命名为 defatult。如果在一个给定的命名空间中存在多个无选择器的 Sidecar 配置，则系统的行为未被定义。如果两个或更多带有 workloadSelector 的 Sidecar 配置选择了同一个工作负载实例，那么系统的行为将无法定义。 注意二 MeshConfig 根命名空间中的 Sidecar 配置将被默认应用于所有没有 Sidecar 配置的命名空间。这个全局默认 Sidecar 配置不应该有任何 workloadSelector。 示例 下面的例子在根命名空间 istio-config 中声明了一个全局默认 Sidecar 配置，该配置将所有命名空间中的 sidecar 配置为只允许向同一命名空间中的其他工作负载以及 istio-system 命名空间中的服务输出流量。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default namespace: istio-config spec: egress: - hosts: - \"./*\" - \"istio-system/*\" 下面的例子在 prod-us1 命名空间中声明了一个 Sidecar 配置，它覆盖了上面定义的全局默认值，并配置了命名空间中的 sidecar，以允许向 prod-us1、prod-apis 和 istio-system 命名空间的公共服务输出流量。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default namespace: prod-us1 spec: egress: - hosts: - \"prod-us1/*\" - \"prod-apis/*\" - \"istio-system/*\" 下面的例子在 prod-us1 命名空间为所有标签为 app: ratings 的 pod 声明了一个 Sidecar 配置，这些 pod 属于 rating.prod-us1 服务。该工作负载接受 9080 端口的入站 HTTP 流量。然后，该流量被转发到在 Unix 域套接字上监听的附加工作负载实例。在出口方向，除了 istio-system 命名空间外，sidecar 只为 prod-us1 命名空间的服务代理绑定在 9080 端口的 HTTP 流量。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: ratings namespace: prod-us1 spec: workloadSelector: labels: app: ratings ingress: - port: number: 9080 protocol: HTTP name: somename defaultEndpoint: unix:///var/run/someuds.sock egress: - port: number: 9080 protocol: HTTP name: egresshttp hosts: - \"prod-us1/*\" - hosts: - \"istio-system/*\" 如果工作负载的部署没有基于 IPTables 的流量捕获，Sidecar 配置是配置连接到工作负载实例的代理上的端口的唯一方法。下面的例子在 prod-us1 命名空间中为属于 productpage.prod-us1 服务的标签为 app: productpage 的所有 pod 声明了一个 Sidecar 配置。假设这些 pod 的部署没有 IPtable 规则（即 istio-init 容器），并且代理元数据 ISTIO_META_INTERCEPTION_MODE 被设置为 NONE，下面的规范允许这样的 pod 在 9080 端口接收 HTTP 流量（在 Istio mutual TLS 内包装），并将其转发给在 127.0.0.1:8080 监听的应用程序。它还允许应用程序与 127.0.0.1:3306 上的 MySQL 数据库进行通信，然后被代理到 mysql.foo.com:3306 上的外部托管 MySQL 服务。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: no-ip-tables namespace: prod-us1 spec: workloadSelector: labels: app: productpage ingress: - port: number: 9080 #绑定到 proxy_instance_ip:9080（0.0.0.0:9080，如果实例没有可用的单播 IP）。 protocol: HTTP name: somename defaultEndpoint: 127.0.0.1:8080 captureMode: NONE #如果为整个代理设置了元数据，则不需要。 egress: - port: number: 3306 protocol: MYSQL name: egressmysql captureMode: NONE #如果为整个代理设置了元数据，则不需要。 bind: 127.0.0.1 hosts: - \"*/mysql.foo.com\" 以及路由到 mysql.foo.com:3306 的相关服务条目。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-mysql namespace: ns1 spec: hosts: - mysql.foo.com ports: - number: 3306 name: mysql protocol: MYSQL location: MESH_EXTERNAL resolution: DNS 也可以在一个代理中混合和匹配流量捕获模式。例如有一个设置，内部服务在 192.168.0.0/16 子网。因此，在虚拟机上设置 IPTables 以捕获 192.168.0.0/16 子网的所有出站流量。假设虚拟机在 172.16.0.0/16 子网有一个额外的网络接口，用于入站流量。下面的 Sidecar 配置允许虚拟机在 172.16.1.32:80（虚拟机的 IP）上为从 172.16.0.0/16 子网到达的流量暴露一个监听器。 注意：虚拟机中代理上的 ISTIO_META_INTERCEPTION_MODE 元数据应包含 REDIRECT 或 TPROXY 作为其值，这意味着基于 IPTables 的流量捕获是激活的。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: partial-ip-tables namespace: prod-us1 spec: workloadSelector: labels: app: productpage ingress: - bind: 172.16.1.32 port: number: 80 #绑定到 172.16.1.32:80 protocol: HTTP name: somename defaultEndpoint: 127.0.0.1:8080 captureMode: NONE egress: #使用系统检测到的默认值设置配置，以处理到 192.168.0.0/16 子网的服务的出站流量，基于服务注册表提供的信息 - captureMode: IPTABLES hosts: - \"*/*\" 关于 Sidecar 配置的详细用法请参考 Istio 官方文档。 参考 Sidecar - Istio 官网文档 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/service-entry.html":{"url":"config/networking/service-entry.html","title":"ServiceEntry","keywords":"","body":"ServiceEntry ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发现的服务就可以访问 / 路由到这些手动指定的服务。一个服务条目描述了一个服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可以是网格的外部服务（如 Web API），也可以是不属于平台服务注册表的网格内部服务（如与 Kubernetes 中的服务对话的一组虚拟机）。此外，服务条目的端点也可以通过使用 workloadSelector 字段动态选择。这些端点可以是使用 WorkloadEntry 对象声明的虚拟机工作负载或 Kubernetes pod。在单一服务下同时选择 pod 和 VM 的能力允许将服务从 VM 迁移到 Kubernetes，而不必改变与服务相关的现有 DNS 名称。 示例 下面的例子声明了一些内部应用程序通过 HTTPS 访问的外部 API。Sidecar 检查了 ClientHello 消息中的 SNI 值，以路由到适当的外部服务。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-https spec: hosts: - api.dropboxapi.com - www.googleapis.com - api.facebook.com location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: DNS 下面的配置在 Istio 的注册表中添加了一组运行在未被管理的虚拟机上的 MongoDB 实例，因此这些服务也可以被视为网格中的任何其他服务。相关的 DestinationRule 被用来启动与数据库实例的 mTLS 连接。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-mongocluster spec: hosts: - mymongodb.somedomain # 未使用 addresses: - 192.192.192.192/24 # VIP ports: - number: 27018 name: mongodb protocol: MONGO location: MESH_INTERNAL resolution: STATIC endpoints: - address: 2.2.2.2 - address: 3.3.3.3 相关的 DestinationRule。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: mtls-mongocluster spec: host: mymongodb.somedomain trafficPolicy: tls: mode: MUTUAL clientCertificate: /etc/certs/myclientcert.pem privateKey: /etc/certs/client_private_key.pem caCertificates: /etc/certs/rootcacerts.pem 下面的例子在一个虚拟服务中使用服务条目和 TLS 路由的组合，根据 SNI 值将流量引导到内部出口（egress）防火墙。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-redirect spec: hosts: - wikipedia.org - \"*.wikipedia.org\" location: MESH_EXTERNAL ports: - number: 443 name: https protocol: TLS resolution: NONE 相关的 VirtualService，以根据 SNI 值进行路由。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: tls-routing spec: hosts: - wikipedia.org - \"*.wikipedia.org\" tls: - match: - sniHosts: - wikipedia.org - \"*.wikipedia.org\" route: - destination: host: internal-egress-firewall.ns1.svc.cluster.local 带有 TLS 匹配的虚拟服务是为了覆盖默认的 SNI 匹配。在没有虚拟服务的情况下，流量将被转发到维基百科的域。 下面的例子演示了专用出口（egress）网关的使用，所有外部服务流量都通过该网关转发。exportTo 字段允许控制服务声明对网格中其他命名空间的可见性。默认情况下，服务会被输出到所有命名空间。下面的例子限制了对当前命名空间的可见性，用 . 表示，所以它不能被其他命名空间使用。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-httpbin namespace : egress spec: hosts: - httpbin.com exportTo: - \".\" location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: DNS 定义一个网关来处理所有的出口（egress）流量。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: istio-egressgateway namespace: istio-system spec: selector: istio: egressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" 和相关的 VirtualService，从 Sidecar 路由到网关服务（istio-egressgateway.istio-system.svc.cluster.local），以及从网关路由到外部服务。请注意，虚拟服务被导出到所有命名空间，使它们能够通过网关将流量路由到外部服务。迫使流量通过像这样一个受管理的中间代理是一种常见的做法。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: gateway-routing namespace: egress spec: hosts: - httpbin.com exportTo: - \"*\" gateways: - mesh - istio-egressgateway http: - match: - port: 80 gateways: - mesh route: - destination: host: istio-egressgateway.istio-system.svc.cluster.local - match: - port: 80 gateways: - istio-egressgateway route: - destination: host: httpbin.com 下面的例子演示了在外部服务的主机中使用通配符。如果连接必须被路由到应用程序请求的 IP 地址（即应用程序解析 DNS 并试图连接到一个特定的 IP），发现模式必须被设置为NONE。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-wildcard-example spec: hosts: - \"*.bar.com\" location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: NONE 下面的例子演示了一个通过客户主机上的 Unix 域套接字提供的服务。解析必须设置为STATIC以使用 Unix 地址端点。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: unix-domain-socket-example spec: hosts: - \"example.unix.local\" location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC endpoints: - address: unix:///var/run/example/socket 对于基于 HTTP 的服务，可以创建一个由多个 DNS 可寻址端点支持的虚拟服务。在这种情况下，应用程序可以使用HTTP_PROXY环境变量来透明地将 VirtualService 的 API 调用重新路由到所选择的后端。例如，下面的配置创建了一个不存在的外部服务，名为foo.bar.com，由三个域名支持：us.foo.bar.com:8080，uk.foo.bar.com:9080和in.foo.bar.com:7080。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc-dns spec: hosts: - foo.bar.com location: MESH_EXTERNAL ports: - number: 80 name: http protocol: HTTP resolution: DNS endpoints: - address: us.foo.bar.com ports: http: 8080 - address: uk.foo.bar.com ports: http: 9080 - address: in.foo.bar.com ports: http: 7080 有了HTTP_PROXY=http://localhost/，从应用程序到http://foo.bar.com的调用将在上面指定的三个域中进行负均衡。换句话说，对http://foo.bar.com/baz的调用将被转译成http://uk.foo.bar.com/baz。 下面的例子说明了包含主题（subject）替代名称的 ServiceEntry 的用法，其格式符合 SPIFFE 标准。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: httpbin namespace : httpbin-ns spec: hosts: - httpbin.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC endpoints: - address: 2.2.2.2 - address: 3.3.3.3 subjectAltNames: - \"spiffe://cluster.local/ns/httpbin-ns/sa/httpbin-service-account\" 下面的例子演示了使用带有workloadSelector的ServiceEntry来处理服务details.bookinfo.com从 VM 到 Kubernetes 的迁移。该服务有两个基于虚拟机的实例，带有 sidecar，以及一组由标准部署对象管理的 Kubernetes pod。网格中该服务的消费者将自动在虚拟机和 Kubernetes 之间进行负载均衡。details.bookinfo.com服务的虚拟机安装了 sidecar，并使用details-legacy服务账户进行引导。Sidecar 接收 80 端口的 HTTP 流量（用 istio mutual TLS 包装），并将其转发给同一端口的 localhost 上的应用程序。 apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: details-vm-1 spec: serviceAccount: details address: 2.2.2.2 labels: app: details instance-id: vm1 --- apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: details-vm-2 spec: serviceAccount: details address: 3.3.3.3 labels: app: details instance-id: vm2 假设还有一个 Kubernetes 部署，带有 pod 标签app: details，使用相同的服务账户details，下面的服务条目声明了一个横跨虚拟机和 Kubernetes 的服务。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: details-svc spec: hosts: - details.bookinfo.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC workloadSelector: labels: app: details 关于 ServiceEntry 配置的详细用法请参考 Istio 官方文档。 参考 Service Entry - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/workload-entry.html":{"url":"config/networking/workload-entry.html","title":"WorkloadEntry","keywords":"","body":"WorkloadEntry WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器，因为它被加入网格。一个 WorkloadEntry 必须伴随着一个 Istio ServiceEntry，通过适当的标签选择工作负载，并提供 MESH_INTERNAL 服务的服务定义（主机名、端口属性等）。一个 ServiceEntry 对象可以根据服务条目中指定的标签选择器来选择多个工作负载条目以及 Kubernetes pod。 当工作负载连接到 istiod 时，自定义资源中的状态字段将被更新，以表明工作负载的健康状况以及其他细节，类似于 Kubernetes 更新 pod 状态的方式。 示例 下面的例子声明了一个工作负载条目，代表 details.bookinfo.com 服务的一个虚拟机。这个虚拟机安装了 sidecar，并使用 details-legacy 服务账户进行引导。该服务通过 80 端口暴露给网格中的应用程序。通往该服务的 HTTP 流量被 Istio mTLS 封装，并被发送到目标端口 8080 的虚拟机上的 sidecar，后者又将其转发到同一端口的 localhost 上的应用程序。 apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: details-svc spec: # 使用服务账户表明工作负载有一个用该服务账户引导的 sidecar 代理。有 sidecar 的 pod 会自动使用 istio mTLS 与工作负载通信。 serviceAccount: details-legacy address: 2.2.2.2 labels: app: details-legacy instance-id: vm1 与其相关的服务条目如下。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: details-svc spec: hosts: - details.bookinfo.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP targetPort: 8080 resolution: STATIC workloadSelector: labels: app: details-legacy 下面的例子使用其完全限定的 DNS 名称声明了同一个虚拟机工作负载。服务条目的解析模式应改为 DNS，以表明客户端侧设备在转发请求之前应在运行时动态地解析 DNS 名称。 apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: details-svc spec: # 使用服务账户表明工作负载有一个用该服务账户引导的 sidecar 代理。有 sidecar 的 pod 会自动使用 istio mTLS 与工作负载通信。 serviceAccount: details-legacy address: vm1.vpc01.corp.net labels: app: details-legacy instance-id: vm1 与其相关的服务条目如下。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: details-svc spec: hosts: - details.bookinfo.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP targetPort: 8080 resolution: DNS workloadSelector: labels: app: details-legacy 关于 WorkloadEntry 配置的详细用法请参考 Istio 官方文档。 参考 WorkloadEntry - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/networking/workload-group.html":{"url":"config/networking/workload-group.html","title":"WorkloadGroup","keywords":"","body":"WorkloadGroup WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可用于启动其代理，包括元数据和身份。它只适用于虚拟机等非 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型，以引导 Istio 代理。 示例 下面的例子声明了一个代表工作负载集合的工作负载组，这些工作负载将在 bookinfo 命名空间的 reviews 下注册。在引导过程中，这组标签将与每个工作负载实例相关联，端口 3550 和 8080 将与工作负载组相关联，并使用 default 服务账户。app.kubernetes.io/version 只是一个标签的例子。 apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: reviews namespace: bookinfo spec: metadata: labels: app.kubernetes.io/name: reviews app.kubernetes.io/version: \"1.3.4\" template: ports: grpc: 3550 http: 8080 serviceAccount: default probe: initialDelaySeconds: 5 timeoutSeconds: 3 periodSeconds: 4 successThreshold: 3 failureThreshold: 3 httpGet: path: /foo/bar host: 127.0.0.1 port: 3100 scheme: HTTPS httpHeaders: - name: Lit-Header value: Im-The-Best 关于 WorkloadGroup 配置的详细用法请参考 Istio 官方文档。 参考 WorkloadGroup - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/security/":{"url":"config/security/","title":"安全配置概述","keywords":"","body":"安全配置概述 安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括： AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/security/authorization-policy.html":{"url":"config/security/authorization-policy.html","title":"AuthorizationPolicy","keywords":"","body":"AuthorizationPolicy AuthorizationPolicy（授权策略）实现了对网格中工作负载的访问控制。 授权策略支持访问控制的 CUSTOM、DENY 和 ALLOW 操作。当 CUSTOM、DENY 和 ALLOW 动作同时用于一个工作负载时，首先评估 CUSTOM 动作，然后是 DENY 动作，最后是 ALLOW 动作。评估是按以下顺序进行： 如果有任何 CUSTOM 策略与请求相匹配，如果评估结果为拒绝，则拒绝该请求。 如果有任何 DENY 策略与请求相匹配，则拒绝该请求。 如果没有适合该工作负载的 ALLOW 策略，允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，允许该请求。 拒绝该请求。 Istio 授权策略还支持 AUDIT 动作，以决定是否记录请求。AUDIT 策略不影响请求是否被允许或拒绝到工作负载。请求将完全基于 CUSTOM、DENY 和 ALLOW 动作被允许或拒绝。 如果工作负载上有一个与请求相匹配的 AUDIT 策略，则请求将被内部标记为应该被审计。必须配置并启用一个单独的插件，以实际履行审计决策并完成审计行为。如果没有启用这样的支持插件，该请求将不会被审计。目前，唯一支持的插件是 Stackdriver 插件。 示例 下面是一个 Istio 授权策略的例子。 它将 action 设置为 ALLOW 来创建一个允许策略。默认动作是 ALLOW，但在策略中明确规定是很有用的。 它允许请求来自： 服务账户 cluster.local/ns/default/sa/sleep 或 命名空间 test 来访问以下工作负载： 在前缀为 /info 的路径上使用 GET 方法，或者 在路径 /data 上使用 POST 方法 且当请求具有由 https://accounts.google.com 发布的有效 JWT 令牌时。 任何其他请求将被拒绝。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: action: ALLOW rules: - from: - source: principals: [\"cluster.local/ns/default/sa/sleep\"] - source: namespaces: [\"test\"] to: - operation: methods: [\"GET\"] paths: [\"/info*\"] - operation: methods: [\"POST\"] paths: [\"/data\"] when: - key: request.auth.claims[iss] values: [\"https://accounts.google.com\"] 下面是另一个例子，它将 action 设置为 DENY 以创建一个拒绝策略。它拒绝来自 dev 命名空间对 foo 命名空间中所有工作负载的 POST 请求。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: action: DENY rules: - from: - source: namespaces: [\"dev\"] to: - operation: methods: [\"POST\"] 下面的授权策略将 action 设置为 AUDIT。它将审核任何对前缀为 /user/profile 的路径的 GET 请求。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: namespace: ns1 name: anyname spec: selector: matchLabels: app: myapi action: AUDIT rules: - to: - operation: methods: [\"GET\"] paths: [\"/user/profile/*\"] 授权策略的范围（目标）由 metadata/namespace 和一个可选的 selector 决定。 metadata/namespace 告诉策略适用于哪个命名空间。如果设置为根命名空间，该策略适用于网格中的所有命名空间。 工作负载 selector 可以用来进一步限制策略的适用范围。 例如，以下授权策略适用于 bar 命名空间中包含标签 app: httpbin 的工作负载。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: policy namespace: bar spec: selector: matchLabels: app: httpbin 以下授权策略适用于命名空间 foo 中的所有工作负载。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: policy namespace: foo spec: {} 以下授权策略适用于网格中所有命名空间中包含标签 version: v1 的工作负载（假设根命名空间被配置为 istio-config）。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: policy namespace: istio-config spec: selector: matchLabels: version: v1 关于 AuthorizationPolicy 配置的详细用法请参考 Istio 官方文档。 关于认证策略条件的详细配置请参考 Istio 官方文档。 参考 Authorization Policy - istio.io Authorization Policy Conditions - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/security/request-authentication.html":{"url":"config/security/request-authentication.html","title":"RequestAuthentication","keywords":"","body":"RequestAuthentication RequestAuthentication（请求认证）定义了工作负载支持哪些请求认证方法。如果请求包含无效的认证信息，它将根据配置的认证规则拒绝该请求。不包含任何认证凭证的请求将被接受，但不会有任何认证的身份。 示例 为了限制只对经过认证的请求进行访问，应该伴随着一个授权规则。 例如，要求对具有标签 app:httpbin 的工作负载的所有请求使用 JWT 认证。 apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin jwtRules: - issuer: \"issuer-foo\" jwksUri: https://example.com/.well-known/jwks.json --- apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin rules: - from: - source: requestPrincipals: [\"*\"] 下一个例子展示了如何为不同的 host 设置不同的 JWT 要求。RequestAuthentication 声明它可以接受由 issuer-foo 或 issuer-bar 签发的 JWT（公钥集是由 OpenID Connect 规范隐性设置的）。 apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin jwtRules: - issuer: \"issuer-foo\" - issuer: \"issuer-bar\" --- apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin rules: - from: - source: requestPrincipals: [\"issuer-foo/*\"] to: - operation: hosts: [\"example.com\"] - from: - source: requestPrincipals: [\"issuer-bar/*\"] to: - operation: hosts: [\"another-host.com\"] 你可以对授权策略进行微调，为每个路径设置不同的要求。例如，除了 /healthz，所有路径都需要 JWT，可以使用相同的 RequestAuthentication，但授权策略可以是： apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: httpbin namespace: foo spec: selector: matchLabels: app: httpbin rules: - from: - source: requestPrincipals: [\"*\"] - to: - operation: paths: [\"/healthz\"] 关于 RequestAuthentication 配置的详细用法请参考 Istio 官方文档。 参考 RequestAuthentication - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/security/peer-authentication.html":{"url":"config/security/peer-authentication.html","title":"PeerAuthentication","keywords":"","body":"PeerAuthentication PeerAuthentication（对等认证）定义了流量将如何被隧道化（或不被隧道化）到 sidecar。 示例 策略允许命名空间 foo 下所有工作负载的 mTLS 流量。 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: foo spec: mtls: mode: STRICT 对于网格级别，根据你的 Istio 安装，将策略放在根命名空间。 策略允许命名空间 foo 下的所有工作负载的 mTLS 和明文流量，但 finance 的工作负载需要 mTLS。 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: foo spec: mtls: mode: PERMISSIVE --- apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: foo spec: selector: matchLabels: app: finance mtls: mode: STRICT 政策允许所有工作负载严格 mTLS，但 8080 端口保留为明文。 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: foo spec: selector: matchLabels: app: finance mtls: mode: STRICT portLevelMtls: 8080: mode: DISABLE 从命名空间（或网格）设置中继承 mTLS 模式的策略，并覆盖 8080 端口的设置。 apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: foo spec: selector: matchLabels: app: finance mtls: mode: UNSET portLevelMtls: 8080: mode: DISABLE 关于 PeerAuthentication 配置的详细用法请参考 Istio 官方文档。 参考 PeerAuthentication- istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"config/security/jwt.html":{"url":"config/security/jwt.html","title":"JWTRule","keywords":"","body":"JWTRule 用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见 OAuth 2.0 和 OIDC 1.0，了解在整个认证流程中如何使用。 示例 JWT 的规格是由 https://example.com 签发，受众要求必须是 bookstore_android.apps.example.com 或 bookstore_web.apps.example.com。该令牌应呈现在 Authorization header（默认）。Json 网络密钥集（JWKS）将按照 OpenID Connect 协议被发现。 issuer: https://example.com audiences: - bookstore_android.apps.example.com bookstore_web.apps.example.com 这个例子在非默认位置（x-goog-iap-jwt-assertion header）指定了令牌。它还定义了 URI 来明确获取 JWKS。 issuer: https://example.com jwksUri: https://example.com/.secret/jwks.json jwtHeaders: - \"x-goog-iap-jwt-assertion\" 关于 JWTRule 配置的详细用法请参考 Istio 官方文档。 参考 JWTRule - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/":{"url":"traffic-management/","title":"流量管理概述","keywords":"","body":"流量管理概述 在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我们将学习如何设置一个 Ingress 资源以允许流量进入我们的集群，以及一个 Egress 资源以使流量流出集群。 使用流量路由，我们将学习如何部署新版本的服务，并在已发布的生产版本的服务旁边运行，而不干扰生产流量。随着两个服务版本的部署，我们将逐步发布（金丝雀发布）新版本，并开始将一定比例的传入流量路由到最新版本。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/gateway.html":{"url":"traffic-management/gateway.html","title":"Gateway","keywords":"","body":"Gateway 作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关都运行一个 Envoy 代理实例，它们在网格的边缘作为负载均衡器运行。入口网关接收入站连接，而出口网关接收从集群出去的连接。 使用入口网关，我们可以对进入集群的流量应用路由规则。我们可以有一个指向入口网关的单一外部 IP 地址，并根据主机头将流量路由到集群内的不同服务。 图 6.2.1：入口和出口网关 我们可以使用 Gateway 资源来配置网关。网关资源描述了负载均衡器的暴露端口、协议、SNI（服务器名称指示）配置等。 下面是一个网关资源的例子： apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway namespace: default spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - dev.example.com - test.example.com 上述网关资源设置了一个代理，作为一个负载均衡器，为入口暴露 80 端口。网关配置被应用于 Istio 入口网关代理，我们将其部署到 istio-system 命名空间，并设置了标签 istio: ingressgateway。通过网关资源，我们只能配置负载均衡器。hosts 字段作为一个过滤器，只有以 dev.example.com 和 test.example.com 为目的地的流量会被允许通过。为了控制和转发流量到 Kubernetes 内部运行的实际服务，我们必须将 VirtualService 资源绑定到它。 图 6.2.2：Gateway 和 VirtualService 例如，我们作为 Istio 安装 demo 的一部分而部署的 Ingress 网关创建了一个具有 LoadBalancer 类型的 Kubernetes 服务，并为其分配了一个外部 IP： $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.0.146.214 80/TCP,443/TCP,15443/TCP 7m56s istio-ingressgateway LoadBalancer 10.0.98.7 XX.XXX.XXX.XXX 15021:31395/TCP,80:32542/TCP,443:31347/TCP,31400:32663/TCP,15443:31525/TCP 7m56s istiod ClusterIP 10.0.66.251 15010/TCP,15012/TCP,443/TCP,15014/TCP,853/TCP 8m6s LoadBalancer Kubernetes 服务类型的工作方式取决于我们运行 Kubernetes 集群的方式和地点。对于云管理的集群（GCP、AWS、Azure等），在你的云账户中配置了一个负载均衡器资源，Kubernetes LoadBalancer 服务将获得一个分配给它的外部 IP 地址。假设我们正在使用 Minikube 或 Docker Desktop。在这种情况下，外部 IP 地址将被设置为 localhost（Docker Desktop），或者，如果我们使用 Minikube，它将保持待定，我们将不得不使用 minikube tunnel 命令来获得一个IP地址。 除了入口网关，我们还可以部署一个出口网关来控制和过滤离开网格的流量。 就像我们配置入口网关一样，我们可以使用相同的网关资源来配置出口网关。这使我们能够集中管理所有流出的流量、日志和授权。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/basic-routing.html":{"url":"traffic-management/basic-routing.html","title":"基本路由","keywords":"","body":"基本路由 我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 VirtualService，我们可以定义流量路由规则，并在客户端试图连接到服务时应用这些规则。例如向 dev.example.com 发送一个请求，最终到达目标服务。 让我们看一下在集群中运行 customers 应用程序的两个版本（v1 和 v2）的例子。我们有两个 Kubernetes 部署，customers-v1 和 customers-v2。属于这些部署的 Pod 有一个标签 version：v1 或一个标签 version：v2 的设置。 图 6.3.1：路由到 Customers 我们想把 VirtualService 配置为将流量路由到应用程序的 V1 版本。70% 的传入流量应该被路由到 V1 版本。30% 的请求应该被发送到应用程序的 V2 版本。 下面是上述情况下 VirtualService 资源的样子： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: customers-route spec: hosts: - customers.default.svc.cluster.local http: - name: customers-v1-routes route: - destination: host: customers.default.svc.cluster.local subset: v1 weight: 70 - name: customers-v2-routes route: - destination: host: customers.default.svc.cluster.local subset: v2 weight: 30 在 hosts 字段下，我们要定义流量被发送到的目标主机。在我们的例子中，这就是 customers.default.svc.cluster.local Kubernetes 服务。 下一个字段是 http，这个字段包含一个 HTTP 流量的路由规则的有序列表。destination 是指服务注册表中的一个服务，也是路由规则处理后请求将被发送到的目的地。Istio 的服务注册表包含所有的 Kubernetes 服务，以及任何用 ServiceEntry 资源声明的服务。 我们也在设置每个目的地的权重（weight）。权重等于发送到每个子集的流量的比例。所有权重的总和应该是 100。如果我们有一个单一的目的地，权重被假定为 100。 通过 gateways 字段，我们还可以指定我们想要绑定这个 VirtualService 的网关名称。比如说： apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: customers-route spec: hosts: - customers.default.svc.cluster.local gateways: - my-gateway http: ... 上面的 YAML 将 customers-route VirtualService 绑定到名为 my-gateway 的网关上。这有效地暴露了通过网关的目标路由。 当一个 VirtualService 被附加到一个网关上时，只允许在网关资源中定义的主机。下表解释了网关资源中的 hosts 字段如何作为过滤器，以及 VirtualService 中的 hosts 字段如何作为匹配。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/subset-and-destinationrule.html":{"url":"traffic-management/subset-and-destinationrule.html","title":"Subset 和 DestinationRule","keywords":"","body":"Subset 和 DestinationRule 目的地指的是不同的子集（subset）或服务版本。通过子集，我们可以识别应用程序的不同变体。在我们的例子中，我们有两个子集，v1 和 v2，它们对应于我们 customer 服务的两个不同版本。每个子集都使用键/值对（标签）的组合来确定哪些 Pod 要包含在子集中。我们可以在一个名为 DestinationRule 的资源类型中声明子集。 下面是定义了两个子集的 DestinationRule 资源的样子。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: customers-destination spec: host: customers.default.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 让我们看看我们可以在 DestinationRule 中设置的流量策略。 DestinationRule 中的流量策略 通过 DestinationRule，我们可以定义设置，如负载均衡配置、连接池大小、局部异常检测等，在路由发生后应用于流量。我们可以在trafficPolicy字段下设置流量策略设置。以下是这些设置： 负载均衡器设置 连接池设置 局部异常点检测 客户端 TLS 设置 端口流量策略 负载均衡器设置 通过负载均衡器设置，我们可以控制目的地使用哪种负载均衡算法。下面是一个带有流量策略的 DestinationRule 的例子，它把目的地的负载均衡算法设置为 round-robin。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: customers-destination spec: host: customers.default.svc.cluster.local trafficPolicy: loadBalancer: simple: ROUND_ROBIN subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 我们还可以设置基于哈希的负载均衡，并根据 HTTP 头、cookies 或其他请求属性提供会话亲和性。下面是一个流量策略的片段，它设置了基于哈希的负载均衡，并使用一个叫做 location 的 cookie 来实现亲和力。 trafficPolicy: loadBalancer: consistentHash: httpCookie: name: location ttl: 4s 连接池配置 这些设置可以在 TCP 和 HTTP 层面应用于上游服务的每个主机，我们可以用它们来控制连接量。 下面是一个片段，显示了我们如何设置对服务的并发请求的限制。 spec: host: myredissrv.prod.svc.cluster.local trafficPolicy: connectionPool: http: http2MaxRequests: 50 异常点检测 异常点检测是一个断路器的实现，它跟踪上游服务中每个主机（Pod）的状态。如果一个主机开始返回 5xx HTTP 错误，它就会在预定的时间内被从负载均衡池中弹出。对于 TCP 服务，Envoy 将连接超时或失败计算为错误。 下面是一个例子，它设置了 500 个并发的 HTTP2 请求（http2MaxRequests）的限制，每个连接不超过 10 个请求（maxRequestsPerConnection）到该服务。每 5 分钟扫描一次上游主机（Pod）（interval），如果其中任何一个主机连续失败 10 次（contracticalErrors），Envoy 会将其弹出 10 分钟（baseEjectionTime）。 trafficPolicy: connectionPool: http: http2MaxRequests: 500 maxRequestsPerConnection: 10 outlierDetection: consecutiveErrors: 10 interval: 5m baseEjectionTime: 10m 客户端 TLS 设置 包含任何与上游服务连接的 TLS 相关设置。下面是一个使用提供的证书配置 mTLS 的例子。 trafficPolicy: tls: mode: MUTUAL clientCertificate: /etc/certs/cert.pem privateKey: /etc/certs/key.pem caCertificates: /etc/certs/ca.pem 其他支持的 TLS 模式有 DISABLE（没有 TLS 连接），SIMPLE（在上游端点发起 TLS 连接），以及 ISTIO_MUTUAL（与 MUTUAL 类似，使用 Istio 的 mTLS 证书）。 端口流量策略 使用 portLevelSettings 字段，我们可以将流量策略应用于单个端口。比如说： trafficPolicy: portLevelSettings: - port: number: 80 loadBalancer: simple: LEAST_CONN - port: number: 8000 loadBalancer: simple: ROUND_ROBIN Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/resiliency.html":{"url":"traffic-management/resiliency.html","title":"弹性","keywords":"","body":"弹性 弹性（Resiliency）是指在面对故障和对正常运行的挑战时，提供和保持可接受的服务水平的能力。这不是为了避免故障。它是以一种没有停机或数据丢失的方式来应对它们。弹性的目标是在故障发生后将服务恢复到一个完全正常的状态。 使服务可用的一个关键因素是在提出服务请求时使用超时（timeout）和重试（retry）策略。我们可以在 Istio 的 VirtualService 上配置这两者。 使用超时字段，我们可以为 HTTP 请求定义一个超时。如果请求的时间超过了超时字段中指定的值，Envoy 代理将放弃请求，并将其标记为超时（向应用程序返回一个 HTTP 408）。连接将保持开放，除非触发了异常点检测。下面是一个为路由设置超时的例子： ... - route: - destination: host: customers.default.svc.cluster.local subset: v1 timeout: 10s ... 除了超时之外，我们还可以配置更细化的重试策略。我们可以控制一个给定请求的重试次数，每次尝试的超时时间，以及我们想要重试的具体条件。 例如，我们可以只在上游服务器返回任何 5xx 响应代码时重试请求，或者只在网关错误（HTTP 502、503 或 504）时重试，或者甚至在请求头中指定可重试的状态代码。重试和超时都发生在客户端。当 Envoy 重试一个失败的请求时，最初失败并导致重试的端点就不再包含在负载均衡池中了。假设 Kubernetes 服务有 3 个端点（Pod），其中一个失败了，并出现了可重试的错误代码。当 Envoy 重试请求时，它不会再向原来的端点重新发送请求。相反，它将把请求发送到两个没有失败的端点中的一个。 下面是一个例子，说明如何为一个特定的目的地设置重试策略。 ... - route: - destination: host: customers.default.svc.cluster.local subset: v1 retries: attempts: 10 perTryTimeout: 2s retryOn: connect-failure,reset ... 上述重试策略将尝试重试任何连接超时（connect-failure）或服务器完全不响应（reset）的失败请求。我们将每次尝试的超时时间设置为 2 秒，尝试的次数设置为 10 次。注意，如果同时设置重试和超时，超时值将是请求等待的最长时间。如果我们在上面的例子中指定了 10 秒的超时，那么即使重试策略中还剩下一些尝试，我们也只能最多等待 10 秒。 关于重试策略的更多细节，请参阅 x-envoy-retry-on 文档。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/fault-injection.html":{"url":"traffic-management/fault-injection.html","title":"错误注入","keywords":"","body":"错误注入 为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入。 有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟缓慢的网络或过载的服务，我们可以中止（abort） HTTP 请求，并返回一个特定的 HTTP 错误代码给调用者。通过中止，我们可以模拟一个有故障的上游服务。 下面是一个中止 HTTP 请求并返回 HTTP 404 的例子，针对 30% 的传入请求。 - route: - destination: host: customers.default.svc.cluster.local subset: v1 fault: abort: percentage: value: 30 httpStatus: 404 如果我们不指定百分比，所有的请求将被中止。请注意，故障注入会影响使用该 VirtualService 的服务。它并不影响该服务的所有消费者。 同样地，我们可以使用 fixedDelay 字段对请求应用一个可选的延迟。 - route: - destination: host: customers.default.svc.cluster.local subset: v1 fault: delay: percentage: value: 5 fixedDelay: 3s 上述设置将对 5% 的传入请求应用 3 秒的延迟。 注意，故障注入将不会触发我们在路由上设置的任何重试策略。例如，如果我们注入了一个 HTTP 500 的错误，配置为在 HTTP 500 上的重试策略将不会被触发。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/advanced-routing.html":{"url":"traffic-management/advanced-routing.html","title":"高级路由","keywords":"","body":"高级路由 在前面，我们了解了如何利用流量的比例（weight 字段）在多个子集之间进行流量路由。在某些情况下，纯粹的基于权重的流量路由或分割已经足够了。然而，在有些场景和情况下，我们可能需要对流量如何被分割和转发到目标服务进行更细化的控制。 Istio 允许我们使用传入请求的一部分，并将其与定义的值相匹配。例如，我们可以匹配传入请求的 URI前缀，并基于此路由流量。 属性 描述 uri 将请求 URI 与指定值相匹配 schema 匹配请求的 schema（HTTP、HTTPS...） method 匹配请求的 method（GET、POST...） authority 匹配请求 authority 头 headers 匹配请求头。头信息必须是小写的，并以连字符分隔（例如：x-my-request-id）。注意，如果我们使用头信息进行匹配，其他属性将被忽略（uri、schema、method、authority）。 上述每个属性都可以用这些方法中的一种进行匹配： 精确匹配：例如，exact: \"value\" 匹配精确的字符串 前缀匹配：例如，prefix: \"value\" 只匹配前缀 这则匹配：例如，regex：\"value\" 根据 ECMAscript 风格的正则进行匹配 例如，假设请求的 URI 看起来像这样：https://dev.example.com/v1/api。为了匹配该请求的 URI，我们会这样写： http: - match: - uri: prefix: /v1 上述片段将匹配传入的请求，并且请求将被路由到该路由中定义的目的地。 另一个例子是使用正则并在头上进行匹配。 http: - match: - headers: user-agent: regex: '.*Firefox.*' 上述匹配将匹配任何用户代理头与 Regex 匹配的请求。 重定向和重写请求 在头信息和其他请求属性上进行匹配是有用的，但有时我们可能需要通过请求 URI 中的值来匹配请求。 例如，让我们考虑这样一种情况：传入的请求使用 /v1/api 路径，而我们想把请求路由到 /v2/api 端点。 这样做的方法是重写所有传入的请求和与 /v1/api 匹配的 authority/host headers 到 /v2/api。 例如： ... http: - match: - uri: prefix: /v1/api rewrite: uri: /v2/api route: - destination: host: customers.default.svc.cluster.local ... 即使目标服务不在 /v1/api 端点上监听，Envoy 也会将请求重写到 /v2/api。 我们还可以选择将请求重定向或转发到一个完全不同的服务。下面是我们如何在头信息上进行匹配，然后将请求重定向到另一个服务： ... http: - match: - headers: my-header: exact: hello redirect: uri: /hello authority: my-service.default.svc.cluster.local:8000 ... redirect 和 destination 字段是相互排斥的。如果我们使用 redirect，就不需要设置 destination。 AND 和 OR 语义 在进行匹配时，我们可以使用 AND 和 OR 两种语义。让我们看一下下面的片段： ... http: - match: - uri: prefix: /v1 headers: my-header: exact: hello ... 上面的片段使用的是 AND 语义。这意味着 URI 前缀需要与 /v1 相匹配，并且头信息 my-header 有一个确切的值 hello。 要使用 OR 语义，我们可以添加另一个 match 项，像这样： ... http: - match: - uri: prefix: /v1 ... - match: - headers: my-header: exact: hello ... 在上面的例子中，将首先对 URI 前缀进行匹配，如果匹配，请求将被路由到目的地。如果第一个不匹配，算法会转移到第二个，并尝试匹配头。如果我们省略路由上的匹配字段，它将总是评估为 true。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/seviceentry.html":{"url":"traffic-management/seviceentry.html","title":"ServiceEntry","keywords":"","body":"ServiceEntry 通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使不属于我们网格的外部服务或内部服务看起来像是我们服务网格的一部分。 当一个服务在服务注册表中时，我们就可以使用流量路由、故障注入和其他网格功能，就像我们对其他服务一样。 下面是一个 ServiceEntry 资源的例子，它声明了一个可以通过 HTTPS 访问的外部 API（api.external-svc.com）。 apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-svc spec: hosts: - api.external-svc.com ports: - number: 443 name: https protocol: TLS resolution: DNS location: MESH_EXTERNAL hosts 字段可以包含多个外部 API，在这种情况下，Envoy sidecar 会根据下面的层次结构来进行检查。如果任何一项不能被检查，Envoy 就会转到层次结构中的下一项。 HTTP Authority 头（在HTTP/2中）和 Host 头（HTTP/1.1中） SNI IP 地址和端口 如果上述数值都无法检查，Envoy 会根据 Istio 的安装配置，盲目地转发请求或放弃该请求。 与 WorkloadEntry 资源一起，我们可以处理虚拟机工作负载向 Kubernetes 迁移的问题。在 WorkloadEntry 中，我们可以指定在虚拟机上运行的工作负载的细节（名称、地址、标签），然后使用 ServiceEntry 中的 workloadSelector 字段，使虚拟机成为 Istio 内部服务注册表的一部分。 例如，假设 customers 的工作负载正在两个虚拟机上运行。此外，我们已经有在 Kubernetes 中运行的 Pod，其标签为 app: customers。 让我们这样来定义 WorkloadEntry 资源： apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: customers-vm-1 spec: serviceAccount: customers address: 1.0.0.0 labels: app: customers instance-id: vm1 --- apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: customers-vm-2 spec: serviceAccount: customers address: 2.0.0.0 labels: app: customers instance-id: vm2 现在我们可以创建一个 ServiceEntry 资源，该资源同时跨越 Kubernetes 中运行的工作负载和虚拟机： apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: customers-svc spec: hosts: - customers.com location: MESH_INTERNAL ports: - number: 80 name: http protocol: HTTP resolution: STATIC workloadSelector: labels: app: customers 在位置字段中设置 MESH_INTERNAL，我们是说这个服务是网格的一部分。这个值通常用于包括未管理的基础设施（VM）上的工作负载的情况。这个字段的另一个值，MESH_EXTERNAL，用于通过 API 消费的外部服务。MESH_INTERNAL 和 MESH_EXTERNAL 设置控制了网格中的 sidecar 如何尝试与工作负载进行通信，包括它们是否会默认使用 Istio 双向 TLS。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/sidecar.html":{"url":"traffic-management/sidecar.html","title":"Sidecar","keywords":"","body":"Sidecar 默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量，并且在转发流量时可以到达网格中的任何服务。 在某些情况下，你可能想改变这种配置，配置代理，所以它只能使用特定的端口和访问某些服务。要做到这一点，你可以在 Istio 中使用 Sidecar 资源。 Sidecar 资源可以被部署到 Kubernetes 集群内的一个或多个命名空间，但如果没有定义工作负载选择器，每个命名空间只能有一个 sidecar 资源。 Sidecar 资源由三部分组成，一个工作负载选择器、一个入口（ingress）监听器和一个出口（egress）监听器。 工作负载选择器 工作负载选择器决定了哪些工作负载会受到 sidecar 配置的影响。你可以决定控制一个命名空间中的所有 sidecar，而不考虑工作负载，或者提供一个工作负载选择器，将配置只应用于特定的工作负载。 例如，这个 YAML 适用于默认命名空间内的所有代理，因为没有定义选择器。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default-sidecar namespace: default spec: egress: - hosts: - \"default/*\" - \"istio-system/*\" - \"staging/*\" 在 egress 部分，我们指定代理可以访问运行在 default、istio-system 和 staging 命名空间的服务。要将资源仅应用于特定的工作负载，我们可以使用 workloadSelector 字段。例如，将选择器设置为 `version: v1 将只适用于有该标签设置的工作负载。 apiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default-sidecar namespace: default spec: workloadSelector: labels: version: v1 egress: - hosts: - \"default/*\" - \"istio-system/*\" - \"staging/*\" 入口和出口监听器 资源的入口（ingress）监听器部分定义了哪些入站流量被接受。同样地，通过出口（egress）监听器，你可以定义出站流量的属性。 每个入口监听器都需要一个端口设置，以便接收流量（例如，下面的例子中的 3000）和一个默认的端点。默认端点可以是一个回环 IP 端点或 Unix 域套接字。端点配置了流量将被转发到哪里。 ... ingress: - port: number: 3000 protocol: HTTP name: somename defaultEndpoint: 127.0.0.1:8080 ... 上面的片段将入口监听器配置为在端口 3000 上监听，并将流量转发到服务监听的端口 8080 上的回环 IP。此外，我们可以设置 bind 字段，以指定一个 IP 地址或域套接字，我们希望代理监听传入的流量。最后，字段 captureMode 可以用来配置如何以及是否捕获流量。 出口监听器有类似的字段，但增加了hosts 字段。通过 hosts 字段，你可以用 namespace/dnsName 的格式指定服务主机。例如， myservice.default 或 default/*。在 hosts 字段中指定的服务可以是来自网格注册表的实际服务、外部服务（用 ServiceEntry 定义），或虚拟服务。 egress: - port: number: 8080 protocol: HTTP hosts: - \"staging/*\" 通过上面的 YAML，sidecar 代理了运行在 staging 命名空间的服务的 8080 端口的流量。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"traffic-management/envoyfilter.html":{"url":"traffic-management/envoyfilter.html","title":"EnvoyFilter","keywords":"","body":"EnvoyFilter EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。 过滤器是叠加应用的，这意味着对于特定命名空间中的特定工作负载，可以有任何数量的过滤器。根命名空间（例如 istio-system）中的过滤器首先被应用，然后是工作负载命名空间中的所有匹配过滤器。 下面是一个 EnvoyFilter 的例子，它在请求中添加了一个名为 api-version 的头。 apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: api-header-filter namespace: default spec: workloadSelector: labels: app: web-frontend configPatches: - applyTo: HTTP_FILTER match: context: SIDECAR_INBOUND listener: portNumber: 8080 filterChain: filter: name: \"envoy.http_connection_manager\" subFilter: name: \"envoy.router\" patch: operation: INSERT_BEFORE value: name: envoy.lua typed_config: \"@type\": \"type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\" inlineCode: | function envoy_on_response(response_handle) response_handle:headers():add(\"api-version\", \"v1\") end 如果你向 $GATEWAY_URL 发送一个请求，你可以注意到 api-version 头被添加了，如下所示： $ curl -s -I -X HEAD http://$GATEWAY_URL HTTP/1.1 200 OK x-powered-by: Express content-type: text/html; charset=utf-8 content-length: 2471 etag: W/\"9a7-hEXE7lJW5CDgD+e2FypGgChcgho\" date: Tue, 17 Nov 2020 00:40:16 GMT x-envoy-upstream-service-time: 32 api-version: v1 server: istio-envoy Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"observability/":{"url":"observability/","title":"可观察性概述","keywords":"","body":"可观察性概述 在本章中，我们将学习一些监控（Prometheus）、追踪（Zipkin）和数据可视化工具（Grafana）。 什么是可观察性？ 由于采用了 sidecar 部署模式，即 Envoy 代理运行在应用实例旁边并拦截流量，这些代理也收集指标。 Envoy 代理收集的指标可以帮助我们获得系统状态的可见性。获得系统的这种可见性是至关重要的，因为我们需要了解正在发生的事情，并授权运维人员对应用程序进行故障排除、维护和优化。 Istio 生成三种类型的遥测数据，为网格中的服务提供可观察性： 指标度量（Metric） 分布式跟踪 访问日志 指标度量 Istio 基于四个黄金信号生成指标：延迟、流量、错误和饱和度。 延迟表示服务一个请求所需的时间。这个指标应该分成成功请求（如 HTTP 200）和失败请求（如 HTTP 500）的延迟。 流量是衡量对系统的需求有多大，它是以系统的具体指标来衡量的。例如，每秒的 HTTP 请求，或并发会话，每秒的检索量，等等。 错误用来衡量请求失败的比率（例如 HTTP 500）。 饱和度衡量一个服务中最紧张的资源有多满。例如，线程池的利用率。 这些指标是在不同的层面上收集的，首先是最细的，即 Envoy 代理层面，然后是服务层面和控制面的指标。 代理级指标 生成指标的一个关键角色是 Envoy，它生成了一套关于所有通过代理的流量的丰富指标。使用 Envoy 生成的指标，我们可以以最低的粒度来监控服务网格，例如 Envoy 代理中的 inidivdual 监听器和集群的指标。 作为网格运维人员，我们有能力控制在每个2工作负载实例中生成和收集哪些 Envoy 指标。 下面是几个代理级指标的例子。 envoy_cluster_internal_upstream_rq{response_code_class=\"2xx\",cluster_name=\"xds-grpc\"} 7163 envoy_cluster_upstream_rq_completed{cluster_name=\"xds-grpc\"} 7164 envoy_cluster_ssl_connection_error{cluster_name=\"xds-grpc\"} 0 envoy_cluster_lb_subsets_removed{cluster_name=\"xds-grpc\"} 0 envoy_cluster_internal_upstream_rq{response_code=\"503\",cluster_name=\"xds-grpc\"} 1 注意你可以从每个 Envoy 代理实例的 /stats 端点查看代理级指标。 服务级指标 服务级别的指标涵盖了我们前面提到的四个黄金信号。这些指标使我们能够监控服务与服务之间的通信。此外，Istio 还提供了一组仪表盘，我们可以根据这些指标来监控服务行为。 就像代理级别的指标一样，运营商可以自定义收集哪些服务级别的指标。 默认情况下，Istio 的标准指标集会被导出到 Prometheus。 下面是几个服务级指标的例子。 控制平面度量 Istio 也会发射控制平面指标，可以帮助监控 Istio 的控制平面和行为，而不是用户服务。 输出的控制平面指标的完整列表可以在这里找到。 控制平面指标包括冲突的入站/出站监听器的数量、没有实例的集群数量、被拒绝或被忽略的配置等指标。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"observability/prometheus.html":{"url":"observability/prometheus.html","title":"Prometheus","keywords":"","body":"Prometheus Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记录指标，跟踪 Istio 和网格中的应用程序的健康状况。 要安装 Prometheus，我们可以使用 Istio 安装包中 /samples/addons 文件夹中的示例安装。 $ kubectl apply -f istio-1.9.0/samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created 要打开 Prometheus 仪表板，我们可以使用 Istio CLI 中的 dashboard 命令： $ istioctl dashboard prometheus http://localhost:9090 现在我们可以在浏览器中打开 http://localhost:9090，进入 Prometheus 仪表板，如下图所示。 图 7.2.1：Prometheus dashboard 部署示例应用 为了看到一些请求和流量，我们将部署一个 Nginx 实例： $ kubectl create deploy my-nginx --image=nginx deployment.apps/my-nginx created 为了能够产生一些流量并访问 Nginx Pod，我们需要以某种方式让它被访问。 最简单的方法是将 Nginx 部署作为 Kubernetes LoadBalancer 服务公开： kubectl expose deployment my-nginx --type=LoadBalancer --name=my-nginx --port 80 注意：在课程的后面，我们将学习如何使用 Istio 资源并通过 Istio 的入口网关暴露服务。 现在我们可以运行 kubectl get services，获得 my-nginx 服务的外部 IP 地址： $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.48.0.1 443/TCP 73m my-nginx LoadBalancer 10.48.0.94 [IP HERE] 80:31191/TCP 4m6s 让我们把这个 IP 地址存储为一个环境变量，这样我们就可以在整个实验中使用它。 export NGINX_IP=$(kubectl get service my-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}') 现在你可以对上述 IP 运行 curl，你应该得到默认的 Nginx 页面。 $ curl $NGINX_IP Welcome to nginx! body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. 让我们向我们在开始时创建的 $NGINX_IP 环境变量提出几个请求。然后，从 Prometheus UI 中，你可以搜索 Istio 的一个指标（例如 istio_requests_total），以了解哪些数据点正在被收集。 下面是一个来自 Prometheus 用户界面的示例元素： istio_requests_total{app=\"my-nginx\", connection_security_policy=\"none\", destination_app=\"my-nginx\", destination_canonical_revision=\"latest\", destination_canonical_service=\"my-nginx\", destination_cluster=\"Kubernetes\", destination_principal=\"unknown\", destination_service=\"my-nginx.default.svc.cluster.local\", destination_service_name=\"my-nginx\", destination_service_namespace=\"default\", destination_version=\"unknown\", destination_workload=\"my-nginx\", destination_workload_namespace=\"default\", instance=\"10.92.4.4:15020\", istio_io_rev=\"default\", job=\"kubernetes-pods\", kubernetes_namespace=\"default\", kubernetes_pod_name=\"my-nginx-6b74b79f57-r59sf\", pod_template_hash=\"6b74b79f57\", reporter=\"destination\", request_protocol=\"http\", response_code=\"200\", response_flags=\"-\", security_istio_io_tlsMode=\"istio\", service_istio_io_canonical_name=\"my-nginx\", service_istio_io_canonical_revision=\"latest\", source_app=\"unknown\", source_canonical_revision=\"latest\", source_canonical_service=\"unknown\", source_cluster=\"unknown\", source_principal=\"unknown\", source_version=\"unknown\", source_workload=\"unknown\", source_workload_namespace=\"unknown\"} 12 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"observability/grafana.html":{"url":"observability/grafana.html","title":"Grafana","keywords":"","body":"Grafana Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各种数据源，并使用图形、表格、热图等将数据可视化。通过强大的查询语言，你可以定制现有的仪表盘并创建更高级的可视化。 通过 Grafana，我们可以监控 Istio 安装和服务网格中运行的应用程序的健康状况。 我们可以使用 grafana.yaml 来部署带有预配置仪表盘的 Grafana 示例安装。该 YAML 文件在 Istio 安装包的 /samples/addons 下。 确保在部署 Grafana 之前部署 Promeheus 插件，因为 Grafana 使用 Prometheus 作为其数据源。 运行下面的命令来部署 Grafana 和预配置的仪表盘： $ kubectl apply -f istio-1.9.0/samples/addons/grafana.yaml serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created 我们不打算在生产中运行这个 Grafana，因为它没有经过性能或安全方面的优化。 Kubernetes 将 Grafana 部署在 istio-system 命名空间。要访问 Grafana，我们可以使用 istioctl dashboard 命令。 $ istioctl dashboard grafana http://localhost:3000 我们可以在浏览器中打开 http://localhost:3000，进入 Grafana。然后，点击首页和 Istio 文件夹，查看已安装的仪表板，如下图所示。 图 7.3.1：Grafana 仪表板 Istio Grafana 安装时预配置了以下仪表板： 1. Istio 控制平面仪表板 从 Istio 控制平面仪表板，我们可以监控 Istio 控制平面的健康和性能。 图 7.3.2：Istio 控制平面仪表板 这个仪表板将向我们显示控制平面的资源使用情况（内存、CPU、磁盘、Go routines），以及关于 Pilot 、Envoy 和 Webhook 的信息。 2. Istio 网格仪表板 网格仪表盘为我们提供了在网格中运行的所有服务的概览。仪表板包括全局请求量、成功率以及 4xx 和 5xx 响应的数量。 图 7.3.3：Istio 网格仪表板 3. Istio 性能仪表板 性能仪表盘向我们展示了 Istio 主要组件在稳定负载下的资源利用率。 图 7.3.4：Istio 性能仪表板 4. Istio 服务仪表板 服务仪表板允许我们在网格中查看关于我们服务的细节。 我们可以获得关于请求量、成功率、持续时间的信息，以及显示按来源和响应代码、持续时间和大小的传入请求的详细图表。 图 7.3.5：Istio 服务仪表板 5. Istio Wasm 扩展仪表板 Istio Wasm 扩展仪表板显示与 WebAssembly 模块有关的指标。从这个仪表板，我们可以监控活动的和创建的 Wasm 虚拟机，关于获取删除 Wasm 模块和代理资源使用的数据。 图 7.3.6：Istio Wasm 扩展仪表板 6. Istio 工作负载仪表板 这个仪表板为我们提供了一个工作负载的详细指标分类。 图 7.3.7：Istio 工作负载仪表板 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"observability/zipkin.html":{"url":"observability/zipkin.html","title":"Zipkin","keywords":"","body":"Zipkin 分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我们可以在请求通过被监控系统的不同部分时追踪它们。 每当一个请求进入服务网格时，Envoy 都会生成一个唯一的请求 ID 和追踪信息，并将其作为 HTTP 头的一部分来存储。任何应用程序都可以将这些头信息转发给它所调用的其他服务，以便在系统中创建一个完整的追踪。 分布式追踪是一个跨度（span）的集合。当请求流经不同的系统组件时，每个组件都会贡献一个跨度。每个跨度都有一个名称，开始和结束的时间戳，一组称为标签（tag）和日志（log）的键值对，以及一个跨度上下文。 标签被应用于整个跨度，并用于查询和过滤。下面是我们在使用 Zipkin 时将看到的几个标签的例子。注意，其中有些是通用的，有些是 Istio 特有的。 istio.mesh_id istio.canonical_service upstream_cluster http.url http.status_code zone 单个跨度与识别跨度、父跨度、追踪 ID 的上下文头一起被发送到一个叫做采集器的组件。采集器对数据进行验证、索引和存储。 当请求流经 Envoy 代理时，Envoy 代理会自动发送各个跨度。请注意，Envoy 只能在边缘收集跨度。我们要负责在每个应用程序中生成任何额外的跨度，并确保我们在调用其他服务时转发追踪头信息。这样一来，各个跨度就可以正确地关联到一个单一的追踪中。 使用 Zipkin 进行分布式追踪 Zipkin 是一个分布式跟踪系统。我们可以轻松地监控服务网格中发生的分布式事务，发现任何性能或延迟问题。 为了让我们的服务参与分布式跟踪，我们需要在进行任何下游服务调用时传播服务的 HTTP 头信息。尽管所有的请求都要经过 Istio sidecar，但 Istio 没有办法将出站请求与产生这些请求的入站请求联系起来。通过在应用程序中传播相关的头信息可以帮助 Zipkin 将这些跟踪信息拼接起来。 Istio 依赖于 B3 跟踪头（以 x-b3 开头的 header）和 Envoy 生成的请求 ID（x-request-id）。B3 头信息用于跨服务边界的跟踪上下文传播。 以下是我们需要在我们的应用程序中对每个发出的请求进行传播的特定头文件名称： x-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3 如果你使用 Lightstep，你还需要转发名为 x-ot-span-context 的头。 传播头信息最常见的方法是从传入的请求中复制它们，并将它们包含在所有从你的应用程序发出的请求中。 你用 Istio 服务网格得到的跟踪只在服务边界捕获。为了了解应用程序的行为并排除故障，你需要通过创建额外的跨度（span）来正确检测你的应用程序。 要安装 Zipkin，我们可以使用 addons 文件夹中的 zipkin.yaml 文件。 $ kubectl apply -f istio-1.9.0/samples/addons/extras/zipkin.yaml deployment.apps/zipkin created service/tracing created service/zipkin created 我们可以通过运行 getmesh istioctl dashboard zipkin 来打开 Zipkin 仪表板。在用户界面上，我们可以选择跟踪查询的标准。点击按钮，从下拉菜单中选择 serviceName，然后选择 customers.default service，点击搜索按钮（或按回车键），就可以搜索到 trace 信息。 图 7.4.1：Zipkin Dashboard 我们可以点击个别 trace 来深入挖掘不同的跨度。详细的视图将显示服务之间的调用时间，以及请求的细节，如方法、协议、状态码等。由于我们只有一个服务在运行（Nginx），所以你不会看到很多细节。稍后，我们将回到 Zipkin，更详细地探索这些 trace。 图 7.4.2：Zipkin trace 详情 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"observability/kiali.html":{"url":"observability/kiali.html","title":"Kiali","keywords":"","body":"Kiali Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观察性，并让我们通过强大的配置和验证能力来操作网格。它通过推断流量拓扑来显示服务网格，并显示网格的健康状况。Kiali 提供了详细的指标，强大的验证，Grafana 访问，以及与 Jaeger 的分布式跟踪的强大集成。 要安装 Kiali，请使用 addons 文件夹中的 kiali.yaml 文件： $ kubectl apply -f istio-1.9.0/samples/addons/kiali.yaml customresourcedefinition.apiextensions.k8s.io/monitoringdashboards.monito ring.kiali.io created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created service/kiali created deployment.apps/kiali created 注意，如果你看到任何错误，例如在版本 monitoringkiali.io/v1alpha 中没有匹配的 MonitoringDashboard，请再次重新运行 kubectl apply 命令。问题是，在安装 CRD（自定义资源定义）和由该 CRD 定义的资源时，可能存在一个匹配条件。 我们可以用 getmesh istioctl dashboard kiali 打开 Kiali。 Kiali 可以生成一个像下图这样的服务图。 图 7.5.1：Kiali Graph 该图向我们展示了服务的拓扑结构，并将服务的通信方式可视化。它还显示了入站和出站的指标，以及通过连接 Jaeger 和 Grafana（如果安装了）的追踪。图中的颜色代表服务网格的健康状况。颜色为红色或橙色的节点可能需要注意。组件之间的边的颜色代表这些组件之间的请求的健康状况。节点形状表示组件的类型，如服务、工作负载或应用程序。 节点和边的健康状况会根据用户的偏好自动刷新。该图也可以暂停以检查一个特定的状态，或重放以重新检查一个特定的时期。 Kiali 提供创建、更新和删除 Istio 配置的操作，由向导驱动。我们可以配置请求路由、故障注入、流量转移和请求超时，所有这些都来自用户界面。如果我们有任何现有的 Istio 配置已经部署，Kiali 可以验证它并报告任何警告或错误。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"security/":{"url":"security/","title":"安全概述","keywords":"","body":"安全概述 在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策略、安全命名和身份。 在 Istio 中，有多个组件参与提供安全功能： 用于管理钥匙和证书的证书颁发机构（CA）。 Sidecar 和周边代理：实现客户端和服务器之间的安全通信，它们作为政策执行点（Policy Enforcement Point，简称PEP）工作 Envoy 代理扩展：管理遥测和审计 配置 API 服务器：分发认证、授权策略和安全命名信息 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"security/authn.html":{"url":"security/authn.html","title":"认证","keywords":"","body":"认证 为了解释什么是认证或 authn，我们将从访问控制试图回答的问题开始：一个主体能否对一个对象执行操作？ 如果我们把上述问题翻译成 Istio 和 Kubernetes 的世界，它将是 \"服务 X 能否对服务 Y 进行操作？\" 这个问题的三个关键部分是：主体、动作和对象。 主体和对象都是 Kubernetes 中的服务。动作，假设我们谈论的是 HTTP，是一个 GET 请求，一个 POST，一个 PUT 等等。 认证是关于主体（或者在我们的例子中是服务的身份）的。认证是验证某种凭证的行为，并确保该凭证是有效和可信的。一旦进行了认证，我们就有了一个经过认证的主体。下次你旅行时，你向海关官员出示你的护照或身份证，他们会对其进行认证，确保你的凭证（护照或身份证）是有效和可信的。 在 Kubernetes 中，每个工作负载都被分配了一个独特的身份，它用来与其他每个工作负载进行通信 - 该身份以服务账户的形式提供给工作负载。服务账户是运行时中存在的身份 Pods。 Istio 使用来自服务账户的 X.509 证书，它根据名为 SPIFFE（每个人的安全生产身份框架）的规范创建一个新的身份。 证书中的身份被编码在证书的 Subject alternate name 字段中，它看起来像这样。 spiffe://cluster.local/ns//sa/ 当两个服务开始通信时，它们需要交换带有身份信息的凭证，以相互验证自己。客户端根据安全命名信息检查服务器的身份，看它是否是服务的授权运行者。 服务器根据授权策略确定客户可以访问哪些信息。此外，服务器可以审计谁在什么时间访问了什么，并决定是否批准或拒绝客户对服务器的调用。 安全命名信息包含从服务身份到服务名称的映射。服务器身份是在证书中编码的，而服务名称是由发现服务或 DNS 使用的名称。从一个身份 A 到一个服务名称 B 的单一映射意味着 \"A 被允许和授权运行服务 B\"。安全命名信息由 Pilot 生成，然后分发给所有 sidecar 代理。 证书创建和轮换 对于网格中的每个工作负载，Istio 提供一个 X.509 证书。一个名为 pilot-agent 的代理在每个 Envoy 代理旁边运行，并与控制平面（istiod）一起工作，自动进行密钥和证书的轮转。 图 8.2.1：证书和秘钥管理 在运行时创建身份时，有三个部分在起作用： Citadel（控制平面的一部分） Istio 代理 Envoy 的秘密发现服务（SDS） Istio Agent 与 Envoy sidecar 一起工作，通过安全地传递配置和秘密，帮助它们连接到服务网格。即使 Istio 代理在每个 pod 中运行，我们也认为它是控制平面的一部分。 秘密发现服务（SDS）简化了证书管理。如果没有 SDS，证书必须作为秘密（Secret）创建，然后装入代理容器的文件系统中。当证书过期时，需要更新秘密，并重新部署代理，因为 Envoy 不会从磁盘动态重新加载证书。当使用 SDS 时，SDS 服务器将证书推送给 Envoy 实例。每当证书过期时，SDS 会推送更新的证书，Envoy 可以立即使用它们。不需要重新部署代理服务器，也不需要中断流量。在 Istio 中，Istio Agent 作为 SDS 服务器，实现了秘密发现服务接口。 每次我们创建一个新的服务账户时，Citadel 都会为它创建一个 SPIFFE 身份。每当我们安排一个工作负载时，Pilot 会用包括工作负载的服务账户在内的初始化信息来配置其 sidecar。 当工作负载旁边的 Envoy 代理启动时，它会联系 Istio 代理并告诉它工作负载的服务账户。代理验证该实例，生成 CSR（证书签名请求），将 CSR 以及工作负载的服务账户证明（在 Kubernetes 中，是 pod 的服务账户 JWT）发送给 Citadel。Citadel 将执行认证和授权，并以签名的 X.509 证书作为回应。Istio 代理从 Citadel 获取响应，将密钥和证书缓存在内存中，并通过 SDS 通过 Unix 域套接字将其提供给 Envoy。将密钥存储在内存中比存储在磁盘上更安全；在使用 SDS 时，Istio 绝不会将任何密钥写入磁盘作为其操作的一部分。Istio 代理还定期刷新凭证，在当前凭证过期前从 Citadel 检索任何新的 SVID（SPIFFE 可验证身份文件）。 图 8.2.2：身份签发流程 SVID 是一个工作负载可以用来向资源或调用者证明其身份的文件。它必须由一个权威机构签署，并包含一个 SPIFFE ID，它代表了提出该文件的服务的身份，例如，spiffe://clusterlocal/ns/my-namespace/sa/my-sa。 这种解决方案是可扩展的，因为流程中的每个组件只负责一部分工作。例如，Envoy 负责过期证书，Istio 代理负责生成私钥和 CSR，Citadel 负责授权和签署证书。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"security/peer-and-request-authn.html":{"url":"security/peer-and-request-authn.html","title":"对等认证和请求认证","keywords":"","body":"对等认证和请求认证 Istio 提供两种类型的认证：对等认证和请求认证。 对等认证 对等认证用于服务间的认证，以验证建立连接的客户端。 当两个服务试图进行通信时，相互 TLS 要求它们都向对方提供证书，因此双方都知道它们在与谁交谈。如果我们想在服务之间启用严格的相互 TLS，我们可以使用 PeerAuthentication 资源，将 mTLS 模式设置为 STRICT。 使用 PeerAuthentication 资源，我们可以打开整个网状结构的相互 TLS（mTLS），而不需要做任何代码修改。 然而，Istio 也支持一种优雅的模式，我们可以选择在一个工作负载或命名空间的时间内进入相互 TLS。这种模式被称为许可模式。 当你安装 Istio 时，允许模式是默认启用的。启用允许模式后，如果客户端试图通过相互 TLS 连接到我，我将提供相互 TLS。如果客户端不使用相互 TLS，我也可以用纯文本响应。我是允许客户端做 mTLS 或不做的。使用这种模式，你可以在你的网状网络中逐渐推广相互 TLS。 简而言之，PeerAuthentication 谈论的是工作负载或服务的通信方式，它并没有说到最终用户。那么，我们怎样才能认证用户呢？ 请求认证 请求认证（RequestAuthentication 资源）验证了附加在请求上的凭证，它被用于终端用户认证。 请求级认证是通过 JSON Web Tokens（JWT） 验证完成的。Istio 支持任何 OpenID Connect 提供商，如 Auth0、Firebase 或 Google Auth、Keycloak、ORY Hydra。因此，就像我们使用 SPIFFE 身份来验证服务一样，我们可以使用 JWT 令牌来验证用户。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"security/mtls.html":{"url":"security/mtls.html","title":"mTLS","keywords":"","body":"mTLS 服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy）。 然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握手过程中，调用者会进行安全命名检查，以验证服务器证书中的服务账户是否被授权运行目标服务。一旦 mTLS 连接建立，Istio 就会将请求从客户端的 Envoy 代理转发到服务器端的 Envoy 代理。在服务器端的授权后，sidecar 将流量转发到工作负载。 我们可以在服务的目标规则中改变 mTLS 行为。支持的 TLS 模式有：DISABLE（无 TLS 连接）、SIMPLE（向上游端点发起 TLS 连接）、MUTUAL（通过出示客户端证书进行认证来使用 mTLS）和 ISTIO_MUTUAL（与 MUTUAL 类似，但使用 Istio 自动生成的证书进行 mTLS）。 什么是 mTLS？ 在一个典型的 Kubernetes 集群中，加密的流量进入集群，经过一个负载均衡器终止 TLS 连接，从而产生解密的流量。然后，解密的流量被发送到集群内的相关服务。由于集群内的流量通常被认为是安全的，对于许多用例，这是一个可以接受的方法。 但对于某些用例，如处理个人身份信息（PII），可能需要额外的保护。在这些情况下，我们希望确保 所有的 网络流量，甚至同一集群内的流量，都是加密的。这为防止窥探（读取传输中的数据）和欺骗（伪造数据来源）攻击提供了额外保障。这可以帮助减轻系统中其他缺陷的影响。 如果手动实现这个完整的数据传输加密系统的话，需要对集群中的每个应用程序进行大规模改造。你需要告诉所有的应用程序终止自己的 TLS 连接，为所有的应用程序颁发证书，并为所有的应用程序添加一个新的证书颁发机构。 Istio 的 mTLS 在应用程序之外处理这个问题。它安装了一个 sidecar，通过 localhost 连接与你的应用程序进行通信，绕过了暴露的网络流量。它使用复杂的端口转发规则（通过 IPTables）来重定向进出 Pod 的流量，使其通过 sidecar。代理中的 Envoy sidecar 处理所有获取 TLS 证书、刷新密钥、终止等逻辑。 Istio 的这种工作方式虽然可以让你避免修改应用程序，但是当它可以工作时，能够工作得很好。而当它失败时，它可能是灾难性的，而且还难以调试。Istio 的 mTLS 值得一提的三个具体要点。 在严格模式（Strict Mode）下，也就是我们要做的，数据平面 Envoy 会拒绝任何传入的明文通信。 通常情况下，如果你对一个不存在的主机进行 HTTP 连接，你会得到一个失败的连接错误。你肯定 不会 得到一个 HTTP 响应。然而，在 Istio 中，你将 总是 成功地发出 HTTP 连接，因为你的连接是给 Envoy 本身的。如果 Envoy 代理不能建立连接，它将像大多数代理一样，返回一个带有 503 错误信息的 HTTP 响应体。 Envoy 代理对一些协议有特殊处理。最重要的是，如果你做一个纯文本的 HTTP 外发连接，Envoy 代理有复杂的能力来解析外发请求，了解各种头文件的细节，并做智能路由。 允许模式 允许模式（Permissive Mode）是一个特殊的选项，它允许一个服务同时接受纯文本流量和 mTLS 流量。这个功能的目的是为了改善 mTLS 的用户体验。 默认情况下，Istio 使用允许模式配置目标工作负载。Istio 跟踪使用 Istio 代理的工作负载，并自动向其发送 mTLS 流量。如果工作负载没有代理，Istio 将发送纯文本流量。 当使用允许模式时，服务器接受纯文本流量和 mTLS 流量，不会破坏任何东西。允许模式给了我们时间来安装和配置 sidecar，以逐步发送 mTLS 流量。 一旦所有的工作负载都安装了 sidecar，我们就可以切换到严格的 mTLS 模式。要做到这一点，我们可以创建一个 PeerAuthentication 资源。我们可以防止非双向 TLS 流量，并要求所有通信都使用 mTLS。 我们可以创建 PeerAuthentication 资源，首先在每个命名空间中分别执行严格模式。然后，我们可以在根命名空间（在我们的例子中是 istio-system）创建一个策略，在整个服务网格中执行该策略： apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: istio-system spec: mtls: mode: STRICT 此外，我们还可以指定selector字段，将策略仅应用于网格中的特定工作负载。下面的例子对具有指定标签的工作负载启用STRICT模式： apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: my-namespace spec: selector: matchLabels: app: customers mtls: mode: STRICT 参考 An Istio/mutual TLS debugging story -fpcomplete.com Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"security/authz.html":{"url":"security/authz.html","title":"授权","keywords":"","body":"授权 授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）主体是否被允许对一个对象执行动作？用户 A 能否向服务 A 的路径 /hello 发送一个 GET 请求？ 请注意，尽管主体可以被认证，但它可能不被允许执行一个动作。你的公司 ID 卡可能是有效的、真实的，但我不能用它来进入另一家公司的办公室。如果我们继续之前的海关官员的比喻，我们可以说授权类似于你护照上的签证章。 这就引出了下一个问题 —— 有认证而无授权（反之亦然）对我们没有什么好处。对于适当的访问控制，我们需要两者。让我给你举个例子：如果我们只认证主体而不授权他们，他们就可以做任何他们想做的事，对任何对象执行任何操作。相反，如果我们授权了一个请求，但我们没有认证它，我们就可以假装成其他人，再次对任何对象执行任何操作。 Istio 允许我们使用 AuthorizationPolicy 资源在网格、命名空间和工作负载层面定义访问控制。AuthorizationPolicy 支持 DENY、ALLOW、AUDIT 和 CUSTOM 操作。 每个 Envoy 代理实例都运行一个授权引擎，在运行时对请求进行授权。当请求到达代理时，引擎会根据授权策略评估请求的上下文，并返回 ALLOW 或 DENY。AUDIT 动作决定是否记录符合规则的请求。注意，AUDIT 策略并不影响请求被允许或拒绝。 没有必要明确地启用授权功能。为了执行访问控制，我们可以创建一个授权策略来应用于我们的工作负载。 AuthorizationPolicy 资源是我们可以利用 PeerAuthentication 策略和 RequestAuthentication 策略中的主体的地方。 在定义 AuthorizationPolicy 的时候，我们需要考虑三个部分。 选择要应用该策略的工作负载 要采取的行动（拒绝、允许或审计） 采取该行动的规则 让我们看看下面这个例子如何与 AuthorizationPolicy 资源中的字段相对应。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: customers-deny namespace: default spec: selector: matchLabels: app: customers version: v2 action: DENY rules: - from: - source: notNamespaces: [\"default\"] 使用selector和matchLabels，我们可以选择策略所适用的工作负载。在我们的案例中，我们选择的是所有设置了app: customers和version: v2标签的工作负载。action 字段被设置为DENY。 最后，我们在规则栏中定义所有规则。我们例子中的规则是说，当请求来自默认命名空间之外时，拒绝对 customers v2 工作负载的请求（action）。 除了规则中的 from 字段外，我们还可以使用 to 和 when 字段进一步定制规则。让我们看一个使用这些字段的例子。 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: customers-deny namespace: default spec: selector: matchLabels: app: customers version: v2 action: DENY rules: - from: - source: notNamespaces: [\"default\"] - to: - operation: methods: [\"GET\"] - when: - key: request.headers [User-Agent] values: [\"Mozilla/*\"] 我们在规则部分添加了to和when字段。如果我们翻译一下上面的规则，我们可以说，当客户的 GET 请求来自default命名空间之外，并且User Agent头的值与正则表达式Mozilla/* 相匹配时，我们会拒绝 customer v2 的工作负载。 总的来说，to 定义了策略所允许的行动，from 定义了谁可以采取这些行动，when 定义了每个请求必须具备的属性，以便被策略所允许，selector 定义了哪些工作负载将执行该策略。 如果一个工作负载有多个策略，则首先评估拒绝的策略。评估遵循这些规则： 如果有与请求相匹配的 DENY 策略，则拒绝该请求 如果没有适合该工作负载的 ALLOW 策略，则允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，则允许该请求。 拒绝该请求 来源 我们在上述例子中使用的源是 notNamespaces。我们还可以使用以下任何一个字段来指定请求的来源，如表中所示。 来源 示例 释义 principals principals: [\"my-service-account\"] 任何是有 my-service-account 的工作负载 notPrincipals notPrincipals: [\"my-service-account\"] 除了 my-service-account 的任何工作负载 requestPrincipals requestPrincipals: [\"my-issuer/hello\"] 任何具有有效 JWT 和请求主体 my-issuer/hello 的工作负载 notRequestPrincipals notRequestPrincipals: [\"*\"] 任何没有请求主体的工作负载（只有有效的 JWT 令牌）。 namespaces namespaces: [\"default\"] 任何来自 default 命名空间的工作负载 notNamespaces notNamespaces: [\"prod\"] 任何不在 prod 命名空间的工作负载 ipBlocks ipBlocks: [\"1.2.3.4\",\"9.8.7.6/15\"] 任何具有 1.2.3.4 的 IP 地址或来自 CIDR 块的 IP 地址的工作负载 notIpBlock ipBlocks: [\"1.2.3.4/24\"] Any IP address that's outside of the CIDR block 操作 操作被定义在 to 字段下，如果多于一个，则使用 AND 语义。就像来源一样，操作是成对的，有正反两面的匹配。设置在操作字段的值是字符串： hosts 和 notHosts ports 和 notPorts methods 和 notMethods paths 和 notPath 所有这些操作都适用于请求属性。例如，要在一个特定的请求路径上进行匹配，我们可以使用路径。[\"/api/*\",\"/admin\"] 或特定的端口 ports: [\"8080\"]，以此类推。 条件 为了指定条件，我们必须提供一个 key 字段。key 字段是一个 Istio 属性的名称。例如，request.headers、source.ip、destination.port 等等。关于支持的属性的完整列表，请参考 授权政策条件。 条件的第二部分是 values 或 notValues 的字符串列表。下面是一个 when 条件的片段： ... - when: - key: source.ip notValues: [\"10.0.1.1\"] Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"advanced/":{"url":"advanced/","title":"高级功能概述","keywords":"","body":"高级功能概述 在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将运行在虚拟机上的工作负载纳入服务网格。 当决定在多集群场景下运行 Istio 时，有多种组合需要考虑。在高层次上，我们需要决定以下几点： 单个集群或多个集群 单个网络或多个网络 单个控制平面或多个控制平面 单个网格或多个网格 上述模式的任何组合都是可能的，然而，并非所有的模式都有意义。在本章中，我们将重点讨论涉及多个集群的场景。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"advanced/multicluster-deployment.html":{"url":"advanced/multicluster-deployment.html","title":"多集群部署","keywords":"","body":"多集群部署 多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可用性，但我们付出的代价是增加了复杂性。如果场景要求高可用性（HA），我们将不得不在多个区域和地区部署集群。 我们需要做出的下一个决定是，决定我们是否要在一个网络内运行集群，或者是否要使用多个网络。 下图显示了一个多集群方案（集群 A、B 和 C），跨两个网络部署。 图 9.2.1：多集群服务网格 网络部署模式 当涉及到多个网络时，集群内部运行的工作负载必须使用 Istio 网关才能到达其他集群的工作负载。使用多个网络可以实现更好的容错和网络地址的扩展。 图 9.2.2：多网络服务网格 控制平面部署模型 Istio 服务网格使用控制平面来配置网格内工作负载之间的所有通信。工作负载所连接的控制平面取决于其配置。 在最简单的情况下，我们有一个服务网格，在一个集群中只有一个控制平面。这就是我们在本课程中一直使用的配置。 共享控制平面模型涉及多个集群，控制平面只在一个集群中运行。该集群被称为主集群，而部署中的其他集群被称为远程集群。这些集群没有自己的控制平面，相反，它们从主集群共享控制平面。 图 9.2.3：共享的控制平面 另一种部署模式是，我们把所有的集群都视为由外部控制平面控制的远程集群。这使我们在控制平面和数据平面之间有了完全的分离。一个典型的外部控制平面的例子是当一个云供应商在管理它。 为了实现高可用性，我们应该在多个集群、区域或地区部署多个控制平面实例，如下图所示。 图 9.2.4：多个控制平面 这种模式提供了更好的可用性和配置隔离。如果其中一个控制平面变得不可用，那么停电就只限于这一个控制平面。为了改善这一点，你可以实施故障转移，并配置工作负载实例，在发生故障时连接到另一个控制平面。 为了达到最高的可用性，我们可以在每个集群内部署一个控制平面。 网格部署模型 到目前为止，我们所看的所有图表和场景都是使用单一的网格。在单网格模型中，所有的服务都在一个网格中，不管它们跨越多少集群和网络。 将多个网格联合起来的部署模型被称为多网格部署。在这种模式下，服务可以跨网格边界进行通信。该模型为我们提供了一个更清晰的组织边界，更强的隔离性，并允许我们重复使用服务名称和命名空间。 当联合两个网格时，每个网格可以暴露一组服务和身份，所有参与的网格都可以识别这些身份。为了实现跨网格的服务通信，我们必须在两个网格之间实现信任。信任可以通过向网格导入信任包和为这些身份配置本地策略来建立。 租户模式 租户是一组共享工作负载的共同访问权和权限的用户。租户之间的隔离是通过网络配置和策略完成的。Istio 支持命名空间和集群租户。请注意，我们在这里谈论的租户是软多租户，而不是硬租户。当多个租户共享同一个 Istio 控制平面时，没有保证对诸如噪音邻居问题的保护。 在一个网格中，Istio 使用命名空间作为租户的单位。如果使用 Kubernetes，我们可以为每个命名空间的工作负载部署授予权限。默认情况下，来自不同命名空间的服务可以通过完全限定名相互通信。 在安全模块中，我们已经学会了如何使用授权策略来提高隔离度，并限制只对适当的调用者进行访问。 在多集群部署模型中，每个集群中共享相同名称的命名空间被认为是同一个命名空间。集群 A 中 default 命名空间的 Customers 服务与集群 B 中 default 命名空间中的 Customers 服务指的是同一个服务。当流量被发送到 Customers 服务时，负载均衡在两个服务的合并端点上进行，如下图所示。 图 9.2.5：多集群共享命名空间 为了在 Istio 中配置集群租约，我们需要将每个集群配置为一个独立的服务网格。网格可以由不同的团队控制和操作，我们可以将网格连接到一起，形成一个多网格部署。如果我们使用与之前相同的例子，在集群 A 的 default 命名空间中运行的服务 Customers 与集群 B 的 default 命名空间中的服务 Customers 所指的不是同一个服务。 租户的另一个重要功能是隔离不同租户的配置。目前，Istio 并没有解决这个问题，不过，它通过在命名空间级别上的范围配置来尝试解决这个问题。 最佳多集群部署 最佳的多集群部署拓扑结构是每个集群都有自己的控制平面。对于正常的服务网格部署规模，建议你使用多网格部署，并有一个单独的系统在外部协调网格。一般建议总是在集群间使用入口网关，即使它们在同一个网络中。直接的 pod 到 pod 的连接需要在多个集群之间填充终端数据，这可能会使事情变得缓慢和复杂。一个更简单的解决方案是让流量通过跨集群的入口来流动。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"advanced/vm.html":{"url":"advanced/vm.html","title":"虚拟机负载","keywords":"","body":"虚拟机负载 如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服务网格，使其成为网格的一部分。 带有虚拟机的 Istio 服务网格有两种架构：单网络架构和多网络架构。 单网络架构 在这种情况下，有一个单一的网络。Kubernetes 集群和在虚拟机上运行的工作负载都在同一个网络中，它们可以直接相互通信。 图 9.3.1：单网络架构 单网络架构 控制面的流量（配置更新、证书签署）是通过 Gateway 发送的。 虚拟机被配置了网关地址，所以它们在启动时可以连接到控制平面。 多网络架构 多网络架构横跨多个网络。Kubernetes 集群在一个网络内，而虚拟机则在另一个网络内。这使得 Kubernetes 集群中的 Pod 和虚拟机上的工作负载无法直接相互通信。 图 9.3.2：多网络架构 多网络架构 所有的流量，控制面和 pod 到工作服的流量都流经网关，网关作为两个网络之间的桥梁。 Istio 中如何表示虚拟机工作负载？ 在 Istio 服务网格中，有两种方式来表示虚拟机工作负载。 工作负载组（WorkloadGroup 资源）类似于 Kubernetes 中的部署（Deployment），它代表了共享共同属性的虚拟机工作负载的逻辑组。 描述虚拟机工作负载的第二种方法是使用工作负载条目（WorkloadEntry 资源）。工作负载条目类似于 Pod，它代表了一个虚拟机工作负载的单一实例。 请注意，创建上述资源将不会提供或运行任何虚拟机工作负载实例。这些资源只是用来参考或指向虚拟机工作负载的。Istio 使用它们来了解如何适当地配置网格，将哪些服务添加到内部服务注册表中，等等。 为了将虚拟机添加到网格中，我们需要创建一个工作负载组，作为模板。然后，当我们配置并将虚拟机添加到网格中时，控制平面会自动创建一个相应的 WorkloadEntry。 我们已经提到，WorkloadEntry 的作用类似于 Pod。在添加虚拟机时，会创建 WorkloadEntry 资源，而当虚拟机的工作负载从网格中移除时，该资源会被自动删除。 除了 WorkloadEntry 资源外，我们还需要创建一个 Kubernetes 服务。创建一个 Kubernetes 服务给了我们一个稳定的主机名和 IP 地址，以便使用选择器字段访问虚拟机工作负载和 pod。这也使我们能够通过 DestinationRule 和 VirtualService 资源使用 Istio 的路由功能。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"troubleshooting/":{"url":"troubleshooting/","title":"问题排查概述","keywords":"","body":"问题排查概述 本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"troubleshooting/envoy-basic.html":{"url":"troubleshooting/envoy-basic.html","title":"Envoy 基础问题","keywords":"","body":"Envoy 基础问题 为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的。Envoy 配置是一个 JSON 文件，分为多个部分。我们需要了解 Envoy 的基本概念是监听器、路由、集群和端点。 这些概念映射到 Istio 和 Kubernetes 资源，如下图所示。 图 10.2.1：Envoy 概念与 Istio 和 Kubernetes 的映射 监听器是命名的网络位置，通常是一个 IP 和端口。Envoy 对这些位置进行监听，这是它接收连接和请求的地方。 每个 sidecar 都有多个监听器生成。每个 sidecar 都有一个监听器，它被绑定到 0.0.0.0:15006。这是 IP Tables 将所有入站流量发送到 Pod 的地址。第二个监听器被绑定到 0.0.0.0:15001，这是所有从 Pod 中出站的流量地址。 当一个请求被重定向（使用 IP Tables 配置）到 15001 端口时，监听器会把它交给与请求的原始目的地最匹配的虚拟监听器。如果它找不到目的地，它就根据配置的 OutboundTrafficPolicy 来发送流量。默认情况下，请求被发送到 PassthroughCluster，该集群连接到应用程序选择的目的地，Envoy 没有进行任何负载均衡。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"troubleshooting/envoy-sample.html":{"url":"troubleshooting/envoy-sample.html","title":"Envoy 示例","keywords":"","body":"Envoy 示例 让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送到 customer 服务（customers.default.svc.cluster.local）的位置。使用 istioctl proxy-config 命令，我们可以列出 web 前端 pod 的所有监听器。 $ istioctl proxy-config listeners web-frontend-64455cd4c6-p6ft2 ADDRESS PORT MATCH DESTINATION 10.124.0.10 53 ALL Cluster: outbound|53||kube-dns.kube-system.svc.cluster.local 0.0.0.0 80 ALL PassthroughCluster 10.124.0.1 443 ALL Cluster: outbound|443||kubernetes.default.svc.cluster.local 10.124.3.113 443 ALL Cluster: outbound|443||istiod.istio-system.svc.cluster.local 10.124.7.154 443 ALL Cluster: outbound|443||metrics-server.kube-system.svc.cluster.local 10.124.7.237 443 ALL Cluster: outbound|443||istio-egressgateway.istio-system.svc.cluster.local 10.124.8.250 443 ALL Cluster: outbound|443||istio-ingressgateway.istio-system.svc.cluster.local 10.124.3.113 853 ALL Cluster: outbound|853||istiod.istio-system.svc.cluster.local 0.0.0.0 8383 ALL PassthroughCluster 0.0.0.0 15001 ALL PassthroughCluster 0.0.0.0 15006 ALL Inline Route: /* 0.0.0.0 15010 ALL PassthroughCluster 10.124.3.113 15012 ALL Cluster: outbound|15012||istiod.istio-system.svc.cluster.local 0.0.0.0 15014 ALL PassthroughCluster 0.0.0.0 15021 ALL Non-HTTP/Non-TCP 10.124.8.250 15021 ALL Cluster: outbound|15021||istio-ingressgateway.istio-system.svc.cluster.local 0.0.0.0 15090 ALL Non-HTTP/Non-TCP 10.124.7.237 15443 ALL Cluster: outbound|15443||istio-egressgateway.istio-system.svc.cluster.local 10.124.8.250 15443 ALL Cluster: outbound|15443||istio-ingressgateway.istio-system.svc.cluster.local 10.124.8.250 31400 ALL Cluster: outbound|31400||istio-ingressgateway.istio-system.svc.cluster.local 从 Web 前端到客户的请求是一个向外的 HTTP 请求，端口为 80。这意味着它被移交给了0.0.0.0:80的虚拟监听器。我们可以使用 Istio CLI 按地址和端口来过滤监听器。你可以添加-o json来获得监听器的 JSON 表示： $ istioctl proxy-config listeners web-frontend-58d497b6f8-lwqkg --address 0.0.0.0 --port 80 -o json ... \"rds\": { \"configSource\": {\"ads\": {}, \"resourceApiVersion\": \"V3\" }, \"routeConfigName\": \"80\" }, ... Listener 使用 RDS（路由发现服务）来寻找路由配置（在我们的例子中是 80）。路由附属于监听器，包含将虚拟主机映射到集群的规则。这允许我们创建流量路由规则，因为 Envoy 可以查看头文件或路径（请求元数据）并对流量进行路由。 一个路由（route）选择一个集群（cluster）。一个集群是一组接受流量的类似的上游主机 —— 它是一个端点的集合。例如，Web 前端服务的所有实例的集合就是一个集群。我们可以在一个集群内配置弹性功能，如断路器、离群检测和 TLS 配置。 使用 routes 命令，我们可以通过名称过滤所有的路由来获得路由的详细信息。 $ istioctl proxy-config routes web-frontend-58d497b6f8-lwqkg --name 80 -o json [ { \"name\": \"80\", \"virtualHosts\": [ { \"name\": \"customers.default.svc.cluster.local:80\", \"domains\": [ \"customers.default.svc.cluster.local\", \"customers.default.svc.cluster.local:80\", \"customers\", \"customers:80\", \"customers.default.svc.cluster\", \"customers.default.svc.cluster:80\", \"customers.default.svc\", \"customers.default.svc:80\", \"customers.default\", \"customers.default:80\", \"10.124.4.23\", \"10.124.4.23:80\" ], ], \"routes\": [ { \"match\": {\"prefix\": \"/\"}, \"route\": { \"cluster\": \"outbound|80|v1|customers.default.svc.cluster.local\", \"timeout\": \"0s\", \"retryPolicy\": { \"retryOn\": \"connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes\", \"numRetries\": 2, \"retryHostPredicate\": [ {\"name\": \"envoy.retry_host_predicates.previous_hosts\"} ], \"hostSelectionRetryMaxAttempts\": \"5\", \"retriableStatusCodes\": [503] }, \"maxGrpcTimeout\": \"0s\" }, ... 路由80配置为每个服务都有一个虚拟主机。然而，由于我们的请求被发送到customers.default.svc.cluster.local，Envoy 会选择与其中一个域匹配的虚拟主机（customers.default.svc.cluster.local:80）。 一旦域被匹配，Envoy 就会查看路由，并选择第一个匹配请求的路由。由于我们没有定义任何特殊的路由规则，它匹配第一个（也是唯一的）定义的路由，并指示 Envoy 将请求发送到名为 outbound|80|v1|customers.default.svc.cluster.local 的集群。 注意集群名称中的 v1 是因为我们部署了一个 DestinationRule 来创建 v1 子集。如果一个服务没有子集，这部分就留空：outbound|80||customers.default.svc.cluster.local。 现在我们有了集群的名称，我们可以查询更多的细节。为了得到一个清楚显示 FQDN、端口、子集和其他信息的输出，你可以省略 -o json 标志。 $ istioctl proxy-config cluster web-frontend-58d497b6f8-lwqkg --fqdn customers.default.svc.cluster.local SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE customers.default.svc.cluster.local 80 - outbound EDS customers.default customers.default.svc.cluster.local 80 v1 outbound EDS customers.default 最后，使用集群的名称，我们可以查询请求最终将到达的实际端点： $ istioctl proxy-config endpoints web-frontend-58d497b6f8-lwqkg --cluster \"outbound|80|v1|customers.default.svc.cluster.local\" ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.120.0.4:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 端点地址等于客户应用程序正在运行的 pod IP。如果我们扩展 customer 的部署，额外的端点会出现在输出中，像这样： $ istioctl proxy-config endpoints web-frontend-58d497b6f8-lwqkg --cluster \"outbound|80|v1|customers.default.svc.cluster.local\" ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.120.0.4:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 10.120.3.2:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 我们也可以用下图来形象地说明上述流程。 图 10.3.1：Envoy 详情 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"troubleshooting/checklist.html":{"url":"troubleshooting/checklist.html","title":"调试清单","keywords":"","body":"调试清单 每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在第一部分，我们要检查配置是否有效。如果配置是有效的，下一步就是看看运行时是如何处理配置的，为此，你需要对 Envoy 配置有基本的了解。 配置 1. 配置是否有效？ Istio CLI 有一个叫 validate 的命令，我们可以用它来验证 YAML 配置。YAML 最常见的问题是缩进和数组符号相关的问题。 要验证一个配置，请将 YAML 文件传递给 validate 命令，像这样： $ istioctl validate -f myresource.yaml validation succeed 如果资源是无效的，CLI 会给我们一个详细的错误。例如，如果我们拼错了一个字段名： unknown field \"worloadSelector\" in v1alpha3.ServiceEntry 我们可以使用另一个命令istioctl analyze。使用这个命令，我们可以检测 Istio 配置的潜在问题。我们可以针对本地的一组配置文件或实时集群运行它。同时，寻找来自 istiod 的任何警告或错误。 下面是该命令的一个输出样本，它捕捉到了目的地主机名称中的一个错字： $ istioctl analyze Error [IST0101] (VirtualService customers.default) Referenced host not found: \"cusomers.default.svc.cluster.local\" Error [IST0101] (VirtualService customers.default) Referenced host+subset in destinationrule not found: \"cusomers.default.svc.cluster.local+v1\" Error: Analyzers found issues when analyzing namespace: default. See https://istio.io/docs/reference/config/analysis for more information about causes and resolutions. 2. 命名是否正确？资源是否在正确的命名空间？ 几乎所有的 Istio 资源都是命名空间范围的。确保它们与你正在处理的服务处于同一命名空间。将 Istio 资源放在同一命名空间中尤其重要，因为选择器也是有命名空间的。 一个常见的错误配置是在应用程序的命名空间中发布 VirtualService（例如 default），然后使用 istio：ingressgateway 选择器来绑定到 istio-system 命名空间中的 ingress 网关部署。这只有在你的 VirtualService 也在 istio-system 命名空间中时才有效。 同样地，不要在 istio-system 命名空间中部署引用应用程序命名空间中的 VirtualService 的 Sidecar 资源。相反，为每个需要入口的应用程序部署一组 Envoy 网关。 3. 资源选择器是否正确？ 验证部署中的 pod 是否有正确的标签设置。正如上一步提到的，资源选择器与资源发布的命名空间绑定。 在这一点上，我们应该有理由相信，配置是正确的。接下来的步骤是进一步研究运行时系统是如何处理配置的。 运行时 Istio CLI 的一个实验性功能可以提供信息，帮助我们了解影响 Pod 或服务的配置。下面是一个针对 Pod 运行 describe 命令的例子，这个 Pod 的主机名称中有一个错字： $ istioctl x describe pod customers-v1-64455cd4c6-xvjzm.default Pod: customers-v1-64455cd4c6-xvjzm Pod Ports: 3000 (svc), 15090 (istio-proxy) -------------------- Service: customers Port: http 80/HTTP targets pod port 3000 DestinationRule: customers for \"customers.default.svc.cluster.local\" Matching subsets: v1 No Traffic Policy VirtualService: customers WARNING: No destinations match pod subsets (checked 1 HTTP routes) Route to cusomers.default.svc.cluster.local 1. Envoy 是否接受（ACK）该配置？ 你可以使用 istioctl proxy-status 命令来检查状态，看看 Envoy 是否接受配置。我们希望所有东西的状态都设置为 SYNCHED。任何其他值都可能表明有错误，你应该检查 Pilot 的日志。 $ istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION customers-v1... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 customers-v1... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 istio-egress... SYNCED SYNCED SYNCED NOT SENT istiod-67b4c76c6-8lwxf 1.9.0 istio-ingress... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 web-frontend-... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 列表显示所有连接到 Pilot 实例的代理。如果列表中缺少一个代理，这意味着它没有连接到 Pilot，也没有收到任何配置。如果任何一个代理被标记为STALE，可能有网络问题，或者我们需要扩展 Pilot。 如果 Envoy 接受了配置，但我们仍然看到问题，我们需要确保配置在 Envoy 中的表现符合预期。 2. 配置在 Envoy 中的表现和预期的一样吗？ 我们可以使用 proxy-config 命令来检索特定 Envoy 实例的信息。请参考下面的表格，我们可以检索不同的代理配置。 命令 描述 istioctl proxy-config cluster [POD] -n [NAMESPACE] 检索 cluster 配置 istioctl proxy-config bootstrap [POD] -n [NAMESPACE] 检索 bootstrap 配置 istioctl proxy-config listener [POD] -n [NAMESPACE] 检索 listener 配置 istioctl proxy-config route [POD] -n [NAMESPACE] 检索 route 配置 istioctl proxy-config endpoints [POD] -n [NAMESPACE] 检索 endpoint 配置 该命令从 Envoy 的管理端点（主要是 /config_dump）收集数据，它包含了很多有用的信息。 另外，请参考显示 Envoy 和 Istio 资源之间映射的图。例如，许多 VirtualService 规则将表现为 Envoy 路由，而 DestinationRules 和 ServiceEntries 则表现为 Cluster。 DestinationRules 不会出现在配置中，除非其主机的 ServiceEntry 首先存在。 让我们以客户的 VirtualService 为例。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: customers spec: hosts: - 'customers.default.svc.cluster.local' http: - route: - destination: host: customers.default.svc.cluster.local port: number: 80 subset: v1 weight: 80 - destination: host: customers.default.svc.cluster.local port: number: 80 subset: v2 weight: 20 timeout: 5s 如果你运行istioctl proxy-config routes [POD] -o json命令，你会看到加权目的地和超时是如何在配置中体现的： .. { \"name\": \"80\", \"virtualHosts\": [ { \"name\": \"customers.default.svc.cluster.local:80\", \"domains\": [ \"customers.default.svc.cluster.local\", ... ], \"routes\": [ { \"match\": {\"prefix\": \"/\"}, \"route\": { \"weightedClusters\": { \"clusters\": [ { \"name\": \"outbound|80|v1|customers.default.svc.cluster.local\", \"weight\": 80 }, { \"name\": \"outbound|80|v2|customers.default.svc.cluster.local\", \"weight\": 20 } ] }, \"timeout\": \"5s\", ... 当你评估 VirtualServices 时，你要寻找主机名是否像你写的那样出现在 Envoy 配置中（例如customers.default.svc.cluster.local），以及路由是否存在（见输出中的 80-20 流量分割）。你也可以使用之前的例子，通过监听器、路由和集群（和端点）来追踪调用。 Envoy 过滤器会表现在你告诉 Istio 把它们放在哪里（EnvoyFilter 资源中的 applyTo 字段）。通常情况下，一个坏的过滤器会表现为 Envoy 拒绝配置（即不显示 SYNCED 状态）。在这种情况下，你需要检查 Istiod 日志中的错误。 3. Istiod（Pilot）中是否有错误？ 从 Pilot 查看错误的最快方法是跟踪日志（使用 --follow 标志），然后应用配置。下面是一个来自 Pilot 的错误的例子，这是由于过滤器的内联代码中的一个错字而导致的。 2020-11-20T21:49:16.017487Z warn ads ADS:LDS: ACK ERROR sidecar~10.120.1.8~web-frontend-58d497b6f8-lwqkg.default~default.svc.cluster.local-4 Internal:Error adding/updating listener (s) virtualInbound: script load error: [string\"fction envoy_on_response (response_handle)...\"]:1: '=' expected near 'envoy_on_response' 如果配置根本没有出现在 Envoy 中（Envoy 没有 ACK 它），或者它是一个 EnvoyFilter 配置，那么这个配置很可能是无效的。Istio 无法从语法上验证 EnvoyFilter 内部的配置。另一个问题可能是，过滤器在 Envoy 的配置中位于错误的位置。 无论哪种情况，Envoy 都会拒绝该配置，因为它是无效的，Pilot 会记录这个错误。一般来说，你可以搜索你的资源的名称来找到错误。 在这里，你必须使用判断力来确定它是你写的配置中的错误，还是 Pilot 的错误导致它产生了一个无效的配置。 检查 Envoy 日志 要检查 Envoy 代理的日志，我们可以使用 kubectl logs 命令： kubectl logs PODNAME -c istio-proxy -n NAMESPACE 要了解访问日志的格式和响应标志，我们可以参考 Envoy 访问日志 的内容。 最常见的响应标志。 NR：没有配置路由，检查 DestinationRule 或 VirtualService。 UO：上游溢出并断路。检查 DestinationRule 中的断路器配置。 UF：上游连接失败，如果使用 Istio 认证，检查 mTLS 配置。 UH：没有健康的上游主机。 配置 istiod 日志 我们可以使用 ControlZ 仪表板，通过 Logging Scopes 菜单配置堆栈跟踪级别和日志级别。 要打开仪表板，请运行： istioctl dashboard controlz $(kubectl -n istio-system get pods -l app=istiod -o jsonpath='{.items [0].metadata.name}').istio-system 一旦仪表板打开，点击 Logging Scopes 选项，调整日志级别和堆栈跟踪级别。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"troubleshooting/istio-dev-env.html":{"url":"troubleshooting/istio-dev-env.html","title":"Istio 开发环境配置","keywords":"","body":"Istio 开发环境配置 本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。 依赖环境 Istio 开发环境依赖以下软件： Docker：测试和运行时 Go 1.11：程序开发 fpm 包构建工具：用来打包 Kubernetes 1.7.3+ 设置环境变量 在编译过程中需要依赖以下环境变量，请根据你自己的 export ISTIO=$GOPATH/src/istio.io # DockerHub 的用户名 USER=jimmysong export HUB=\"docker.io/$USER\" # Docker 镜像的 tag，这里为了方便指定成了固定值，也可以使用 install/updateVersion.sh 来生成 tag export TAG=$USER # GitHub 的用户名 export GITHUB_USER=rootsongjc # 指定 Kubernetes 集群的配置文件地址 export KUBECONFIG=${HOME}/.kube/config 全量编译 编译过程中需要下载很多依赖包，请确认你的机器可以科学上网。 执行下面的命令可以编译 Istio 所有组件的二进制文件。 make 以在 Mac 下编译为例，编译完成后所有的二进制文件将位于 $GOPATH/out/darwin_amd64/release。 执行下面的命令构建镜像。 make docker 执行下面的命令将镜像推送到 DockerHub。 make push 也可以编译单独组件的镜像，详见开发指南。 构建 YAML 文件 执行下面的命令可以生成 YAML 文件。 make generate_yaml 生成的 YAML 文件位于 repo 根目录的 install/kubernetes 目录下。 参考 Istio Dev Guide - github.com Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"ecosystem/":{"url":"ecosystem/","title":"Istio 生态概述","keywords":"","body":"Istio 生态概述 Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目，这些项目有的是对 Istio 本身的扩展，有的是可以与 Istio 集成，还有的是 Istio 周边的开源工具。 本章将分门别类为读者介绍 Istio 生态中的开源项目，以帮助读者对 Istio 的开源生态有个更直观的了解，另一方面也可以作为读者个人的选型参考。 Istio 周边开源项目 下表中列举 Istio 生态中的开源项目，按照开源时间排序。 项目名称 开源时间 类别 描述 主导公司 Star 数量 与 Istio 的关系 Envoy 2016年 9 月 网络代理 云原生高性能边缘/中间服务代理 Lyft 18300 默认的数据平面 Istio 2017 年 5 月 服务网格 连接、保护、控制和观察服务。 Google 28400 控制平面 Emissary Gateway 2018 年 2 月 网关 用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建 Ambassador 3500 可连接 Istio APISIX 2019 年 6 月 网关 云原生 API 网关 API7 7400 可作为 Istio 的数据平面运行也可以单独作为网关 MOSN 2019 年 12 月 代理 云原生边缘网关及代理 蚂蚁 3400 可作为 Istio 数据平面 Slime 2021 年 1月 扩展 基于 Istio 的智能服务网格管理器 网易 204 为 Istio 增加一个管理平面 GetMesh 2021 年 2 月 工具 Istio 集成和命令行管理工具 Tetrate 91 实用工具，可用于 Istio 多版本管理 Aeraki 2021 年 3 月 扩展 管理 Istio 的任何七层负载 腾讯 280 扩展多协议支持 Layotto 2021 年 6 月 运行时 云原生应用运行时 蚂蚁 325 可以作为 Istio 的数据平面 Hango Gateway 2021 年 8 月 网关 基于 Envoy 和 Istio 构建的 API 网关 网易 187 可与 Istio 集成 注意 开源时间以 GitHub 仓库创建时间为准 Star 数量统计截止时间为 2021年11月11 日 参考 Building Ambassador, an Open Source API Gateway on Kubernetes and Envoy - blog.ambassador.io 蚂蚁开源多运行时项目 Layotto 简介 - cloudnative.to Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to 使用 Aeraki 在 Isito 中支持 Dubbo、 Thrift、 Redis，以及任何七层协议 - cloudnative.to Apache APISIX 的全流量 API 网关统筹集群流量 - cloudnative.to GetMesh 官网 - istio.tetratelabs.io MOSN 官网 - mosn.io Istio 官网 - istio.io Envoy 官网 - envoyproxy.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"ecosystem/slime.html":{"url":"ecosystem/slime.html","title":"Slime","keywords":"","body":"Slime Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。 Slime 试图解决的问题 Slime 项目的诞生主要为了解决以下问题： 网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题 如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流 Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是： 构建可拔插控制器 数据平面监控 CRD 转换 通过以上方式 Slime 可以实现配置懒加载和插件管理器。 Slime 架构 Slime 内部分为三大模块，其架构图如下所示。 图 11.2.1：Slime 内部架构图 Slime 内部三大组件为： slime-boot：在 Kubernetes 上部署 Slime 模块的 operator。 slime-controller：Slime 的核心组件，监听 Slime CRD 并将其转换为Istio CRD。 slime-metric：用于获取服务 metrics 信息的组件，slime-controller 会根据其获取的信息动态调整服务治理规则。 目前 Slime 内置了三个控制器子模块： 配置懒加载（按需加载）：用户无须手动配置 SidecarScope，Istio 可以按需加载服务配置和服务发现信息； HTTP 插件管理：使用新的 CRD——pluginmanager/envoyplugin 包装了可读性，摒弃了可维护性较差的 envoyfilter，使得插件扩展更为便捷； 自适应限流：结合监控信息自动调整限流策略； 什么是 SidecarScope？ SidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 Sidecar 资源中的 egress 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。 使用 Slime 作为 Istio 的控制平面 为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。 图 11.2.2：Slime 工作流程图 具体步骤如下： Slime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化； 开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中； Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中； Istio 监听 Istio CRD 的创建； Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中； 以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 Slime GitHub。 配置懒加载 为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。 Slime 实现 Sidecar Proxy 配置懒加载的方法是： 让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置； 当某个服务的调用链被 VirtualService 中的路由信息重新定义时， Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 ServiceFence 的 CRD 来维护服务调用关系以解决服务信息缺失问题。 使用 Global Proxy 初始化服务调用拓扑 Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。 使用 ServiceFence 维护服务调用拓扑 在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。 如何开启配置懒加载 配置懒加载功能对于终端用户是透明的，只需要 Kubernetes Service 上打上 istio.dependency.servicefence/status:\"true\" 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。 HTTP 插件管理 Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。 Slime 共有两个 CRD 用于 HTTP 插件管理，分别是： PluginManager：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序； EnvoyPlugin：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 patch.typed_config 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余， 关于 Slime 中插件管理的详细使用方式请见 Slime GitHub。 自适应限流 Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。 Slime 自适应限流的流程图如下所示。 图 11.2.3：Slime 的自适应限流流程图 Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。 Slime 创建的 CRD SmartLimiter 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的SmartLimiter 定义如下： apiVersion: microservice.netease.com/v1alpha1 kind: SmartLimiter metadata: name: a namespace: default spec: descriptors: - action: fill_interval: seconds: 1 quota: \"30/{pod}\" # 30为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10 condition: \"{cpu}>0.8\" # 根据监控项{cpu}的值自动填充该模板 更多 Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 Slime：让 Istio 服务网格变得更加高效与智能 及 Slime 的 GitHub。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。 参考 Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to Slime GitHub 文档 - github.com Sidecar - istio.io Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/":{"url":"practice/","title":"实际案例概述","keywords":"","body":"实际案例概述 在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同的Istio功能。 Online Boutique 是一个云原生微服务演示应用程序。Online Boutique 由一个 10 层的微服务应用组成。该应用是一个基于 Web 的电子商务应用，用户可以浏览商品，将其添加到购物车，并购买商品。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/create-cluster.html":{"url":"practice/create-cluster.html","title":"创建集群","keywords":"","body":"创建集群 我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账户，并按照以下步骤创建一个 Kubernetes 集群。 从导航中，选择 Kubernetes Engine。 点击创建集群。 将集群命名为 boutique-demo。 选择区域选项，选择最接近你的位置的区域。 点击 default-pool，在节点数中输入 5。 点击 Nodes（节点）。 点击机器配置机器类型下拉，选择e2-medium (2 vCPU, 4 GB memory)。 点击 \"Create\"来创建集群。 集群的创建将需要几分钟的时间。一旦安装完成，集群将显示在列表中，如下图所示。 图 12.2.1：部署在 GCP 中的 Kubernetes 集群 你也可以将 Online Boutique 应用程序部署到托管在其他云平台上的 Kubernetes 集群，如 Azure 或 AWS。 访问集群 我们有两种方式来访问集群。我们可以从浏览器中使用 Cloud Shell。要做到这一点，点击集群旁边的 Connect，然后点击 Run in Cloud Shell 按钮。点击该按钮可以打开 Cloud Shell，并配置 Kubernetes CLI 来访问该集群。 第二个选择是在你的电脑上安装 gcloud CLI，然后从你的电脑上运行相同的命令。 安装 Istio 我们将使用 GetMesh CLI 在集群中安装 Istio 1.10.3。 1. 下载 GetMesh CLI curl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 2. 安装 Istio getmesh istioctl install --set profile=demo 安装完成后，给默认命名空间设置上 istio-injection=enabled 标签： kubectl label namespace default istio-injection=enabled Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/online-boutique.html":{"url":"practice/online-boutique.html","title":"部署 Online Boutique 应用","keywords":"","body":"部署 Online Boutique 应用 在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。 1. 克隆仓库 git clone https://github.com/GoogleCloudPlatform/microservices-demo.git 2. 前往 microservices-demo 目录 cd microservices-demo 3. 创建 Kubernetes 资源 kubectl apply -f release/kubernetes-manifests.yaml 4. 检查所有 Pod 都在运行 $ kubectl get pods NAME READY STATUS RESTARTS AGE adservice-5c9c7c997f-n627f 2/2 Running 0 2m15s cartservice-6d99678dd6-767fb 2/2 Running 2 2m16s checkoutservice-779cb9bfdf-l2rs9 2/2 Running 0 2m18s currencyservice-5db6c7d559-9drtc 2/2 Running 0 2m16s emailservice-5c47dc87bf-dk7qv 2/2 Running 0 2m18s frontend-5fcb8cdcdc-8c9dk 2/2 Running 0 2m17s loadgenerator-79bff5bd57-q9qkd 2/2 Running 4 2m16s paymentservice-6564cb7fb9-f6dwr 2/2 Running 0 2m17s productcatalogservice-5db9444549-hkzv7 2/2 Running 0 2m17s recommendationservice-ff6878cf5-jsghw 2/2 Running 0 2m18s redis-cart-57bd646894-zb7ch 2/2 Running 0 2m15s shippingservice-f47755f97-dk7k9 2/2 Running 0 2m15s 5. 创建 Istio 资源 kubectl apply -f ./istio-manifests 部署了一切后，我们就可以得到入口网关的 IP 地址并打开前端服务： INGRESS_HOST=\"$(kubectl -n istio-system get service istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\" echo \"$INGRESS_HOST\" 在浏览器中打开 INGRESS_HOST，你会看到前端服务，如下图所示。 图 12.3.1：前端服务 我们需要做的最后一件事是删除 frontend-external 服务。frontend-external 服务是一个 LoadBalancer 服务，它暴露了前端。由于我们正在使用 Istio 的入口网关，我们不再需要这个 LoadBalancer 服务了。 要删除服务，运行： kubectl delete svc frontend-external Online Boutique 应用清单还包括一个负载发生器，它正在生成对所有服务的请求——这是为了让我们能够模拟网站的流量。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/observability.html":{"url":"practice/observability.html","title":"部署可观察性工具","keywords":"","body":"部署可观察性工具 接下来，我们将部署可观察性、分布式追踪、数据可视化工具： kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml 如果你在安装 Kiali 的时候发现以下错误 No matches for kind \"MonitoringDashboard\" in version \"monitoring.kiali.io/v1alpha1\" 请重新运行以上命令。 要从 Google Cloud Shell 打开仪表盘，我们可以运行 getmesh istioctl dash kiali 命令，例如，然后点击 Web Preview 按钮，选择仪表盘运行的端口（Kiali 为 20001）。如果你使用你的终端，运行 Istio CLI 命令就可以了。 下面是 Boutique 图表在 Kiali 中的样子： 图 12.4.1：Boutique 应用在 Kiali 中的样子 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/traffic-routing.html":{"url":"practice/traffic-routing.html","title":"路由流量","keywords":"","body":"路由流量 我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不同的标头。让我们看看如何部署所需的资源并将一定比例的流量路由到不同的前端服务版本。 在我们创建任何资源之前，让我们删除现有的前端部署（kubectl delete deploy frontend），并创建一个版本标签设置为 original 。 apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: selector: matchLabels: app: frontend version: original template: metadata: labels: app: frontend version: original annotations: sidecar.istio.io/rewriteAppHTTPProbers: \"true\" spec: containers: - name: server image: gcr.io/google-samples/microservices-demo/frontend:v0.2.1 ports: - containerPort: 8080 readinessProbe: initialDelaySeconds: 10 httpGet: path: \"/_healthz\" port: 8080 httpHeaders: - name: \"Cookie\" value: \"shop_session-id=x-readiness-probe\" livenessProbe: initialDelaySeconds: 10 httpGet: path: \"/_healthz\" port: 8080 httpHeaders: - name: \"Cookie\" value: \"shop_session-id=x-liveness-probe\" env: - name: PORT value: \"8080\" - name: PRODUCT_CATALOG_SERVICE_ADDR value: \"productcatalogservice:3550\" - name: CURRENCY_SERVICE_ADDR value: \"currencyservice:7000\" - name: CART_SERVICE_ADDR value: \"cartservice:7070\" - name: RECOMMENDATION_SERVICE_ADDR value: \"recommendationservice:8080\" - name: SHIPPING_SERVICE_ADDR value: \"shippingservice:50051\" - name: CHECKOUT_SERVICE_ADDR value: \"checkoutservice:5050\" - name: AD_SERVICE_ADDR value: \"adservice:9555\" - name: ENV_PLATFORM value: \"gcp\" resources: requests: cpu: 100m memory: 64Mi limits: cpu: 200m memory: 128Mi 将以上文件存储为 frontend-original.yaml 并使用 kubectl apply -f frontend-original.yaml 命令创建部署。 现在我们准备创建一个 DestinationRule，定义两个版本的前端——现有的（original）和新的（v1）。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: frontend spec: host: frontend.default.svc.cluster.local subsets: - name: original labels: version: original - name: v1 labels: version: 1.0.0 将上述 YAML 保存为 frontend-dr.yaml，并使用 kubectl apply -f frontend-dr.yaml 创建它。 接下来，我们将更新 VirtualService，并指定将所有流量路由到的子集。在这种情况下，我们将把所有流量路由到原始版本的前端。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend-ingress spec: hosts: - '*' gateways: - frontend-gateway http: - route: - destination: host: frontend port: number: 80 subset: original 将上述 YAML 保存为 frontend-vs.yaml，并使用 kubectl apply -f frontend-vs.yaml 更新 VirtualService 资源。 现在我们将 VirtualService 配置为将所有进入的流量路由到 original 子集，我们可以安全地创建新的前端部署。 apiVersion: apps/v1 kind: Deployment metadata: name: frontend-v1 spec: selector: matchLabels: app: frontend version: 1.0.0 template: metadata: labels: app: frontend version: 1.0.0 annotations: sidecar.istio.io/rewriteAppHTTPProbers: \"true\" spec: containers: - name: server image: gcr.io/tetratelabs/boutique-frontend:1.0.0 ports: - containerPort: 8080 readinessProbe: initialDelaySeconds: 10 httpGet: path: \"/_healthz\" port: 8080 httpHeaders: - name: \"Cookie\" value: \"shop_session-id=x-readiness-probe\" livenessProbe: initialDelaySeconds: 10 httpGet: path: \"/_healthz\" port: 8080 httpHeaders: - name: \"Cookie\" value: \"shop_session-id=x-liveness-probe\" env: - name: PORT value: \"8080\" - name: PRODUCT_CATALOG_SERVICE_ADDR value: \"productcatalogservice:3550\" - name: CURRENCY_SERVICE_ADDR value: \"currencyservice:7000\" - name: CART_SERVICE_ADDR value: \"cartservice:7070\" - name: RECOMMENDATION_SERVICE_ADDR value: \"recommendationservice:8080\" - name: SHIPPING_SERVICE_ADDR value: \"shippingservice:50051\" - name: CHECKOUT_SERVICE_ADDR value: \"checkoutservice:5050\" - name: AD_SERVICE_ADDR value: \"adservice:9555\" - name: ENV_PLATFORM value: \"gcp\" resources: requests: cpu: 100m memory: 64Mi limits: cpu: 200m memory: 128Mi 将上述 YAML 保存为 frontend-v1.yaml，然后用 kubectl apply -f frontend-v1.yaml 创建它。 如果我们在浏览器中打开 INGRESS_HOST，我们仍然会看到原始版本的前端。让我们更新 VirtualService 中的权重，开始将 30% 的流量路由到 v1 的子集。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: frontend-ingress spec: hosts: - '*' gateways: - frontend-gateway http: - route: - destination: host: frontend port: number: 80 subset: original weight: 70 - destination: host: frontend port: number: 80 subset: v1 weight: 30 将上述 YAML 保存为 frontend-30.yaml，然后用 kubectl apply -f frontend-30.yaml 更新 VirtualService。 如果我们刷新几次网页，我们会注意到来自前端 v1 的更新标头，看起来像下图中的样子。 图 12.5.1：更新后的标头 如果我们打开 Kiali 并点击 Graph，我们会发现有两个版本的前端在运行，如下图所示。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/fault-injection.html":{"url":"practice/fault-injection.html","title":"错误注入","keywords":"","body":"错误注入 在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的请求注入延迟。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendationservice spec: hosts: - recommendationservice http: - route: - destination: host: recommendationservice fault: delay: percentage: value: 50 fixedDelay: 5s 将上述 YAML 保存为 recommendation-delay.yaml，然后用 kubectl apply -f recommendation-delay.yaml 创建 VirtualService。 我们可以在浏览器中打开 INGRESS_HOST，然后点击其中一个产品。推荐服务的结果显示在屏幕底部的”Other Products You Might Light“部分。如果我们刷新几次页面，我们会注意到，该页面要么立即加载，要么有一个延迟加载页面。这个延迟是由于我们注入了 5 秒的延迟。 我们可以打开 Grafana（getmesh istioctl dash grafana）和 Istio 服务仪表板。确保从服务列表中选择recommendationsservice，在 Reporter 下拉菜单中选择 source，并查看显示延迟的 Client Request Duration，如下图所示。 图 12.6.1：Recommendations 服务延迟 同样地，我们可以注入一个中止。在下面的例子中，我们为发送到产品目录服务的 50% 的请求注入一个 HTTP 500。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productcatalogservice spec: hosts: - productcatalogservice http: - route: - destination: host: productcatalogservice fault: abort: percentage: value: 50 httpStatus: 500 将上述 YAML 保存为 productcatalogservice-abort.yaml，然后用 kubectl apply -f productcatalogservice-abort.yaml 更新 VirtualService。 如果我们刷新几次产品页面，我们应该得到如下图所示的错误信息。 图 12.6.2：注入错误 请注意，错误信息说，失败的原因是故障过滤器中止。如果我们打开 Grafana（getmesh istioctl dash grafana），我们也会注意到图中报告的错误。 我们可以通过运行 kubectl delete vs productcatalogservice 来删除 VirtualService。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "},"practice/resiliency.html":{"url":"practice/resiliency.html","title":"弹性","keywords":"","body":"弹性 为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的环境变量。这个变量会在每次调用服务时注入一个额外的休眠。 通过运行 kubectl edit deploy productcatalogservice 来编辑产品目录服务部署。这将打开一个编辑器。滚动到有环境变量的部分，添加 EXTRA_LATENCY 环境变量。 ... spec: containers: - env: - name: EXTRA_LATENCY value: 6s ... 保存并推出编辑器。 如果我们刷新页面，我们会发现页面需要 6 秒的时间来加载）——那是由于我们注入的延迟。 让我们给产品目录服务添加一个 2 秒的超时。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productcatalogservice spec: hosts: - productcatalogservice http: - route: - destination: host: productcatalogservice timeout: 2s 将上述 YAML 保存为 productcatalogservice-timeout.yaml，并使用 kubectl apply -f productcatalogservice-timeout.yaml 创建 VirtualService。 如果我们刷新页面，我们会注意到一个错误信息的出现： rpc error: code = Unavailable desc = upstream request timeout could not retrieve products 该错误表明对产品目录服务的请求超时了。我们修改了服务，增加了 6 秒的延迟，并将超时设置为 2 秒。 让我们定义一个重试策略，有三次尝试，每次尝试的超时为 1 秒。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productcatalogservice spec: hosts: - productcatalogservice http: - route: - destination: host: productcatalogservice retries: attempts: 3 perTryTimeout: 1s 由于我们在产品目录服务部署中留下了额外的延迟，我们仍然会看到错误。让我们打开 Zipkin 中的追踪，看看重试策略的作用。 使用 getmesh istioctl dash zipkin 来打开 Zipkin 仪表盘。点击 + 按钮，选择 serviceName 和 frontend.default。为了只得到至少一秒钟的响应（这就是我们的 perTryTimeout），选择 minDuration，在文本框中输入 1s。点击搜索按钮，显示所有追踪。 点击 Filter 按钮，从下拉菜单中选择 productCatalogService.default。你应该看到花了 1 秒钟的 trace。这些 trace 对应于我们之前定义的 perTryTimeout。 图 12.7.1：Zipkin 中的 trace 运行 kubectl delete vs productcatalogservice 删除 VirtualService。 Copyright © 2017-2022 | Distributed under CC BY 4.0 | jimmysong.io all right reserved. Updated at 2022-03-09 03:30:11 "}}